{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menadione Tn-seq notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download from genewiz sftp\n",
    "sftp jstien01_student_bbk@gweusftp.azenta.com\n",
    "lcd /d/in16/u/sj003/men_tnseq\n",
    "cd 40-842749567\n",
    "mget *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: A1_R1_001.fastq.gz: No such file or directory\n",
      "gzip: A1_R2_001.fastq.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# files in /d/in16/u/sj003/men_tnseq/fastq\n",
    "\n",
    "#!ls fastq\n",
    "!zcat A1_R1_001.fastq.gz | head -5\n",
    "!zcat A1_R2_001.fastq.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File checks to make sure downloaded correctly--matching sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check length of files to see if reads same in all files?\n",
    "!cd fastq\n",
    "!FILES=*.fastq.gz\n",
    "!for file in $FILES; do wc -l $file; done >> sanity_check.txt\n",
    "# all 3 files have different line counts, R2 is index. R1 and R3 more similar, different because of line continuation?\n",
    "\n",
    "# check files downloaded correctly\n",
    "!for file in $FILES; do md5sum -c $file.md5; done >> md5_check.txt\n",
    "\n",
    "#all 'OK'\n",
    "\n",
    "#head\n",
    "!for file in $FILES; do echo $file; zcat $file | head -10 $file; done\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "line count was the same for R1 and R2 when we did cattle in vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: A1_R1_001.fastq.gz: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# for A1 sample see how many times header appears in file\n",
    "!cd fastq\n",
    "!zgrep -c 'N:0:AACGTGAT' A1_R1_001.fastq.gz\n",
    "#64796903\n",
    "!zgrep -c 'N:0:AACGTGAT' A1_R2_001.fastq.gz\n",
    "#64796903\n",
    "!zgrep -c 'N:0:AACGTGAT' A1_R3_001.fastq.gz\n",
    "#64796903"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ON mac the zgrep command doesn't seem to work (or zcat?) use gunzip?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates same number of reads in all files.\n",
    "\n",
    "Run fastqc and then can double-check read length for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-a2cea0d7dfa0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-a2cea0d7dfa0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    FILES=/d/in16/u/sj003/men_tnseq/fastq/*.fastq.gz\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!cd fastq\n",
    "!module load fastqc\n",
    "!module load multiqc\n",
    "!FILES=*.fastq.gz\n",
    "!for f in $FILES; do fastqc ${f} -o fastqc; done\n",
    "!cd fastqc\n",
    "!multiqc .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-03ca3f647ae6>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-03ca3f647ae6>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for file in $FILES; do echo $file; done\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#test bash command\n",
    "\n",
    "pwd\n",
    "FILES=*fastq.gz\n",
    "for file in $FILES; do echo $file; done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look fine--reads of 151bp for R1 and R3 (corresponding to Read 1 and Read 3) and i7 read (R2) is 8bp which represents the barcode.\n",
    "\n",
    "Next step is to iterate through reads in R1 and R2 to 'demultiplex' by combining to one 'template' the reads that have the same barcode in R2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@A01968:63:H77VYDSX5:4:1101:28212:1031 2:N:0:AACGTGAT\r\n",
      "CGCAGTAT\r\n",
      "+\r\n",
      "FFFFFF:F\r\n",
      "@A01968:63:H77VYDSX5:4:1101:29369:1031 2:N:0:AACGTGAT\r\n",
      "GAGCACCT\r\n",
      "+\r\n",
      "FF:FFFFF\r\n",
      "@A01968:63:H77VYDSX5:4:1101:32027:1031 2:N:0:AACGTGAT\r\n",
      "CATACCAA\r\n",
      "+\r\n",
      "FFFFFFFF\r\n",
      "@A01968:63:H77VYDSX5:4:1101:3341:1047 2:N:0:AACGTGAT\r\n",
      "GTACATGC\r\n",
      "+\r\n",
      ":FFFF::F\r\n",
      "@A01968:63:H77VYDSX5:4:1101:9073:1047 2:N:0:AACGTGAT\r\n",
      "GCCGCCCC\r\n",
      "+\r\n",
      "FFFFFFFF\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!zcat fastq/A1_R2_001.fastq.gz | head -20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 1 is the read identifier with the barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zcat: can't stat: fastq/A1_R1_001.fastq.gz (fastq/A1_R1_001.fastq.gz.Z): No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!zcat fastq/A1_R1_001.fastq.gz | head -20\n",
    "!zcat fastq/A1_R1_001.fastq.gz | head -1000 > test_1000_R1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on laptop need to install gnused before can use sed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all unique reads in R2 line 2 of each read\n",
    "#!head -1000 data/fastq/A1_R2_001.fastq > test_1000_R2.txt\n",
    "#!brew install gnu-sed\n",
    "#!sed -n '1~2p' test_1000_R2.txt | sort | uniq -c | sort -nr\n",
    "# need gsed installed for this\n",
    "#!awk 'NR % 2 == 0' data/fastq/test_1000_R2.txt | sort | uniq -c | sort -nr\n",
    "# print every 4 lines starting from line 2\n",
    "!awk 'NR % 4 == 2' data/fastq/test_1000_R2.txt > test.txt\n",
    "cat test.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12 GGGGGGGG\n",
      "   3 ACAAGAAT\n",
      "   2 GCCGCCCT\n",
      "   2 CGGCGTCA\n",
      "   2 ATTGAAGT\n",
      "   2 ATATACAA\n",
      "   2 ACTATCGT\n",
      "   2 AATTAACT\n",
      "   2 AAGGTTAA\n",
      "   1 TTTTGAGG\n",
      "   1 TTTTACCT\n",
      "   1 TTTAGTCA\n",
      "   1 TTGCACGA\n",
      "   1 TTCGTGCA\n",
      "   1 TTCGTACT\n",
      "   1 TTCCCAGT\n",
      "   1 TTCATAAG\n",
      "   1 TTCACGTC\n",
      "   1 TTAGTAAC\n",
      "   1 TTAGACAT\n",
      "   1 TTAAAACC\n",
      "   1 TGTTGTGA\n",
      "   1 TGTTGTCT\n",
      "   1 TGTCCATC\n",
      "   1 TGGTTCAA\n",
      "   1 TGCACTTT\n",
      "   1 TGCAACGG\n",
      "   1 TGATCTTA\n",
      "   1 TGAGCACA\n",
      "   1 TGACATGA\n",
      "   1 TGACATCT\n",
      "   1 TGACATAT\n",
      "   1 TCTTGTCA\n",
      "   1 TCTGCCTC\n",
      "   1 TCTACTTC\n",
      "   1 TCTAACGC\n",
      "   1 TCGTTCAG\n",
      "   1 TCGTCTTA\n",
      "   1 TCGTCCAG\n",
      "   1 TCATTGAA\n",
      "   1 TCATTCAT\n",
      "   1 TCATGTTA\n",
      "   1 TCACATGC\n",
      "   1 TCACAAGT\n",
      "   1 TCAATTAC\n",
      "   1 TCAACAGC\n",
      "   1 TATTAATT\n",
      "   1 TATTAAGC\n",
      "   1 TATGATCT\n",
      "   1 TAGTCAAG\n",
      "   1 TAGTATTT\n",
      "   1 TAGCTGAT\n",
      "   1 TAGCATAA\n",
      "   1 TAGACTCA\n",
      "   1 TACTTGAC\n",
      "   1 TACTATTC\n",
      "   1 TACGATAC\n",
      "   1 TAATCATC\n",
      "   1 TAAGACCC\n",
      "   1 TAAAATCA\n",
      "   1 GTTCTGCT\n",
      "   1 GTGCCGAG\n",
      "   1 GTGACCCA\n",
      "   1 GTGACACG\n",
      "   1 GTCTAATT\n",
      "   1 GTCCACAT\n",
      "   1 GTCCAACT\n",
      "   1 GTAGCATG\n",
      "   1 GTACCGAT\n",
      "   1 GTACATGC\n",
      "   1 GTAATGCA\n",
      "   1 GTAAATAG\n",
      "   1 GTAAAAGA\n",
      "   1 GGGCCGCA\n",
      "   1 GGCAACTC\n",
      "   1 GGCAAACA\n",
      "   1 GGACGACT\n",
      "   1 GCTAGTAG\n",
      "   1 GCGTCGAC\n",
      "   1 GCGCGCAT\n",
      "   1 GCGAAGAG\n",
      "   1 GCCTTAAA\n",
      "   1 GCCTATAT\n",
      "   1 GCCTAACA\n",
      "   1 GCCGCCCC\n",
      "   1 GCATATCG\n",
      "   1 GCACGCAA\n",
      "   1 GCAATCTA\n",
      "   1 GCAAAAAT\n",
      "   1 GATCCCTA\n",
      "   1 GAGCACCT\n",
      "   1 GACGGATT\n",
      "   1 GACGCAAT\n",
      "   1 GACCAGTG\n",
      "   1 GACCACAA\n",
      "   1 GACACGAG\n",
      "   1 GAATTTAG\n",
      "   1 GAAGGTCC\n",
      "   1 GAAAGACA\n",
      "   1 GAAACCAC\n",
      "   1 GAAACAAC\n",
      "   1 CTGGGCAG\n",
      "   1 CTGCGTAA\n",
      "   1 CTGCAAAT\n",
      "   1 CTGCAAAA\n",
      "   1 CTCCCTAA\n",
      "   1 CTACGGAC\n",
      "   1 CTACACTG\n",
      "   1 CTAATGAC\n",
      "   1 CGTTGTTT\n",
      "   1 CGTGCCCG\n",
      "   1 CGGTTGGC\n",
      "   1 CGGTATAG\n",
      "   1 CGGGCTTT\n",
      "   1 CGGACACT\n",
      "   1 CGCTCTGT\n",
      "   1 CGCTAAGG\n",
      "   1 CGCCTCGA\n",
      "   1 CGCCGTAC\n",
      "   1 CGCCGAAC\n",
      "   1 CGCCCCCC\n",
      "   1 CGCCCATA\n",
      "   1 CGCCCAAG\n",
      "   1 CGCAGTAT\n",
      "   1 CGCAACTT\n",
      "   1 CGAGACAT\n",
      "   1 CGAGACAG\n",
      "   1 CGAATGGG\n",
      "   1 CGAAGTAA\n",
      "   1 CGAACTAG\n",
      "   1 CCTGCGCC\n",
      "   1 CCGTTCAA\n",
      "   1 CCGGCACC\n",
      "   1 CCGCGCAT\n",
      "   1 CCGATTTA\n",
      "   1 CCGACCCT\n",
      "   1 CCCTGGTA\n",
      "   1 CCCTAGAT\n",
      "   1 CCCGGCAG\n",
      "   1 CCCCTAAC\n",
      "   1 CCCACTTT\n",
      "   1 CCATGGAC\n",
      "   1 CCATACGA\n",
      "   1 CCAAGACT\n",
      "   1 CCAACCTA\n",
      "   1 CCAAATAT\n",
      "   1 CATGCTCC\n",
      "   1 CATCCCAT\n",
      "   1 CATCAACC\n",
      "   1 CATATTAG\n",
      "   1 CATAGAAT\n",
      "   1 CATAGAAG\n",
      "   1 CATACCTA\n",
      "   1 CATACCAA\n",
      "   1 CAGTGGTG\n",
      "   1 CAGGCTCT\n",
      "   1 CACACTAA\n",
      "   1 CACAATAC\n",
      "   1 CAAGGTTA\n",
      "   1 CAAGATTT\n",
      "   1 CAACTGGC\n",
      "   1 CAACGCAC\n",
      "   1 CAACCGCT\n",
      "   1 CAAAGATG\n",
      "   1 CAAACCGA\n",
      "   1 ATTTCCTA\n",
      "   1 ATTACAAA\n",
      "   1 ATTAATGA\n",
      "   1 ATGTAAGG\n",
      "   1 ATGGCCAA\n",
      "   1 ATGATTGC\n",
      "   1 ATGATGAC\n",
      "   1 ATGATCCT\n",
      "   1 ATCTGACA\n",
      "   1 ATATTCAA\n",
      "   1 ATATCAAA\n",
      "   1 ATATAACT\n",
      "   1 ATAATTTT\n",
      "   1 AGTGGTTG\n",
      "   1 AGTCCCCA\n",
      "   1 AGTAACGT\n",
      "   1 AGGCCTAA\n",
      "   1 AGGAAATG\n",
      "   1 AGCTCTAA\n",
      "   1 AGCCTGTC\n",
      "   1 AGATGACA\n",
      "   1 AGATAACT\n",
      "   1 AGAGAACG\n",
      "   1 AGACCGGC\n",
      "   1 AGAAACCC\n",
      "   1 ACTCGCTA\n",
      "   1 ACTCGATA\n",
      "   1 ACTCCCAC\n",
      "   1 ACTACCAT\n",
      "   1 ACTAACAC\n",
      "   1 ACGGTCAT\n",
      "   1 ACGCTTCC\n",
      "   1 ACGACGCC\n",
      "   1 ACGACCGA\n",
      "   1 ACCGCCCG\n",
      "   1 ACCGATTT\n",
      "   1 ACCGACTT\n",
      "   1 ACCCTCCT\n",
      "   1 ACCCAAAA\n",
      "   1 ACCATCTC\n",
      "   1 ACCAGCAG\n",
      "   1 ACCAAGAC\n",
      "   1 ACCAAACA\n",
      "   1 ACATAACG\n",
      "   1 ACAGGTAG\n",
      "   1 ACAGGATC\n",
      "   1 ACAGCCCC\n",
      "   1 AATTGACT\n",
      "   1 AAGGAGCA\n",
      "   1 AAGGACGG\n",
      "   1 AAGCCGGA\n",
      "   1 AAGCAATG\n",
      "   1 AAGACGAA\n",
      "   1 AACCGCCA\n",
      "   1 AACCGAAA\n",
      "   1 AACCAGAA\n",
      "   1 AACATTAC\n",
      "   1 AACATCCT\n",
      "   1 AACATACT\n",
      "   1 AAATCGCA\n",
      "   1 AAAGACTT\n",
      "   1 AAAATGTA\n",
      "   1 AAAAGTTA\n",
      "   1 AAAAACGA\n",
      "   1 AAAAAAAT\n"
     ]
    }
   ],
   "source": [
    "!sort test.txt | uniq -c | sort -nr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how many duplicates of certain barcodes. need to match barcodes with read name to eliminate ones that have repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do again with whole file (takes a really long time)\n",
    "!awk 'NR % 4 == 2' data/fastq/A1_R2_001.fastq | sort | uniq -c | sort -nr > count_barcodes.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top counts\n",
    "1312199 GGGGGGGG\n",
    "16805 CAAAACGA\n",
    "16371 GCAACATT\n",
    "14835 AATACAAC\n",
    "14507 CCAAAGAC\n",
    "14483 AATATAAC\n",
    "14427 TAACAACA\n",
    "14120 AAAACATA\n",
    "13989 TAAAACTA\n",
    "13544 CCGAAACA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find distribution of these reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which reads are associated with these really big duplications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thiss just puts pattern in file--not whole read\n",
    "!grep 'GGGGGGGG' data/fastq/A1_R1_001.fastq > gggg_dupl.fastq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paste two files together so barcode is added onto read header?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bash solution\n",
    "!paste -d \" \" file1.txt file2.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new fastq file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python\n",
    "seq_list = []\n",
    "with open ('test_1000_R1.txt', 'r') as f1, open('test_1000_R2.txt', 'r') as f2:\n",
    "    #line1_f1 = f1.readline()\n",
    "    #for line in f1.read().split(\"\\n\")[0]:\n",
    "    #    print(line.rstrip())\n",
    "    # header and sequence for R1 reads\n",
    "    for lineno, line in enumerate(f1):  \n",
    "        if lineno % 4 == 0:\n",
    "            f1_head = line.rstrip()\n",
    "        # second line and every 4  (sequence)  \n",
    "        if lineno %4 == 1:\n",
    "            #print(line)\n",
    "            f1_seq = line\n",
    "        # spacer line\n",
    "        if lineno %4 ==2:\n",
    "            f1_spacer = line\n",
    "        #quality line\n",
    "        if lineno %4 ==3:\n",
    "            f1_quality = line\n",
    "\n",
    "        \n",
    "    #create header and barcode for index reads\n",
    "    for lineno, line in enumerate(f2):\n",
    "        if lineno % 4 == 0:\n",
    "            f2_head = line.rstrip()\n",
    "            \n",
    "        if lineno % 4 == 1:\n",
    "            #print(line)\n",
    "            f2_barcode = line.rstrip()\n",
    "        # add barcode to end of header in read1 file\n",
    "        new_head = f1_head + \" BC:\" + f2_barcode\n",
    "\n",
    "        list_entry = new_head + \"\\n\" + f1_seq + f1_spacer + f1_quality\n",
    "    \n",
    "        seq_list.append(list_entry)\n",
    "\n",
    "# write new file with barcodes\n",
    "with open('new_barcode_file_1000.txt', 'w') as outfile:\n",
    "    outfile.writelines(seq_list)\n",
    "outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sequences for the R1 reads that have the GGGGGGGG 'random barcode' in the first 1000 reads, these sequences all have huge runs of G's?\n",
    "\n",
    "\"For Illumina NextSeq/NovaSeq data, polyG can happen in read tails since G means no signal in the Illumina two-color systems. fastp can detect the polyG in read tails and trim them. This feature is enabled for NextSeq/NovaSeq data by default, and you can specify -g or --trim_poly_g to enable it for any data, or specify -G or --disable_trim_poly_g to disable it. NextSeq/NovaSeq data is detected by the machine ID in the FASTQ records. \" \n",
    "Need to map reads first because will exclude artefactual PCR amplicons that have identical P7 as well as same genomic insertion site. (from fastp documentation, https://github.com/OpenGene/fastp)\n",
    "\n",
    "But maybe need to add the barcode to the header of each read first. Adapt above to make python script that can work on entire files of reads and make new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_barcode(file1, file2):\n",
    "    \"\"\"\n",
    "    A function to extract the barcode from the associated p7 index read to the header of each read in the fastq file\n",
    "\n",
    "    Input               file1                       fastq file for sequence reads (R1 or R3)\n",
    "                        file2                       fastq file for i7 index reads (R2)\n",
    "    Output              barcoded<sample>.fastq      new fastq file which has barcode added to header   \n",
    "\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import os.path\n",
    "    import sys\n",
    "\n",
    "    # identify sample name\n",
    "    filename = str(file1)\n",
    "    bn_sample = os.path.basename(file1)\n",
    "    sample = re.sub(\".fastq\", \"\", bn_sample)\n",
    "    print(sample)\n",
    "\n",
    "    seq_list = []\n",
    "    with open (file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "        #count lines in R1 and R2 and proceed if equal\n",
    "       # if sum(1 for _ in f1) != sum(1 for _ in f2):\n",
    "        #    sys.exit(\"Number of reads do not match\")\n",
    "            \n",
    "    # header and sequence for R1 reads\n",
    "        for index, (line1, line2) in enumerate(zip(f1, f2)):\n",
    "            if index % 4 == 0:\n",
    "                f1_head = line1.rstrip()\n",
    "                f2_head = line2.rstrip()\n",
    "            # second line and every 4  (sequence)  \n",
    "            if index %4 == 1:\n",
    "                f1_seq = line1.rstrip()\n",
    "                #create header and barcode for index reads\n",
    "                #>A01968:63:H77VYDSX5:4:1101:25455:1423 1:N:0:AACGTGAT BC:GGGGGGGG\n",
    "                f2_barcode = line2.rstrip()\n",
    "                # add barcode to end of header in read1 file\n",
    "                new_head = f1_head + \"_BC:\" + f2_barcode\n",
    "                #replace whitespace\n",
    "                new_head = new_head.replace(\" \", \"_\")\n",
    "                \n",
    "            # don't need these if do after trimming\n",
    "            #quality line\n",
    "            if index %4 ==3:\n",
    "                f1_quality = line1.rstrip()\n",
    "                list_entry = new_head + \"\\n\" + f1_seq + \"\\n\" + \"+\" + \"\\n\" + f1_quality + \"\\n\"\n",
    "                seq_list.append(list_entry)\n",
    "    print(len(seq_list))\n",
    "    # write new file with barcodes\n",
    "    new_filename = \"barcode_\" + sample + \".fastq\"\n",
    "    with open(new_filename, 'w') as outfile:\n",
    "        outfile.writelines(seq_list)\n",
    "    outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_5000_R1\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "add_barcode(\"test_5000_R1.fastq\", \"test_5000_R2.fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1_R1_001\n"
     ]
    }
   ],
   "source": [
    "add_barcode(\"data/fastq/A1_R1_001.fastq\", \"data/fastq/A1_R2_001.fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1_R3_001\n",
      "64796903\n",
      "barcode_A1_R3_001.fastq\n"
     ]
    }
   ],
   "source": [
    "# can also apply to read2, but no point as won't be using for mapping\n",
    "#add_barcode(\"data/fastq/A1_R3_001.fastq\", \"data/fastq/A1_R2_001.fastq\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map small file to see if header stays connected to read (installed new bwa-mem2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample first 10000 bases of barcoded fastq\n",
    "!head -10000 barcode_A1_R1_001.fastq > new_barcode_file_A1_10000.fastq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate tnseq\n",
    "# have to index file first for bwa-mem2\n",
    "!bwa-mem2 index ref_seqs/Mbovis_AF2122-97.fasta\n",
    "!bwa-mem2 mem -C ref_seqs/Mbovis_AF2122-97.fasta barcode_test_5000_R1.fastq > test_map_5000.sam\n",
    "# still getting error: ERROR! Unable to open the file: ref_seqs/Mbovis_AF212297.fasta.bwt.2bit.64\n",
    "# there were multiple fasta files, deleted all others and manually coppied and pasted text from ref file. changed to Mbovis_AF2122-97.fasta\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "barcode doesn't appear in sam header automatically, -C to append FASTA/FASTQ comment to SAM output (appends to end of file--column 12 in sam file) (-V            output the reference FASTA header in the XR tag--gives blank file)\n",
    "\n",
    "Column 4 in sam header is meant to be the start mapping position (should be insert position). in test file, all have start position as 0--probably didn't map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    starts  barcodes  size\n",
      "0  2736492  AAAAAAAT     1\n",
      "1  2736492  AAAAAAGG     1\n",
      "2  2736492  AAAAACAG     1\n",
      "3  2736492  AAAAACAT     1\n",
      "4  2736492  AAAAACCT     1\n"
     ]
    }
   ],
   "source": [
    "#parse sam file for start and barcode\n",
    "\n",
    "def parse_samfile(samfile):\n",
    "    import pandas as pd\n",
    "    \"\"\"\n",
    "    find molecular barcode and start of mapping in each read\n",
    "    \"\"\"\n",
    "\n",
    "    sam_list = []\n",
    "    barcodes = []\n",
    "    id_dups = []\n",
    "    data = pd.read_csv(samfile, sep=\"\\t\", skiprows=2, header=None)\n",
    "    #print(data.head())\n",
    "    ids = data[0]\n",
    "    starts = data[3]\n",
    "    # sometimes dividing up final lines? 13 for 1000 or 15 for 100000\n",
    "    barcodeLines = list(data[15])\n",
    "    #extract molecular barcode\n",
    "    for line in barcodeLines:\n",
    "        barcodes.append(line.split(\"BC:\",1)[1])\n",
    "    # remove non-unique rows?\n",
    "    #dup_tup = pd.DataFrame(ids)\n",
    "    dup_tup = pd.DataFrame(starts)\n",
    "    dup_tup.columns=[\"starts\"]\n",
    "    dup_tup['barcodes'] = barcodes\n",
    "    # get count of duplicates for each unique row\n",
    "    sorted_dup_tup = dup_tup.groupby(dup_tup.columns.tolist(), as_index=False).size()\n",
    "    \n",
    "    return sorted_dup_tup\n",
    "    \n",
    "    \n",
    "p = parse_samfile(\"test_map_10000.sam\")\n",
    "print(p.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column 3 (0 indexed) is start, column 13/15 has both barcodes, column 9 is sequence. No duplicate start/barcode combos apparent in the 10000bp test file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better approach may be to process reads and find duplicates using Picard find duplicates tool (Mark Duplicates)\n",
    "\n",
    "https://gatk.broadinstitute.org/hc/en-us/articles/360037051452-EstimateLibraryComplexity-Picard-\n",
    "\n",
    "can use argument --BARCODE_TAG 'BC' to identify barcode\n",
    "--READ_NAME_REGEX null if duplicate sets are very high and we are not trying to establish library complexity\n",
    "\n",
    "Reads must be mapped and sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c bioconda picard\n",
    "!java -jar picard.jar MarkDuplicates \\\n",
    "      I=input.bam \\\n",
    "      O=marked_duplicates.bam \\\n",
    "      M=marked_dup_metrics.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step before mapping is to remove the transposon sequence from each read\n",
    "## From TRANSIT/TPP docs:\n",
    "\n",
    "1. Convert .fastq files to .fasta format (.reads).\n",
    "    https://bioinformaticsworkbook.org/dataWrangling/fastaq-manipulations/converting-fastq-format-to-fasta.html#gsc.tab=0\n",
    "    This would only keep header and sequence.\n",
    "\n",
    "(creates .reads file using 'fastq2reads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gsed -n '1~4s/^@/>/p;2~4p' new_barcode_file_1000.fastq > new_barcode_file_1000.fasta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Identify reads with the transposon prefix in R1 . The sequence searched for is ACTTATCAGCCAACCTGTTA (or TAAGAGACAG for Tn5), which must start between cycles 5 and 10 (inclusive). (Note that this ends in the canonical terminus of the Himar1 transposon, TGTTA.) The “staggered” position of this sequence is due to insertion a few nucleotides of variable length in the primers used in the Tn-Seq sample prep protocol (e.g. 4 variants of Sol_AP1_57, etc.). The number of msmatchen searching reads for the transposon sequence pattern can be adjusted as an option in the interface; the default is 1.\n",
    "\n",
    "    Use fastp for this? First try with TPP?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(thoth)\n",
    "#!tpp -bwa /s/software/bwa/bwa/bwa -ref $my_path/refseqs/Mtb/Mtb_H37Rv.fasta \\\n",
    "#    -reads1 $my_path/ncbi/files/dejesus/SRR4113428_1.fastq \\\n",
    "#        -reads2 $my_path/ncbi/files/dejesus/SRR4113428_2.fastq \\\n",
    "#            -output $my_path/dejesus_mtb/tpp_results/SRR4113428\n",
    "\n",
    "# paired-end (laptop)\n",
    "# this will look for barcode which isnt' there, but maybe useful to trim read 2 and extract genomic sequence?\n",
    "!tpp -bwa ~/anaconda3/envs/tnseq/bin/bwa-mem2 -ref ref_seqs/Mbovis_AF2122-97.fasta \\\n",
    "    -reads1 barcode_A1_R1_001.fastq -reads2 data/fastq/A1_R3_001.fastq \\\n",
    "        -output tpp_test_A1_paired\n",
    "\n",
    "# Error: unexpected format of headers in .fastq files\n",
    "#Assume this is due to adding barcode to header for read1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastp can use index reads for UMI processing--can extract from index reads and append to first part of read names so it will be in sam/bam files, can specify prefix 'UMI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: fastp\n"
     ]
    }
   ],
   "source": [
    "!fastp -i data/fastq/A1_R1_001.fastq -I data/fastq/A1_R3_001.fastq -o data/fastp_out/A1_R1_001_trimmed.fastq -O data/fastp_out/A1_R3_001_trimmed.fastq \\\n",
    " -U --umi_loc=index1 --umi_prefix=UMI --unpaired1 -l=20 --adapter_fasta adapter_file.txt\n",
    "\n",
    "#It reads from index that is already in multiplexed header--not from index file, so not useful unless specify P7 reads as R2 and it can get umi from read2\n",
    "#--unpaired1 means will save unpaired read 1s, -l 20 means will require reads to be at least 20 bp after trimming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added an 'adapter file' to use with fastp: 'adapter_file.txt' which includes the transposon sequence pattern for R1 and R2.\n",
    "\n",
    "This is leading to very short R1 sequences which actually match the transposon sequence (no genomic?) Are R1 and R3 reversed and R3 is really read1?\n",
    "\n",
    "Do tests with shorter number of reads (100000)\n",
    "\n",
    "Use R2 index reads as read2?\n",
    "\n",
    "#temporarily disable adapter trimming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fastp -i data/fastq/A1_R1_001.fastq -I data/fastq/A1_R2_001.fastq -o data/fastp_out/A1_R1_001_trimmed.fastq -O data/fastp_out/A1_R3_001_trimmed.fastq \\\n",
    " #-U --umi_loc=read2 --umi_prefix=UMI --umi_len=8 --unpaired1 -l 20 --adapter_fasta adapter_file.txt --reads_to_process=100000\n",
    "\n",
    "fastp -i data/fastq/A1_R1_001.fastq -I data/fastp_out/A1_R2_001_trimmed.fastq \\ \n",
    "-U --umi_loc=read2 --umi_prefix=UMI --umi_len=8 --reads_to_process=100000 -o data/fastp_out/A1_R1_001_trimmed.fastq -O data/fastp_out/A1_R3_001_trimmed.fastq\n",
    " \n",
    " #this doesn't work--needs to have properly paired ends"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is new heading that fastp makes that includes UMI: @A01968:63:H77VYDSX5:4:1101:32027:1031:UMI_AACGTGAT 1:N:0:AACGTGAT\n",
    "I imagine this will work for TPP and still have heading included in mapped reads if I just move it to end of first field instead of randomly adding to end. MIght be easier to filter with picard then. but not sure it matters if I don't do tpp since I can parse from sam files myself--it is only using P5 index from the header of the reads--NOT UMI barcode from p7 index files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpaired fastp for adapter trimming\n",
    "!fastp -i new_barcode_file_A1_10000.fastq -o data/fastp_out/test_A1_R1.fastq --adapter_fasta adapter_file.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just leaving super short reads like before (15 bp long--also still part of primer, no genomic sequence left). Maybe because I have both ends of inverted repeat sequence in there\n",
    "\n",
    "@A01641:207:HNJLJDSX3:1:2678:31331:37059 1:N:0:AACGTGAT BC:CGCAGTAT\n",
    "GTCTAGAGACCGGGG\n",
    "+\n",
    "F::FF:FFFFFFFFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without adapter fasta file--using transposon seq for R1 only\n",
    "!fastp -i data/fastq/A1_R1_001.fastq -o data/fastp_out/test_repeat_A1_R1.fastq --adapter_sequence=ACTTATCAGCCAACCTGTTA --reads_to_process=100000\n",
    "#results in mostly 15 bp identical sequences\n",
    "#reads with adapter trimmed: 98991 (trims nearly all)\n",
    "\n",
    "#try with R3\n",
    "!fastp -i data/fastq/A1_R3_001.fastq -o data/fastp_out/test_repeat_A1_R3.fastq --adapter_sequence=ACTTATCAGCCAACCTGTTA --reads_to_process=100000\n",
    "#reads with adapter trimmed: 17\n",
    "\n",
    "#using Read 2 transposon sequence:\n",
    "!fastp -i data/fastq/A1_R3_001.fastq -o data/fastp_out/test_repeat_A1_R3.fastq --adapter_sequence=TGGTCGTGGTAT --reads_to_process=100000\n",
    "#bases trimmed due to adapters: 7559\n",
    "\n",
    "# using rc of read 2 transposon sequence:\n",
    "!fastp -i data/fastq/A1_R3_001.fastq -o data/fastp_out/test_repeat_A1_R3.fastq --adapter_sequence=TACCACGACCA --reads_to_process=100000\n",
    "\n",
    "\n",
    "# using automatic detection\n",
    "!fastp -i data/fastq/A1_R1_001.fastq -o data/fastp_out/test_automatic_A1_R1.fastq --reads_to_process=100000\n",
    "#finds truseq adapter read1, but not many are actually trimmed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First trim with sequencing adapters. Then do again in separate step (cutadapt) to cut off transposon prefix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "added barcode info to both paired-end reads (R1 and R3). After mapping will try to remove duplicates and make wigs again? Will have to put into header of both reads as TPP needs these to match.\n",
    "\n",
    "Paired end takes hours for single sample. Not going to be useful as R2 contains empty barcodes. Use single-end to see if get anything useful. otherwise do manually and compare with trimmed files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tpp on paired ends with matched headers. doesn't map with no barcodes on 2nd read (set for paired end#)\n",
    "!tpp -bwa ~/anaconda3/envs/tnseq/bin/bwa-mem2 -ref ref_seqs/Mbovis_AF2122-97.fasta \\\n",
    "    -reads1 barcode_A1_R1_001.fastq -reads2 barcode_A1_R3_001.fastq -output tpp_test_A1_paired\n",
    "\n",
    "# try for single end \n",
    "#!tpp -bwa ~/anaconda3/envs/tnseq/bin/bwa-mem2 -ref ref_seqs/Mbovis_AF2122-97.fasta \\\n",
    "#    -reads1 barcode_A1_R1_001.fastq -output tpp_test_A1_single\n",
    "\n",
    "#on thoth\n",
    "!tpp -bwa /s/software/bwa/bwa/bwa -ref /d/in16/u/sj003/refseqs/mbovis/Mbovis_AF2122_97.fasta \\\n",
    "    -reads1 fastq/A1_R1_001.fastq.gz -output tpp/tpp_test_A1_single\n",
    "#TypeError: sequence item 1: expected str instance, bytes found\n",
    "#gunzipped file and tried again with unzipped file--seemed to work, but want to use barcoded fastq instead so aborted\n",
    "#!gunzip fastq/A1_R1_001.fastq.gz\n",
    "#!tpp -bwa /s/software/bwa/bwa/bwa -ref /d/in16/u/sj003/refseqs/mbovis/Mbovis_AF2122_97.fasta \\\n",
    "#    -reads1 fastq/A1_R1_001.fastq -output tpp/tpp_test_A1_single\n",
    "\n",
    "#zip file on laptop, send to thoth, try again\n",
    "!gzip barcode_A1_R1_001.fastq \n",
    "!scp barcode_A1_R1_001.fastq.gz sj003@ssh.cryst.bbk.ac.uk:/d/in16/u/sj003/men_tnseq/fastq/\n",
    "!gunzip fastq/barcode_A1_R1_001.fastq.gz\n",
    "!tpp -bwa /s/software/bwa/bwa/bwa -ref /d/in16/u/sj003/refseqs/mbovis/Mbovis_AF2122_97.fasta \\\n",
    "    -reads1 fastq/barcode_A1_R1_001.fastq -output tpp/tpp_test_A1_single\n",
    "\n",
    "#barcoded reads were wrong--all had same sequence information (bug in program)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "3. Extract genomic part of read 1. This is the suffix following the transposon sequence pattern above. However, for reads coming from fragments shorter than the read length, the adapter might appear at the other end of R1, TACCACGACCA. If so, the adapter suffix is stripped off. (These are referred to as “truncated” reads, but they can still be mapped into the genome just fine by BWA.) The length of the genomic part must be at least 20 bp.\n",
    "\n",
    "4. Extract barcodes from read 2. Read 2 is searched for GATGGCCGGTGGATTTGTGnnnnnnnnnnTGGTCGTGGTAT”. The length of the barcode is typically 10 bp, but can be varaible, and must be between 5-15 bp.\n",
    "    -can tpp skip this step (it takes absolutely forever)?\n",
    "    -can this be done manually and fed in? or can I use header from .sam files in step 8 instead of barcode\n",
    "\n",
    "5. Extract genomic portions of read 2. This is the part following TGGTCGTGGTAT…. It is often the whole suffix of the read. However, if the read comes from a short DNA fragment that is shorter than the read length, the adapter on the other end might appear, in which case it is stripped off and the nucleotides in the middle representing the genomic insert, TGGTCGTGGTATxxxxxxxTAACAGGTTGGCTGATAAG. The insert must be at least 20 bp long (inserts shorter than this are discarded, as they might map to spurious locations in the genome).\n",
    "    -can this work in paired end without barcodes? \n",
    "\n",
    "6. Map genomic parts of R1 and R2 into the genome using BWA. Mismatches are allowed, but indels are ignored. No trimming is performed. BWA is run in ‘sampe’ mode (treating reads as pairs). Both reads of a pair must map (on opposite strands) to be counted.\n",
    "    \n",
    "\n",
    "7. Count the reads mapping to each TA site in the reference genome (or all sites for Tn5).\n",
    "\n",
    "8. Reduce raw read counts to unique template counts. Group reads by barcode AND mapping location of read 2 (aka fragment “endpoints”).\n",
    "    -i can do this with mapped reads\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#original read1\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "GTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACCGAGGCATCCCAGAGCTTGCCGGCTTCCTCGCCGCGCGGAATCTTCGAGAGCACCGGCCCGAAGAACGCCACACCATTGACATGGATCGTCGGCGTACCGACGTCCTCGCCCAC\n",
    "\n",
    "#trimmed read1\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "ACCGAGGCATCCCAGAGCTTGCCGGCTTCCTCGCCGCGCGGAATCTTCGAGAGCACCGGCCCGAAGAACGCCACACCATTGACATGGATCGTCGGCGTACCGACGTCCTCGCCCAC\n",
    "\n",
    "#original read2\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "ACTCACCGCTGCTGACTCCAACGAGAGGACCGCGCCATGCTCGAGAAGGCCCCCCAGAAGTCTGGCGCCGATTTCTGGTTCGATCCGCTGTGCCCGTGGTGCTGGATCACGTCGCGCTGGATCCTCGAGGTGGCAAAGGTCCGCGACATCG\n",
    "\n",
    "#trimmed read2\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "ACTCACCGCTGCTGACTCCAACGAGAGGACCGCGCCATGCTCGAGAAGGCCCCCCAGAAGTCTGGCGCCGATTTCTGGTTCGATCCGCTGTGCCCGTGGTGCTGGATCACGTCGCGCTGGATCCTCGAGGTGGCAAAGGTCCGCGACATCG\n",
    "\n",
    "#paired.barcodes2\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "XXXXXXXXXX\n",
    "\n",
    "#paired.genomic2\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "XXXXXXXXXX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 1 is successfully trimmed using himar1 sequence. Read2 isn't trimmed (can't find himar1 sequence). Can use these files for ds analysis? (_paired.trimmed1). This is final fastq for read1 for tpp. _paired.genomic2 is final file for read 2.\n",
    "\n",
    "I should maybe map using .trimmed1 and .trimmed2? or run tpp in single-end mode only\n",
    "\n",
    "Mendum, smith, butler and that group only gets single-end reads from sequencing. OK to ignore read 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate (non-tpp) strategy\n",
    "1. add barcodes to read 1 from index reads \n",
    "2. fastp for illumina trimming and QC (.fastq, filtered and trimmed)\n",
    "3. then map with bwa-mem2 (.sam/.bam)\n",
    "4. then filter reads for ...TGGTA in first 32 bp of read and start/barcode (filtered and mapped)\n",
    "5. then count template reads mapping to each TA site (generate .wig files)\n",
    "6. use transit for essentiality/resampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline\n",
    "\n",
    "1. add barcodes to Read1 from index reads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_barcode(READ1, INDEX)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. fastp for qc and illumina adapter trimming\n",
    "\n",
    "I believe the reads have already been trimmed by genewiz when multiplexed as fastp doesnt detect many of the sequencing adapters. Could skip trimming, but useful for shortening test file and also qc and catching any untrimmed adapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automatic adapter trimming\n",
    "#!fastp -i barcode_A1_R1_001.fastq.gz -o trimmed/test_A1_R1_trimmed.fastq --reads_to_process=10000\n",
    "# this trims all reads and nothing left!\n",
    "#Detecting adapter sequence for read1...\n",
    "#GTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACCGAGGCATCCCAGAGCTTGCCGG\n",
    "\n",
    "#this is trasposon sequence and also genomic dna. skip trimming and map without? more aggressive trimming using barcode processed file\n",
    "\n",
    "#disable adapter trimming or indicate only truseq adapter\n",
    "\n",
    "!fastp -a GATCGGAAGAGCACACGTCTGAACTCCAGTCAC -i barcode_A1_R1_001.fastq -o trimmed/test_A1_R1_trimmed.fastq --reads_to_process=10000\n",
    "\n",
    "#on thoth\n",
    "!fastp -a GATCGGAAGAGCACACGTCTGAACTCCAGTCAC -i fastq/barcode_A1_R1_001.fastq -o trimmed/barcode_A1_R1_001.fastq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Map read 1 with bwa-mem2 and sort\n",
    "    -can use TPP single end for this?\n",
    "\n",
    "    Use snakemake mapping/sorting script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda activate tnseq\n",
    "# # have to index file first for bwa-mem2\n",
    "# !bwa-mem2 index ref_seqs/Mbovis_AF2122-97.fasta\n",
    "# !bwa-mem2 -t 3 -C ref_seqs/Mbovis_AF2122-97.fasta data/fastp_out/A1_R1_trimmed.fastq > mapped_reads/A1_R1.sam\n",
    "# # bwa samse?\n",
    "# #!samtools view -Sb mapped_reads/A1_R1.bam\n",
    "# !samtools sort -T -o sorted_reads/A1_R1_sorted.sam -O sam mapped_reads/A1_R1.sam \n",
    "\n",
    "\n",
    "# with snakemake (maps, sorts, indexes and creates flagstats report)\n",
    "#make config.yaml file\n",
    "\n",
    "\n",
    "!cd ~/tn_seq/menadione_tnseq/\n",
    "!conda activate snakemake\n",
    "!snakemake -np -s ~/snakemake/tnseq/snakefile.smk\n",
    "!snakemake --cores 2 -s ~/snakemake/tnseq/snakefile.smk\n",
    "#!snakemake -np -s $my_path/snakemake/tnseq/snakefile.smk\n",
    "#!nohup snakemake --cores 8 -s $my_path/snakemake/map_bwa/pe/snakefile.smk > nohup_map.out 2>&1 &"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "samtools doesn't recognise the .sam format made by bwa in the -C mode includes quality scores, etc? When transit adds barcodes, it does it to names when making read files, so I've moved barcode to name so I don't have to worry about keeping any fields after the sequence.\n",
    "\n",
    "More info about -C parameter and formation of sam headers:\n",
    "\"Append append FASTA/Q comment to SAM output. This option can be used to transfer read meta information (e.g. barcode) to the SAM output. Note that the FASTA/Q comment (the string after a space in the header line) must conform the SAM spec (e.g. BC:Z:CGTAC). Malformated comments lead to incorrect SAM output.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "samtools sort mapped_reads/test_A1_R1.sam \n",
    "[E::aux_parse] unrecognized type ':'\n",
    "[W::sam_read1_sam] Parse error at line 3\n",
    "samtools sort: truncated file. Aborting\n",
    "\n",
    "# mapped reads file from bwa mem\n",
    "@SQ\tSN:NC_002945.3\tLN:4345492\n",
    "@PG\tID:bwa\tPN:bwa\tVN:0.7.17-r1188\tCL:bwa mem -t 2 -C ref_seqs/Mbovis_AF2122-97.fasta trimmed/test_A1_R1_trimmed.fastq\n",
    "A01641:207:HNJLJDSX3:1:2678:31331:37059\t0\tNC_002945.3\t2736492\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACCGAGGCATCCCAGAGCTTGCCGGCTTCCTCGCCGCGCGGAATCTTCGAGAGCACCGGCCCGAAGAACGCCACACCATTGACATGGATCGTCGGCGTACCGACGTCCTCGCCCAC\tF::FF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFF:FFFFFF:FFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\t1:N:0:AACGTGAT BC:CGCAGTAT\n",
    "A01641:207:HNJLJDSX3:1:2678:31331:37059\t0\tNC_002945.3\t2736492\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACCGAGGCATCCCAGAGCTTGCCGGCTTCCTCGCCGCGCGGAATCTTCGAGAGCACCGGCCCGAAGAACGCCACACCATTGACATGGATCGTCGGCGTACCGACGTCCTCGCCCAC\tF::FF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFF:FFFFFF:FFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\t1:N:0:AACGTGAT BC:GAGCACCT\n",
    "A01641:207:HNJLJDSX3:1:2678:31331:37059\t0\tNC_002945.3\t2736492\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACCGAGGCATCCCAGAGCTTGCCGGCTTCCTCGCCGCGCGGAATCTTCGAGAGCACCGGCCCGAAGAACGCCACACCATTGACATGGATCGTCGGCGTACCGACGTCCTCGCCCAC\tF::FF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFF:FFFFFF:FFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\t1:N:0:AACGTGAT BC:CATACCAA\n",
    "\n",
    "# mapped reads from previous project\n",
    "@SQ\tSN:NC_000962.3\tLN:4411532\n",
    "@PG\tID:bwa\tPN:bwa\tVN:0.7.17-r1188\tCL:/Users/jenniferstiens/anaconda3/envs/tnseq/bin/bwa mem /Users/jenniferstiens/tn_seq/data/Mtb_H37Rv.fasta hiseq_trimmed_tpp_MtbA022.trimmed1\n",
    "GWNJ-0957:701:GW201208000:7:1101:13210:1309\t16\tNC_000962.3\t2970926\t60\t113M\t*\t0\t0\tCAGCAGCACCTCTCCCCAGAGGGCCGCAAAACCTATCGCAGCACGTTGCGGGGCTTCTTCGTGTCGGCCTACGAAATGGACCGGGTGCGCGACTATGTCGCAGACTCCCTGCC\t*\tNM:i:1\tMD:Z:64G48\tAS:i:108\tXS:i:0\n",
    "GWNJ-0957:701:GW201208000:7:1101:10419:1327\t0\tNC_000962.3\t589184\t60\t111M\t*\t0\t0\tTGAACGCGTTCTTCACCACGGCGATGGCGCTGCGTCTTCTTCACTCTGATCCCGGCAGTCCGGCGTGCCGGGTTTTTGAAGGCGAGCTGTACGATCACTGGACCATCGGGC\t*\tNM:i:9\tMD:Z:10G23C1C0G3G4C10A31A4G16\tAS:i:66\tXS:i:0\n",
    "GWNJ-0957:701:GW201208000:7:1101:14742:1327\t16\tNC_000962.3\t2468660\t60\t114M\t*\t0\t0\tGTGCGCGACAAGCGCACCGATCAGGCCTTGGCTAAGCTGAGCAGCGACGCGTTTCTCAAGCAGTACTCCCAGGTCGCAGTTACCTCGATCGACAAAATCGCGTACTGGTCGCAA\t*\tNM:i:8\tMD:Z:15T16C20C1G8T11A3G19T13\tAS:i:74\tXS:i:0\n",
    "GWNJ-0957:701:GW201208000:7:1101:31974:1397\t4\t*\t0\t0\t*\t*\t0\t0\tCTAGAGGGCCCAATTCGCCCTATAGTGAGTTGGATTGCTCTTCACTGGCCGTCGTTTATCAACGTCGTGACTGGGAAAACCCTGGCGTTACCCCACTTAATCGCCTTGCAGCC\t*\tAS:i:0\tXS:i:0\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "except for barcode, all my reads are identical. maybe this is something wrong with the barcoded file--since fastp also removed whole read.\n",
    "\n",
    "Yes--barcode script was wrong. fixed bug. This works for downstream snakemake trimming/mapping pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out unmapped reads--part of snakemake pipeline. Changed snakemake to include header (@SQ) in output .sam file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. filter reads for transposon sequence (in first 32 bp?)\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@HD\tVN:1.6\tSO:coordinate\n",
    "@SQ\tSN:NC_002945.3\tLN:4345492\n",
    "@PG\tID:bwa\tPN:bwa\tVN:0.7.17-r1188\tCL:bwa mem -t 2 -C ref_seqs/Mbovis_AF2122-97.fasta trimmed/test_5000_R1_trimmed.fastq\n",
    "@PG\tID:samtools\tPN:samtools\tPP:bwa\tVN:1.16.1\tCL:samtools sort -T sorted_reads/test_5000_R1 -O SAM mapped_reads/test_5000_R1.sam\n",
    "A01968:63:H77VYDSX5:4:1101:8757:2503_1:N:0:AACGTGAT_BC:GAACAACT\t16\tNC_002945.3\t20232\t60\t118M33S\t*\t0\t0\tATACGCGTTCGATGACCTCGGTGCCGGCCGCCGTAATCGGCGACTTATTTCGTGGGCGGGTGCGCAGTGGGCGGCGGGCTCCGTGCGAGATGCGTGCCAGGATGGCCAGCAATATGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
    "A01968:63:H77VYDSX5:4:1101:15022:1611_1:N:0:AACGTGAT_BC:CGCCACCT\t0\tNC_002945.3\t26121\t60\t32S119M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGCACGCTCATCGTGTGTCCTTGCGGCCAGGGATAGCGCCGTAGCTGATCGTAGATAGTGGTGCGGCACATGTCGTCGCCAGTGGCCGCCAGCAGCGGGTCCCGCGCCTGCGGTGTG\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:119\tAS:i:11XS:i:0\n",
    "A01968:63:H77VYDSX5:4:1101:20184:2942_1:N:0:AACGTGAT_BC:CCGACTGA\t0\tNC_002945.3\t31581\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGCGTCTCGCTGAGCGAGGCGGCGATGGAGACCGACGCAGAAACCCTGGCGGAAGCCATCCTGCTCACCGCCGACGTGTCCTGCCTTAAAGCGTTGCTGGAAGTACGCAACGAGATC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFF\tNM:i:0\tMD:Z:118\tAS:i:11XS:i:0\n",
    "A01968:63:H77VYDSX5:4:1101:24089:1595_1:N:0:AACGTGAT_BC:TATTCTAA\t16\tNC_002945.3\t35118\t60\t118M33S\t*\t0\t0\tGTACCGGGCGGCGCTGATGCTCAGCCTAAAAGATGCGATCAGCCGAGATAAACGGCGAATGGAAATGGGTATTACGAACTATTTCACAAAACTTCGCATTCCGGGTGCCCGAGTCATAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:11XS:i:0\n",
    "A01968:63:H77VYDSX5:4:1101:15311:2644_1:N:0:AACGTGAT_BC:AATCACTC\t0\tNC_002945.3\t37337\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAATCAGCTATCAGGACCTCATCGCGCGCGCGGCGGCATGCATCCCCCCGCTACGGCGTCTTGACATCAAACGCGGTGAACCCGTGCTGATCACCGCCCCCACCAACCTGGAATTCCT\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFFF,F:FFFFFFFFFF,FFFFF\tNM:i:2\tMD:Z:65C33A18\tAS:i:10XS:i:0\n",
    "A01968:63:H77VYDSX5:4:1101:28682:1689_1:N:0:AACGTGAT_BC:TGAGGCGA\t16\tNC_002945.3\t39750\t60\t109M33S\t*\t0\t0\tTCAGCGCGTCGAGGTCGTCGCTTTCGGCACGCAGGTCTGCCACGAACGGCCCAGGATCCGCCATCACCACCTCCTGAGGTAACAGTTCGTCGGGAAAGGCATGTTTGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\t,F::FF::F:F,FFFFFF:FF::FFFFF,FFFFFFF,,,FFFF:F,FFF,FFFFFF,FFF:,FFFFFF::F:F,FFFFFFFF,:FFF::FFF,FFFFFF:FFFFFFFF,F:F:FFFF:FFFFFF:F:FF::,F:,F,FFFFF\tNM:i:1\tMD:Z:0C108\tAS:i:108\tXS:i:0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice <first 32 bp of sequence line> | grep <transposon tag> > concatenate to .tag_positive file\n",
    "\n",
    "\n",
    "\n",
    "#filter reads in samfile for transposon tag in sequence\n",
    "#from tpp (https://github.com/mad-lab/transit/blob/master/src/pytpp/tpp_tools.py)\n",
    "# find index of H[1..m] in G[1..n] with up to max mismatches\n",
    "# note: this find first match, not necessarily the best (with min mismatches)\n",
    "\n",
    "def mmfind1(G,n,H,m,max): # lengths; assume n>m\n",
    "  \"\"\"\n",
    "  A function to find matches of transposon sequence in read\n",
    "    Input               G                           sequence string\n",
    "                        n                           length of read sequence\n",
    "                        H                           pattern string\n",
    "                        m                           length of pattern\n",
    "                        max                         maximum mismatches\n",
    "    Output              i, -1                       start position of match, or -1 for no match\n",
    "  \"\"\"\n",
    "\n",
    "  a = G[:n].find(H[:m])\n",
    "  if a!=-1: return a # shortcut for perfect matches\n",
    "  for i in range(0,n-m):  # range of 0 to difference between seq length and len pattern\n",
    "    cnt = 0\n",
    "    for k in range(m):\n",
    "      if G[i+k]!=H[k]: cnt += 1\n",
    "      if cnt>max: break\n",
    "    if cnt<=max: return i\n",
    "  return -1\n",
    "    \n",
    "def find_tags(samfile, target_tag, mismatch_max=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find transposon tag in sequence of each read.\n",
    "\n",
    "      Input           samfile           file of mapped reads in .sam format\n",
    "                      target_tag        string that matches transposon sequence\n",
    "                      mismatch_max      number of mismatches allowed\n",
    "      Output          match_list        list of length of number of reads in .sam file \n",
    "                                        with match start position or -1 if no match\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "\n",
    "    match_list = []\n",
    "    # this is giving errors in parsing\n",
    "    data = pd.read_csv(samfile, sep=\"\\t\", comment=\"@\", header=None, usecols=[0,1,2,3,4,5,6,7,8,9,10])\n",
    "    seqs = data[9]\n",
    "    #search string for transposon seq with 2 mismatches\n",
    "    for i in range(0, len(seqs)):\n",
    "      seq = seqs[i]\n",
    "      res = mmfind1(seq, len(seq), target_tag, len(target_tag), mismatch_max)\n",
    "      match_list.append(res)\n",
    "    #print(len(match_list))\n",
    "    print(\"Number of reads with no transposon tag: \", match_list.count(-1))\n",
    "    print(\"Number of reads with tag within first 32 bases: \", sum(1 for i in match_list if i >0 and i < 32))\n",
    "    bn = os.path.basename(samfile)\n",
    "    file_name = bn + \"_tags\" + \".pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "      pickle.dump(match_list, f)\n",
    "    return match_list\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reads with no transposon tag:  611\n",
      "Number of reads with tag within first 32 bases:  635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 14,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 14,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 14,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 14,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 14,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_tags(\"sorted_reads/test_5000_R1.sam\", \"ACTTATCAGCCAACCTGTTA\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_barcodes(read):\n",
    "    \"\"\"\n",
    "    find molecular barcode and start of mapping in each read\n",
    "    \"\"\"\n",
    "    #extract molecular barcode from a single read and return tuple\n",
    "    \n",
    "    read_name = read.split()[0]\n",
    "    barcode = read_name.split(\"BC:\",1)[1]\n",
    "    read_start = read.split()[3]\n",
    "    bar_start = (barcode, read_start) \n",
    "    return bar_start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def filter_mapped_reads(sam_file, tag=\"ACTTATCAGCCAACCTGTTA\", mis_max=2, position_pkl=[]):\n",
    "  \"\"\" \n",
    "  \"\"\"\n",
    "  import sys\n",
    "  import pickle\n",
    "  import os\n",
    "  import pandas as pd\n",
    "  from operator import itemgetter\n",
    "  if position_pkl == []:\n",
    "    #create pos_list\n",
    "    pos_list = find_tags(sam_file, tag, mis_max)\n",
    "  else:\n",
    "    with open(position_pkl, 'rb') as f:\n",
    "      pos_list = pickle.load(f)\n",
    "  #read sam_file and sort lines between header and reads\n",
    "  header = []\n",
    "  reads  = []\n",
    "  barcode_list = []\n",
    "  good_reads = []\n",
    "  notags_reads = []\n",
    "  bad_reads = []\n",
    "  with open(sam_file, 'r') as f:\n",
    "    for line in f:\n",
    "      line = line.strip()\n",
    "      if line[0] == \"@\":\n",
    "        header.append(line)\n",
    "      else:\n",
    "        reads.append(line)\n",
    "  #compare number of reads in sam file to positions in pos_list\n",
    "  if len(reads)==len(pos_list):\n",
    "    for i in range(0, len(pos_list)):\n",
    "      if pos_list[i] != -1:\n",
    "        #find barcode/start combo\n",
    "        bc_start = find_barcodes(reads[i])\n",
    "        #add to list \n",
    "        barcode_list.append(bc_start) #list will essentially be sorted by read start because reads are sorted?\n",
    "        barcode_list.sort(key=itemgetter(0))  #maybe this will speed up search by barcode?\n",
    "        # if hasn't been added before, add read to good_reads\n",
    "        if barcode_list.count(bc_start) < 2:\n",
    "          good_reads.append(reads[i])\n",
    "        else:\n",
    "          bad_reads.append(reads[i])\n",
    "      else:\n",
    "        notags_reads.append(reads[i])\n",
    "    print(\"number of good reads (with tag): \", len(good_reads))\n",
    "    print(\"number of bad reads (with no tag): \", len(notags_reads))\n",
    "    print(\"Number of reads with duplicate barcode/starts: \", len(bad_reads))\n",
    "    bn = os.path.basename(sam_file)\n",
    "    outfile = \"tag_filtered_\" + bn\n",
    "    with open(outfile, 'w') as f:\n",
    "      for line in header:\n",
    "        f.write(f\"{line}\\n\")\n",
    "      for line in good_reads:\n",
    "        f.write(f\"{line}\\n\")\n",
    "  else:\n",
    "    sys.exit(\"reads file length doesn't match position file\")\n",
    "  \n",
    "  return barcode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reads with no transposon tag:  611\n",
      "Number of reads with tag within first 32 bases:  635\n",
      "number of good reads (with tag):  603\n",
      "number of bad reads (with no tag):  611\n",
      "Number of reads with duplicate barcode/starts:  32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('AAAAACAG', '274566'),\n",
       " ('AAAAACGC', '3897925'),\n",
       " ('AAAACTAG', '224546'),\n",
       " ('AAAAGTCT', '3648873'),\n",
       " ('AAAATGTA', '1384138'),\n",
       " ('AAAATTAA', '3373285'),\n",
       " ('AAACAAAG', '1165185'),\n",
       " ('AAACAAGT', '475611'),\n",
       " ('AAACATTG', '3973761'),\n",
       " ('AAACCTTT', '2558063'),\n",
       " ('AAACTTCC', '985131'),\n",
       " ('AAAGCAAA', '2948421'),\n",
       " ('AAATAAAC', '0'),\n",
       " ('AAATAGTT', '1616431'),\n",
       " ('AAATAGTT', '1616431'),\n",
       " ('AAATCGCA', '2075169'),\n",
       " ('AAATGGAA', '3140968'),\n",
       " ('AACAACCA', '2773084'),\n",
       " ('AACAACCT', '3499990'),\n",
       " ('AACAATGC', '3143862'),\n",
       " ('AACACACA', '0'),\n",
       " ('AACACCTC', '1696057'),\n",
       " ('AACATACT', '1349442'),\n",
       " ('AACATTAC', '165903'),\n",
       " ('AACCAATC', '925154'),\n",
       " ('AACCAGAA', '87640'),\n",
       " ('AACTAACC', '0'),\n",
       " ('AACTATTC', '3859816'),\n",
       " ('AACTCCAA', '1775261'),\n",
       " ('AACTGTCA', '3905561'),\n",
       " ('AAGACATG', '2183427'),\n",
       " ('AAGACCAA', '2883723'),\n",
       " ('AAGACGCT', '837708'),\n",
       " ('AAGACGTC', '812555'),\n",
       " ('AAGATCAC', '307998'),\n",
       " ('AAGATGTA', '3847260'),\n",
       " ('AAGCAATA', '1480841'),\n",
       " ('AAGCCAGA', '1973979'),\n",
       " ('AAGCGCTT', '99975'),\n",
       " ('AAGGAGCA', '3152336'),\n",
       " ('AAGGCCCA', '2253087'),\n",
       " ('AAGGCGCA', '363748'),\n",
       " ('AAGGTAGT', '4142776'),\n",
       " ('AAGTTTGG', '3599231'),\n",
       " ('AATACGTT', '177547'),\n",
       " ('AATAGATA', '2787004'),\n",
       " ('AATAGATC', '4247668'),\n",
       " ('AATAGCCG', '1352287'),\n",
       " ('AATCACTC', '37337'),\n",
       " ('AATCTACA', '159782'),\n",
       " ('AATGTTTG', '970924'),\n",
       " ('AATTAACT', '176268'),\n",
       " ('AATTAACT', '176268'),\n",
       " ('AATTACAT', '3128164'),\n",
       " ('AATTGACT', '0'),\n",
       " ('AATTGCAT', '1043320'),\n",
       " ('AATTTCCC', '3376955'),\n",
       " ('AATTTGAT', '3829208'),\n",
       " ('ACAAACAG', '3254248'),\n",
       " ('ACAAATAA', '3494487'),\n",
       " ('ACAAATTT', '388628'),\n",
       " ('ACAACGAA', '4210229'),\n",
       " ('ACAAGAAT', '2040005'),\n",
       " ('ACAAGAAT', '2040005'),\n",
       " ('ACAAGAAT', '0'),\n",
       " ('ACAAGTGA', '395005'),\n",
       " ('ACAATCCT', '3046336'),\n",
       " ('ACAATGCA', '0'),\n",
       " ('ACACTCTT', '2186689'),\n",
       " ('ACAGAACC', '2933863'),\n",
       " ('ACAGCCCC', '301037'),\n",
       " ('ACAGGATC', '47375'),\n",
       " ('ACAGGTAG', '2122305'),\n",
       " ('ACAGGTTA', '3514410'),\n",
       " ('ACAGTCAT', '280048'),\n",
       " ('ACAGTGGT', '0'),\n",
       " ('ACATAACG', '3546160'),\n",
       " ('ACATAGAA', '231801'),\n",
       " ('ACATCCCC', '2374575'),\n",
       " ('ACATCCGA', '4116895'),\n",
       " ('ACATGCCA', '3667275'),\n",
       " ('ACCAACTA', '941152'),\n",
       " ('ACCAAGAC', '388520'),\n",
       " ('ACCACTCT', '1956286'),\n",
       " ('ACCAGCAG', '3865744'),\n",
       " ('ACCCATCC', '4302845'),\n",
       " ('ACCCATTT', '573414'),\n",
       " ('ACCCTCCT', '1071009'),\n",
       " ('ACCGACGG', '1357292'),\n",
       " ('ACCGACGG', '1357296'),\n",
       " ('ACCGATTT', '438799'),\n",
       " ('ACCGCACA', '3852128'),\n",
       " ('ACCGGACG', '3520380'),\n",
       " ('ACCTACTG', '1537685'),\n",
       " ('ACCTTCCT', '2789051'),\n",
       " ('ACGAAATG', '2601592'),\n",
       " ('ACGACCGA', '2293447'),\n",
       " ('ACGATGAT', '0'),\n",
       " ('ACGCAGTG', '1280025'),\n",
       " ('ACGCCAAC', '890509'),\n",
       " ('ACGCTCCC', '925690'),\n",
       " ('ACGGGGTA', '1208405'),\n",
       " ('ACGGTCAT', '2857059'),\n",
       " ('ACGTTACG', '2700413'),\n",
       " ('ACTAAACC', '1492941'),\n",
       " ('ACTAACTT', '1955656'),\n",
       " ('ACTACCAA', '2357305'),\n",
       " ('ACTACCAT', '3337019'),\n",
       " ('ACTACGCG', '4148395'),\n",
       " ('ACTATCGT', '3006598'),\n",
       " ('ACTATCGT', '3006598'),\n",
       " ('ACTCAATC', '4099789'),\n",
       " ('ACTCGCTA', '3524653'),\n",
       " ('ACTCTATG', '1232032'),\n",
       " ('ACTCTATG', '1232032'),\n",
       " ('ACTCTGCC', '1275900'),\n",
       " ('ACTGTGAT', '2207679'),\n",
       " ('ACTTACAC', '1964214'),\n",
       " ('ACTTCATT', '2017698'),\n",
       " ('AGAAAATC', '1193249'),\n",
       " ('AGAAAGCA', '558545'),\n",
       " ('AGAACTAA', '1773171'),\n",
       " ('AGACCTTT', '500641'),\n",
       " ('AGACTAGA', '2201803'),\n",
       " ('AGACTAGA', '2201803'),\n",
       " ('AGAGAACG', '1916722'),\n",
       " ('AGAGCACC', '1124741'),\n",
       " ('AGAGGCGG', '3723077'),\n",
       " ('AGATAACT', '3619982'),\n",
       " ('AGATACTG', '0'),\n",
       " ('AGATGACA', '1501009'),\n",
       " ('AGATGCAT', '583027'),\n",
       " ('AGCCGACG', '2725768'),\n",
       " ('AGCCTTCC', '0'),\n",
       " ('AGCGCAAC', '2362959'),\n",
       " ('AGCGCCCA', '4223560'),\n",
       " ('AGCGTAAC', '454023'),\n",
       " ('AGCTACAC', '3901948'),\n",
       " ('AGCTACAG', '3958909'),\n",
       " ('AGCTCCTA', '466100'),\n",
       " ('AGCTCGTT', '522787'),\n",
       " ('AGCTCTAA', '369058'),\n",
       " ('AGGAAATG', '896544'),\n",
       " ('AGGCAGCT', '1276930'),\n",
       " ('AGGCTGAC', '3386631'),\n",
       " ('AGTAAAGT', '2195162'),\n",
       " ('AGTAAATA', '3510220'),\n",
       " ('AGTAGCCC', '1004830'),\n",
       " ('AGTCCGAG', '925690'),\n",
       " ('AGTCGAGG', '1352283'),\n",
       " ('AGTCTCGC', '543795'),\n",
       " ('AGTGACTT', '3702549'),\n",
       " ('AGTGGCAT', '1352191'),\n",
       " ('ATAAACGG', '4093935'),\n",
       " ('ATAACGAG', '0'),\n",
       " ('ATAATTTT', '1162150'),\n",
       " ('ATACAACT', '2122205'),\n",
       " ('ATACACGC', '0'),\n",
       " ('ATATCAAA', '4303873'),\n",
       " ('ATATGCTT', '1510039'),\n",
       " ('ATATTCAA', '3444385'),\n",
       " ('ATATTTCT', '0'),\n",
       " ('ATCAACAC', '229915'),\n",
       " ('ATCCACAA', '2015049'),\n",
       " ('ATCCCATA', '2475182'),\n",
       " ('ATCCCTTT', '856144'),\n",
       " ('ATCCGAAC', '918008'),\n",
       " ('ATCCGAAC', '918008'),\n",
       " ('ATCCTCAT', '2788695'),\n",
       " ('ATCCTGTT', '0'),\n",
       " ('ATCCTGTT', '0'),\n",
       " ('ATCGCGGC', '3920426'),\n",
       " ('ATCGGAGA', '2048202'),\n",
       " ('ATCTACAT', '0'),\n",
       " ('ATCTGTGA', '3842501'),\n",
       " ('ATGAATAT', '3375747'),\n",
       " ('ATGATCCT', '3768899'),\n",
       " ('ATGCCCCC', '3784342'),\n",
       " ('ATGGCCAA', '3460259'),\n",
       " ('ATGGCGGG', '2557705'),\n",
       " ('ATGGGCAC', '3268475'),\n",
       " ('ATGGTCAA', '3353319'),\n",
       " ('ATGTAAGG', '3395920'),\n",
       " ('ATGTCAAA', '160624'),\n",
       " ('ATGTCGTG', '3822317'),\n",
       " ('ATTACAAA', '0'),\n",
       " ('ATTACATT', '1669127'),\n",
       " ('ATTACCAA', '1956777'),\n",
       " ('ATTAGGGT', '4315069'),\n",
       " ('ATTCAGTA', '2601625'),\n",
       " ('ATTCCTCG', '2691651'),\n",
       " ('ATTCGTGG', '923833'),\n",
       " ('ATTCGTGG', '923833'),\n",
       " ('ATTGAAGT', '2202647'),\n",
       " ('ATTGAAGT', '2202647'),\n",
       " ('ATTGCGAG', '918006'),\n",
       " ('ATTGTCCG', '548378'),\n",
       " ('ATTTAAGA', '176424'),\n",
       " ('ATTTCCTA', '2883723'),\n",
       " ('ATTTTGGA', '2976310'),\n",
       " ('ATTTTTAC', '2869305'),\n",
       " ('CAAAAAGA', '3069342'),\n",
       " ('CAAAGACG', '3723077'),\n",
       " ('CAAAGATG', '468017'),\n",
       " ('CAAAGATT', '475403'),\n",
       " ('CAAAGTAC', '627749'),\n",
       " ('CAAAGTAT', '573387'),\n",
       " ('CAAAGTTA', '4313507'),\n",
       " ('CAACCGTC', '3288129'),\n",
       " ('CAACGCAC', '514243'),\n",
       " ('CAACTGGC', '2616325'),\n",
       " ('CAAGCAGA', '0'),\n",
       " ('CAAGCCCA', '1517377'),\n",
       " ('CAAGGTTA', '3525866'),\n",
       " ('CAATATCT', '561373'),\n",
       " ('CAATATTG', '3444337'),\n",
       " ('CAATTCCT', '575415'),\n",
       " ('CAATTTAA', '3178462'),\n",
       " ('CACAAGGA', '1711155'),\n",
       " ('CACAATAC', '0'),\n",
       " ('CACACCGC', '4288641'),\n",
       " ('CACAGGTC', '121806'),\n",
       " ('CACATAAT', '2930793'),\n",
       " ('CACATGCC', '1760529'),\n",
       " ('CACCAAAC', '4170071'),\n",
       " ('CACCATTG', '2136794'),\n",
       " ('CACGCAGA', '3368646'),\n",
       " ('CACGCTAT', '1965457'),\n",
       " ('CACGTGAA', '1379737'),\n",
       " ('CACTCACG', '2596621'),\n",
       " ('CACTCTAT', '1234657'),\n",
       " ('CACTTAGG', '2883723'),\n",
       " ('CACTTGGG', '2999533'),\n",
       " ('CAGACCTA', '3722122'),\n",
       " ('CAGATCTA', '2072383'),\n",
       " ('CAGCATCA', '2269079'),\n",
       " ('CAGCCGCA', '2871085'),\n",
       " ('CAGTGACA', '977272'),\n",
       " ('CAGTGGTG', '824174'),\n",
       " ('CAGTGTAC', '57868'),\n",
       " ('CAGTGTTC', '3204130'),\n",
       " ('CATAACCA', '547822'),\n",
       " ('CATAATCG', '4283122'),\n",
       " ('CATACCAA', '4166393'),\n",
       " ('CATACTTT', '2102047'),\n",
       " ('CATATTAG', '2106474'),\n",
       " ('CATCCCCC', '3448247'),\n",
       " ('CATGCTCC', '0'),\n",
       " ('CATGTCAT', '1604510'),\n",
       " ('CATTATCA', '1274093'),\n",
       " ('CATTTTTC', '4066934'),\n",
       " ('CCAAATAT', '2071996'),\n",
       " ('CCAACAGC', '0'),\n",
       " ('CCAACCTA', '2883723'),\n",
       " ('CCAATACC', '3397372'),\n",
       " ('CCACCCTG', '1616130'),\n",
       " ('CCACCGAC', '0'),\n",
       " ('CCACTGCG', '3850241'),\n",
       " ('CCAGCCTT', '2529108'),\n",
       " ('CCATACGA', '1397158'),\n",
       " ('CCATCAAC', '2087073'),\n",
       " ('CCATCGCA', '660680'),\n",
       " ('CCATGGAC', '0'),\n",
       " ('CCCACTTT', '0'),\n",
       " ('CCCAGACT', '2207493'),\n",
       " ('CCCCACCT', '3367581'),\n",
       " ('CCCCTAAC', '645188'),\n",
       " ('CCCCTAAG', '3160898'),\n",
       " ('CCCCTGCT', '1991237'),\n",
       " ('CCCGGCAG', '1675020'),\n",
       " ('CCCTGAAG', '3320660'),\n",
       " ('CCCTGTTT', '853667'),\n",
       " ('CCCTTACT', '3398609'),\n",
       " ('CCGACCCT', '2387889'),\n",
       " ('CCGACTCT', '1307048'),\n",
       " ('CCGACTGA', '31581'),\n",
       " ('CCGCGACC', '943994'),\n",
       " ('CCGGCCAC', '3512077'),\n",
       " ('CCGGGAGA', '2056208'),\n",
       " ('CCGGGTTT', '3831850'),\n",
       " ('CCGTAAGC', '0'),\n",
       " ('CCGTAATG', '3200697'),\n",
       " ('CCTCATCC', '3904998'),\n",
       " ('CCTGATAA', '870742'),\n",
       " ('CCTGCAGT', '2953752'),\n",
       " ('CCTGCGCC', '2086116'),\n",
       " ('CCTTATCC', '2296940'),\n",
       " ('CGAACCTA', '1620795'),\n",
       " ('CGAAGCGG', '2005628'),\n",
       " ('CGAAGTAA', '2294833'),\n",
       " ('CGAATTGC', '1357470'),\n",
       " ('CGAGACAG', '39857'),\n",
       " ('CGAGACAT', '2729375'),\n",
       " ('CGAGACGC', '595088'),\n",
       " ('CGATGACT', '222102'),\n",
       " ('CGCAACTT', '3934575'),\n",
       " ('CGCAAGGC', '0'),\n",
       " ('CGCCAATC', '528347'),\n",
       " ('CGCCACCT', '26121'),\n",
       " ('CGCCAGCA', '2556188'),\n",
       " ('CGCCGAAC', '2674221'),\n",
       " ('CGCCGTAC', '852818'),\n",
       " ('CGCGTACG', '3540539'),\n",
       " ('CGCTAAGG', '2883723'),\n",
       " ('CGCTCCTC', '4053752'),\n",
       " ('CGGACACT', '162598'),\n",
       " ('CGGCCCGT', '420404'),\n",
       " ('CGGCGTCA', '1209806'),\n",
       " ('CGGCGTCA', '1209806'),\n",
       " ('CGGGCTTT', '2179617'),\n",
       " ('CGGTCAAT', '2367435'),\n",
       " ('CGGTTGGC', '3272799'),\n",
       " ('CGTAATAA', '551845'),\n",
       " ('CGTAATAA', '551845'),\n",
       " ('CGTATCCA', '4053752'),\n",
       " ('CGTCGCAA', '695889'),\n",
       " ('CGTGACAA', '3912539'),\n",
       " ('CGTGCCCG', '3003340'),\n",
       " ('CGTGCCCG', '3003340'),\n",
       " ('CGTGGGAG', '2191082'),\n",
       " ('CGTTGTTT', '268158'),\n",
       " ('CTAACAGT', '3462593'),\n",
       " ('CTAAGCGG', '3769375'),\n",
       " ('CTACGGAC', '2883723'),\n",
       " ('CTACTTAA', '647077'),\n",
       " ('CTAGCACT', '3828970'),\n",
       " ('CTAGCACT', '3828970'),\n",
       " ('CTAGTATG', '4141420'),\n",
       " ('CTCACTTC', '2700413'),\n",
       " ('CTCACTTC', '2700413'),\n",
       " ('CTCATCTA', '1314045'),\n",
       " ('CTCATCTA', '1314045'),\n",
       " ('CTCCCACT', '2476770'),\n",
       " ('CTCCCCCC', '3268312'),\n",
       " ('CTCCCTAC', '2349218'),\n",
       " ('CTCGAAAA', '3779515'),\n",
       " ('CTCGACGC', '496032'),\n",
       " ('CTCGCCGT', '0'),\n",
       " ('CTCGGCGA', '3651635'),\n",
       " ('CTCGTAAA', '1994326'),\n",
       " ('CTCGTGAC', '4027272'),\n",
       " ('CTGCAAAA', '1539502'),\n",
       " ('CTGCAAAA', '1539502'),\n",
       " ('CTGCGTAA', '3510905'),\n",
       " ('CTGCTCCT', '2323670'),\n",
       " ('CTTAGGCG', '3745192'),\n",
       " ('CTTCGTGG', '3926596'),\n",
       " ('CTTGCCCC', '1748657'),\n",
       " ('CTTGCTCC', '2551825'),\n",
       " ('CTTTAAAC', '2725529'),\n",
       " ('GAAACAAC', '2630679'),\n",
       " ('GAAACCAC', '4334043'),\n",
       " ('GAAACCAG', '1281722'),\n",
       " ('GAAAGCTA', '190977'),\n",
       " ('GAAATGTT', '491869'),\n",
       " ('GAACTATT', '4168829'),\n",
       " ('GAAGATAA', '0'),\n",
       " ('GAAGCCTA', '651696'),\n",
       " ('GAAGCTAT', '3521561'),\n",
       " ('GAAGGTCC', '1549263'),\n",
       " ('GAATAAAT', '1019979'),\n",
       " ('GAATATTT', '1779433'),\n",
       " ('GAATGTGA', '1878101'),\n",
       " ('GAATTCTT', '223131'),\n",
       " ('GAATTTAG', '2800426'),\n",
       " ('GACAAAAT', '2534983'),\n",
       " ('GACACAGC', '3575760'),\n",
       " ('GACACGAG', '1066421'),\n",
       " ('GACCAAGC', '51951'),\n",
       " ('GACCACAA', '2981237'),\n",
       " ('GACCAGTG', '702977'),\n",
       " ('GACCCTGC', '1741622'),\n",
       " ('GACCTAAT', '1923751'),\n",
       " ('GACCTGAA', '0'),\n",
       " ('GACCTTGT', '4234348'),\n",
       " ('GACCTTGT', '4234348'),\n",
       " ('GACGCCCA', '3238510'),\n",
       " ('GACGCGAA', '3237028'),\n",
       " ('GACGGAGA', '2046725'),\n",
       " ('GACGGCCC', '631073'),\n",
       " ('GACTAATT', '3796239'),\n",
       " ('GACTATAT', '3393264'),\n",
       " ('GACTGTGA', '2821958'),\n",
       " ('GACTTCGT', '2916736'),\n",
       " ('GAGAACCC', '138138'),\n",
       " ('GAGACTAG', '3859816'),\n",
       " ('GAGAGACA', '2425313'),\n",
       " ('GAGATTTG', '3721209'),\n",
       " ('GAGGGACA', '0'),\n",
       " ('GAGGGCTC', '1276944'),\n",
       " ('GAGGGTGC', '3863850'),\n",
       " ('GATATATA', '2762603'),\n",
       " ('GATATCAC', '0'),\n",
       " ('GATCACAC', '2074888'),\n",
       " ('GATCACCT', '4283460'),\n",
       " ('GATCCGCT', '341987'),\n",
       " ('GATTACGT', '382825'),\n",
       " ('GCAACCAA', '188101'),\n",
       " ('GCAATCTA', '0'),\n",
       " ('GCACACAC', '3038147'),\n",
       " ('GCACGCAA', '2677761'),\n",
       " ('GCACTAAC', '2083303'),\n",
       " ('GCACTTGT', '2609284'),\n",
       " ('GCATAACA', '1693625'),\n",
       " ('GCATGAAA', '3669913'),\n",
       " ('GCATTCCG', '2187030'),\n",
       " ('GCCATCAC', '737482'),\n",
       " ('GCCCAAAG', '4069407'),\n",
       " ('GCCCAAGG', '0'),\n",
       " ('GCCCAATT', '1971399'),\n",
       " ('GCCCACGC', '0'),\n",
       " ('GCCGACAA', '752423'),\n",
       " ('GCCGCCCT', '1111361'),\n",
       " ('GCCGCCCT', '1111361'),\n",
       " ('GCCGCCCT', '1111361'),\n",
       " ('GCCGGCGG', '1935084'),\n",
       " ('GCCTCGGC', '2369829'),\n",
       " ('GCCTTAAA', '2332345'),\n",
       " ('GCGAAGAG', '3657895'),\n",
       " ('GCGCGCAT', '0'),\n",
       " ('GCGCTATG', '2354053'),\n",
       " ('GCGTCGAC', '992103'),\n",
       " ('GCGTCGAC', '992103'),\n",
       " ('GCTAGACC', '4262890'),\n",
       " ('GCTCAGGA', '3670104'),\n",
       " ('GCTCCTGG', '1778019'),\n",
       " ('GGAATTCT', '878309'),\n",
       " ('GGACACGA', '4312798'),\n",
       " ('GGACAGCC', '3251794'),\n",
       " ('GGACCCTG', '1778730'),\n",
       " ('GGATAACG', '3852657'),\n",
       " ('GGATGTAC', '364277'),\n",
       " ('GGCAAACA', '4216050'),\n",
       " ('GGCACGAC', '154949'),\n",
       " ('GGCACTAA', '384582'),\n",
       " ('GGCATGTT', '2149568'),\n",
       " ('GGCCACTT', '2084754'),\n",
       " ('GGCCCAAT', '122870'),\n",
       " ('GGCCGCCC', '373583'),\n",
       " ('GGCGCGAA', '1166620'),\n",
       " ('GGCTAATC', '3566053'),\n",
       " ('GGCTTTTA', '1165724'),\n",
       " ('GGGCCTAT', '3638327'),\n",
       " ('GGGCTAAA', '0'),\n",
       " ('GGGGGGGG', '533657'),\n",
       " ('GGGGGGGG', '573428'),\n",
       " ('GGGGGGGG', '642706'),\n",
       " ('GGGGGGGG', '754840'),\n",
       " ('GGGGGGGG', '758941'),\n",
       " ('GGGGGGGG', '1124741'),\n",
       " ('GGGGGGGG', '1210940'),\n",
       " ('GGGGGGGG', '1283827'),\n",
       " ('GGGGGGGG', '1535492'),\n",
       " ('GGGGGGGG', '1598628'),\n",
       " ('GGGGGGGG', '2733020'),\n",
       " ('GGGGGGGG', '2744873'),\n",
       " ('GGGGGGGG', '3140337'),\n",
       " ('GGGGGGTG', '3254215'),\n",
       " ('GGGGTGCG', '4120066'),\n",
       " ('GGGGTGGT', '2730604'),\n",
       " ('GGTAAATC', '920700'),\n",
       " ('GGTAACTT', '4200966'),\n",
       " ('GGTATAAT', '3380597'),\n",
       " ('GGTATTAG', '3055285'),\n",
       " ('GGTGGCAG', '3002083'),\n",
       " ('GGTTCAAC', '3465963'),\n",
       " ('GGTTCAAC', '3465963'),\n",
       " ('GGTTCAAC', '3465963'),\n",
       " ('GGTTTACC', '0'),\n",
       " ('GTAAAAGA', '3896893'),\n",
       " ('GTAAATTT', '3569834'),\n",
       " ('GTAATAAA', '1531619'),\n",
       " ('GTAATCAG', '3527076'),\n",
       " ('GTAATGCA', '800222'),\n",
       " ('GTACATGC', '3096307'),\n",
       " ('GTACCGAT', '1779433'),\n",
       " ('GTATGAGC', '2819624'),\n",
       " ('GTATTACT', '1281000'),\n",
       " ('GTCCAACT', '3254215'),\n",
       " ('GTCCACAC', '4235791'),\n",
       " ('GTCCGTAA', '4279411'),\n",
       " ('GTCCGTTA', '938819'),\n",
       " ('GTCTAATT', '3523385'),\n",
       " ('GTCTACTT', '2193045'),\n",
       " ('GTCTCAAC', '3278585'),\n",
       " ('GTGACCCA', '169495'),\n",
       " ('GTGATAAC', '3416610'),\n",
       " ('GTGCACTG', '833502'),\n",
       " ('GTGCATGA', '925692'),\n",
       " ('GTGCCGAG', '3969507'),\n",
       " ('GTTAAGAC', '3681921'),\n",
       " ('GTTATTTT', '1400712'),\n",
       " ('GTTATTTT', '1400712'),\n",
       " ('GTTCATAT', '3337936'),\n",
       " ('GTTGGATA', '642706'),\n",
       " ('GTTTCATG', '0'),\n",
       " ('GTTTTACT', '1985715'),\n",
       " ('TAAAACTC', '3768560'),\n",
       " ('TAAAATCA', '165864'),\n",
       " ('TAAAATTA', '2816170'),\n",
       " ('TAAAGCAG', '0'),\n",
       " ('TAAAGTGC', '3263053'),\n",
       " ('TAAAGTGC', '3263053'),\n",
       " ('TAAATAGT', '3732352'),\n",
       " ('TAACAGTT', '0'),\n",
       " ('TAACATGA', '2954618'),\n",
       " ('TAACGGAA', '715727'),\n",
       " ('TAACGGCA', '2195180'),\n",
       " ('TAACTGCT', '0'),\n",
       " ('TAAGGTAC', '2597824'),\n",
       " ('TAAGTCCA', '1604277'),\n",
       " ('TAATCAAC', '2028715'),\n",
       " ('TAATCAGC', '2314015'),\n",
       " ('TAATCAGC', '2314015'),\n",
       " ('TAATCATC', '2887188'),\n",
       " ('TAATGCCA', '695523'),\n",
       " ('TAATGTCC', '3158760'),\n",
       " ('TAATGTTA', '738583'),\n",
       " ('TACCATGA', '2861954'),\n",
       " ('TACGATAC', '364282'),\n",
       " ('TACGCTGT', '991250'),\n",
       " ('TACTAATT', '1031012'),\n",
       " ('TACTACAA', '2967757'),\n",
       " ('TACTACAA', '0'),\n",
       " ('TACTTGAC', '3731089'),\n",
       " ('TACTTGCT', '2017115'),\n",
       " ('TACTTTTC', '108988'),\n",
       " ('TACTTTTC', '108988'),\n",
       " ('TAGAACCA', '2597440'),\n",
       " ('TAGACTCA', '2181115'),\n",
       " ('TAGCACCC', '0'),\n",
       " ('TAGCATAA', '454023'),\n",
       " ('TAGGCCGT', '134437'),\n",
       " ('TAGGCCGT', '134437'),\n",
       " ('TAGTCAAG', '1369436'),\n",
       " ('TAGTGTGT', '3718413'),\n",
       " ('TATACAGA', '1118720'),\n",
       " ('TATGATCT', '3373687'),\n",
       " ('TATTAATT', '3871092'),\n",
       " ('TCAAAATG', '934228'),\n",
       " ('TCAACAGC', '921005'),\n",
       " ('TCAATTAC', '3586266'),\n",
       " ('TCAATTGG', '3544928'),\n",
       " ('TCACAATG', '3511515'),\n",
       " ('TCACAGGT', '4200957'),\n",
       " ('TCACATGC', '1420823'),\n",
       " ('TCACCAAG', '0'),\n",
       " ('TCACCTGT', '2896113'),\n",
       " ('TCACTTAC', '4294475'),\n",
       " ('TCACTTCA', '361207'),\n",
       " ('TCATGCAA', '2857505'),\n",
       " ('TCATGTTA', '1872929'),\n",
       " ('TCATTCAC', '1349985'),\n",
       " ('TCATTCAT', '1057494'),\n",
       " ('TCCAAATA', '439737'),\n",
       " ('TCCACCCC', '1483152'),\n",
       " ('TCCACTCT', '0'),\n",
       " ('TCCATAAG', '4032275'),\n",
       " ('TCCATTCC', '0'),\n",
       " ('TCCCGAAT', '528347'),\n",
       " ('TCCGACAC', '3868104'),\n",
       " ('TCGAAAAA', '3343507'),\n",
       " ('TCGATCGC', '1331494'),\n",
       " ('TCGCAAAT', '1115790'),\n",
       " ('TCGCGATA', '3917018'),\n",
       " ('TCGCTCCC', '2367435'),\n",
       " ('TCGTCCAG', '854455'),\n",
       " ('TCGTCTTA', '3884021'),\n",
       " ('TCGTTGAC', '2335410'),\n",
       " ('TCGTTGAC', '2335410'),\n",
       " ('TCTAAAAA', '941152'),\n",
       " ('TCTAACGC', '3970721'),\n",
       " ('TCTACTAT', '0'),\n",
       " ('TCTATGAC', '0'),\n",
       " ('TCTCAACA', '2535730'),\n",
       " ('TCTGCTAA', '1991193'),\n",
       " ('TCTGGCCA', '1117262'),\n",
       " ('TGAAGTGA', '3998427'),\n",
       " ('TGACAAGC', '2672094'),\n",
       " ('TGACATAT', '389473'),\n",
       " ('TGACATCT', '1041105'),\n",
       " ('TGACATGA', '0'),\n",
       " ('TGATAAAA', '1708144'),\n",
       " ('TGATCGCC', '2911389'),\n",
       " ('TGATCTTA', '583027'),\n",
       " ('TGCACTTA', '1850708'),\n",
       " ('TGCATTAA', '3719320'),\n",
       " ('TGCCCCTT', '1357857'),\n",
       " ('TGCCGAAC', '331336'),\n",
       " ('TGCCTTCC', '1287953'),\n",
       " ('TGCGTCTC', '0'),\n",
       " ('TGCGTGCT', '2916021'),\n",
       " ('TGCTAAAA', '572609'),\n",
       " ('TGCTCACT', '3623533'),\n",
       " ('TGCTCACT', '3623533'),\n",
       " ('TGCTTGTG', '2354053'),\n",
       " ('TGGACCCA', '3082057'),\n",
       " ('TGGTTCAA', '1912477'),\n",
       " ('TGTAGATG', '0'),\n",
       " ('TGTCAGTA', '3444337'),\n",
       " ('TGTCCATC', '2426612'),\n",
       " ('TGTTCCAG', '87311'),\n",
       " ('TGTTCCTG', '3045250'),\n",
       " ('TGTTGTCT', '0'),\n",
       " ('TTAACAAC', '3722332'),\n",
       " ('TTAAGGGA', '4108672'),\n",
       " ('TTAATCTA', '3804604'),\n",
       " ('TTACCTGC', '3672454'),\n",
       " ('TTACGCAC', '1950073'),\n",
       " ('TTAGACAT', '181212'),\n",
       " ('TTATAACG', '3673147'),\n",
       " ('TTATGATT', '1121348'),\n",
       " ('TTCAACCC', '1302082'),\n",
       " ('TTCAACGC', '0'),\n",
       " ('TTCACGTC', '394906'),\n",
       " ('TTCCAAAA', '253845'),\n",
       " ('TTCCAGCA', '1307284'),\n",
       " ('TTCCATTT', '580902'),\n",
       " ('TTCCCACT', '3584406'),\n",
       " ('TTCCTCTA', '0'),\n",
       " ('TTCGTACT', '223235'),\n",
       " ('TTCGTGCA', '2286455'),\n",
       " ('TTGCAATT', '1779433'),\n",
       " ('TTGCACGA', '1994898'),\n",
       " ('TTGCATTC', '0'),\n",
       " ('TTGTGCTA', '592636'),\n",
       " ('TTTAAGCG', '2978486'),\n",
       " ('TTTATTGC', '1295441'),\n",
       " ('TTTCGTAG', '2318816'),\n",
       " ('TTTCTCTT', '547791'),\n",
       " ('TTTTAACT', '1731629'),\n",
       " ('TTTTAATT', '1536283'),\n",
       " ('TTTTAATT', '1536283'),\n",
       " ('TTTTAATT', '1536283'),\n",
       " ('TTTTTCCA', '3697348')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_mapped_reads(\"sorted_reads/test_5000_R1.sam\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Filter for PCR amplification reads--reduce to single template counts (do at same time as finding transposon tag)\n",
    "\n",
    "Will I be able to use the start position to do this?\n",
    "\n",
    "Need to use new program since barcode now in different place.\n",
    "\n",
    "1) function to identify barcode and start of each read\n",
    "2) when found good_read --> apply function and compare barcode:start to list\n",
    "3) if not in list --> append to good_reads, else to bad_reads\n",
    "\n",
    "Need to parse sam flag--position is strand specific (sam flag is 2nd field (index=1) and 0 is forward, 16 is reverse). Start position is left-most position of alignment, so for forward it is the start, but for reverse it is the end. To get reverse start, have to add to position the length of the alignment. This is given in the CIGAR string (6th field, so index=5). \n",
    "\n",
    "reverse strand (samflag=16): 118M33S  so first number up to 'M' is length of match (33S is softclip of transposon seq)\n",
    "forward strand (samflag=0): 33S118M so number AFTER S. But this wont need to be parsed, as left-most position is correct for forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse samflag \n",
    "\n",
    "def parse_samflag(read):\n",
    "    \"\"\"\n",
    "    Function to determine strand orientation of alignment from read header\n",
    "    \"\"\"\n",
    "    samflag = read.split()[1]\n",
    "    if samflag==\"16\":\n",
    "        strand = \"R\"\n",
    "    elif samflag==\"0\":\n",
    "        strand = \"F\"\n",
    "    else:\n",
    "        strand = \"*\"\n",
    "    return strand\n",
    "\n",
    "def align_len(read):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to determine read alignment length from start position of reads \n",
    "    filtered for transposon tag (should have soft-clipped ~30-33 before gDNA)\n",
    "\n",
    "    Some cases may exist like 29S89M33S--transposon at both ends? or adapter not trimmed? \n",
    "    if forward, can ignore last part of alignment, if reverse, can ignore first part\n",
    "    \n",
    "    \"\"\"\n",
    "    import re\n",
    "    cig = read.split()[5]\n",
    "    cig_list = parse_cigar(cig)\n",
    "    cig_last = len(cig_list) - 1\n",
    "    strand = parse_samflag(read)\n",
    "    if strand == \"R\":\n",
    "        # assume alignment without extra bases at 3' end \n",
    "        #make sure no soft-clipping at start\n",
    "        #if 'M' in cig_list[0]:  #this could be 3' end alignment\n",
    "        if \"S\" in cig_list[cig_last]:  #check for soft-clipping (tag) at 5' end of read\n",
    "            match_len = cig_list[cig_last - 1] #aligned part is next part\n",
    "            #soft_clip = cig_list[cig_last]\n",
    "            match_len = int(match_len.split(\"M\")[0])\n",
    "        else:\n",
    "            match_len = 0\n",
    "        \n",
    "    elif strand == \"F\":\n",
    "        #for F strand, softclipped bases at 3' end are cigar_list[2] and can be ignored?, if no soft-clipping at 5' end, will be no tag\n",
    "        match_len = cig_list[1]\n",
    "        match_len = int(match_len.split(\"M\")[0])\n",
    "        #soft_clip = cig_list[0]\n",
    "    else:\n",
    "        match_len = 0\n",
    "    return match_len\n",
    "\n",
    "def rev_start(pos, alignlen):\n",
    "    \"\"\"\n",
    "    Function to find start position for reverse aligned reads\n",
    "    \"\"\"\n",
    "    rev_start = int(pos) + alignlen\n",
    "    return rev_start\n",
    "\n",
    "def parse_cigar(cigar):\n",
    "    \"\"\"\n",
    "    Parse cigar string \n",
    "    regex: \\*|([0-9]+[MIDNSHPX=])+\n",
    "    \"\"\"\n",
    "    import re\n",
    "    #keep delimiter\n",
    "    #cig_list = re.split('([0-9]+M|[0-9]+S)', cigar)\n",
    "    #remove empty strings\n",
    "    #cig_list = [i for i in cig_list if i]\n",
    "\n",
    "    #find all number/letter combos in cigar string\n",
    "    cig_list = re.findall(r'[0-9]+[MIDNSHPX=]+', cigar)\n",
    "    return cig_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "109\n",
      "118\n",
      "95\n",
      "118\n",
      "69\n",
      "118\n",
      "79\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "121\n",
      "121\n",
      "116\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "72\n",
      "118\n",
      "119\n",
      "118\n",
      "113\n",
      "118\n",
      "114\n",
      "118\n",
      "88\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "89\n",
      "89\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "85\n",
      "118\n",
      "118\n",
      "87\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "111\n",
      "85\n",
      "118\n",
      "118\n",
      "86\n",
      "94\n",
      "118\n",
      "110\n",
      "118\n",
      "107\n",
      "118\n",
      "119\n",
      "118\n",
      "48\n",
      "93\n",
      "101\n",
      "118\n",
      "87\n",
      "93\n",
      "118\n",
      "119\n",
      "118\n",
      "93\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "47\n",
      "116\n",
      "116\n",
      "92\n",
      "59\n",
      "34\n",
      "118\n",
      "31\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "109\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "75\n",
      "118\n",
      "101\n",
      "122\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "91\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "118\n",
      "107\n",
      "118\n",
      "118\n",
      "119\n",
      "35\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "67\n",
      "118\n",
      "118\n",
      "113\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "109\n",
      "118\n",
      "118\n",
      "53\n",
      "97\n",
      "118\n",
      "85\n",
      "119\n",
      "105\n",
      "118\n",
      "85\n",
      "118\n",
      "118\n",
      "71\n",
      "77\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "59\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "115\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "77\n",
      "62\n",
      "118\n",
      "37\n",
      "118\n",
      "77\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "70\n",
      "118\n",
      "118\n",
      "73\n",
      "118\n",
      "118\n",
      "42\n",
      "45\n",
      "118\n",
      "118\n",
      "121\n",
      "118\n",
      "118\n",
      "118\n",
      "54\n",
      "118\n",
      "118\n",
      "80\n",
      "119\n",
      "59\n",
      "112\n",
      "118\n",
      "118\n",
      "113\n",
      "102\n",
      "75\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "47\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "114\n",
      "117\n",
      "118\n",
      "118\n",
      "104\n",
      "102\n",
      "52\n",
      "102\n",
      "118\n",
      "96\n",
      "60\n",
      "118\n",
      "118\n",
      "118\n",
      "55\n",
      "118\n",
      "110\n",
      "132\n",
      "132\n",
      "118\n",
      "66\n",
      "102\n",
      "118\n",
      "111\n",
      "120\n",
      "118\n",
      "83\n",
      "87\n",
      "87\n",
      "118\n",
      "63\n",
      "118\n",
      "54\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "36\n",
      "142\n",
      "118\n",
      "108\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "113\n",
      "118\n",
      "34\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "37\n",
      "34\n",
      "118\n",
      "108\n",
      "108\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "120\n",
      "104\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "105\n",
      "118\n",
      "118\n",
      "118\n",
      "113\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "79\n",
      "79\n",
      "118\n",
      "118\n",
      "118\n",
      "95\n",
      "119\n",
      "118\n",
      "73\n",
      "83\n",
      "32\n",
      "119\n",
      "74\n",
      "70\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "103\n",
      "118\n",
      "89\n",
      "71\n",
      "71\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "104\n",
      "118\n",
      "118\n",
      "53\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "71\n",
      "108\n",
      "118\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "68\n",
      "68\n",
      "118\n",
      "118\n",
      "115\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "108\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "107\n",
      "65\n",
      "119\n",
      "119\n",
      "37\n",
      "118\n",
      "39\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "99\n",
      "99\n",
      "118\n",
      "42\n",
      "118\n",
      "118\n",
      "107\n",
      "118\n",
      "118\n",
      "118\n",
      "63\n",
      "121\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "76\n",
      "118\n",
      "54\n",
      "96\n",
      "73\n",
      "118\n",
      "118\n",
      "99\n",
      "118\n",
      "119\n",
      "89\n",
      "87\n",
      "120\n",
      "103\n",
      "119\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "105\n",
      "118\n",
      "110\n",
      "118\n",
      "99\n",
      "104\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "107\n",
      "113\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "101\n",
      "118\n",
      "48\n",
      "96\n",
      "118\n",
      "89\n",
      "118\n",
      "118\n",
      "103\n",
      "118\n",
      "118\n",
      "118\n",
      "49\n",
      "118\n",
      "118\n",
      "118\n",
      "117\n",
      "118\n",
      "45\n",
      "118\n",
      "59\n",
      "103\n",
      "113\n",
      "118\n",
      "114\n",
      "118\n",
      "122\n",
      "118\n",
      "118\n",
      "114\n",
      "96\n",
      "118\n",
      "118\n",
      "118\n",
      "91\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "64\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "113\n",
      "118\n",
      "119\n",
      "118\n",
      "48\n",
      "118\n",
      "118\n",
      "92\n",
      "118\n",
      "118\n",
      "118\n",
      "82\n",
      "118\n",
      "118\n",
      "118\n",
      "75\n",
      "118\n",
      "118\n",
      "118\n",
      "114\n",
      "118\n",
      "69\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "89\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "69\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "63\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "60\n",
      "72\n",
      "72\n",
      "36\n",
      "36\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "46\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "35\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "40\n",
      "113\n",
      "38\n",
      "118\n",
      "118\n",
      "118\n",
      "92\n",
      "63\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "92\n",
      "74\n",
      "118\n",
      "118\n",
      "83\n",
      "118\n",
      "118\n",
      "118\n",
      "64\n",
      "118\n",
      "75\n",
      "75\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "118\n",
      "73\n",
      "118\n",
      "52\n",
      "118\n",
      "118\n",
      "35\n",
      "114\n",
      "118\n",
      "79\n",
      "119\n",
      "118\n",
      "32\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "62\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "84\n",
      "118\n",
      "118\n",
      "58\n",
      "105\n",
      "118\n",
      "90\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "80\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "111\n",
      "118\n",
      "118\n",
      "118\n",
      "74\n",
      "59\n",
      "118\n",
      "119\n",
      "118\n",
      "114\n",
      "93\n",
      "66\n",
      "107\n",
      "90\n",
      "118\n",
      "118\n",
      "118\n",
      "85\n",
      "120\n",
      "118\n",
      "48\n",
      "118\n",
      "99\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "92\n",
      "119\n",
      "53\n",
      "47\n",
      "46\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "120\n",
      "118\n",
      "118\n",
      "120\n",
      "118\n",
      "118\n",
      "43\n",
      "68\n",
      "105\n",
      "118\n",
      "118\n",
      "118\n",
      "121\n",
      "118\n",
      "118\n",
      "111\n",
      "83\n",
      "118\n",
      "95\n",
      "106\n",
      "118\n",
      "118\n",
      "48\n",
      "107\n",
      "118\n",
      "119\n",
      "118\n",
      "112\n",
      "120\n",
      "118\n",
      "40\n",
      "42\n",
      "121\n",
      "99\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "104\n",
      "118\n",
      "40\n",
      "100\n",
      "100\n",
      "41\n",
      "41\n",
      "95\n",
      "40\n",
      "118\n",
      "54\n",
      "106\n",
      "118\n",
      "49\n",
      "33\n",
      "118\n",
      "118\n",
      "118\n",
      "33\n",
      "85\n",
      "118\n",
      "105\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "57\n",
      "118\n",
      "118\n",
      "47\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "83\n",
      "118\n",
      "119\n",
      "118\n",
      "103\n",
      "119\n",
      "118\n",
      "120\n",
      "118\n",
      "113\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "119\n",
      "89\n",
      "119\n",
      "118\n",
      "76\n",
      "97\n",
      "118\n",
      "118\n",
      "36\n",
      "118\n",
      "118\n",
      "73\n",
      "119\n",
      "119\n",
      "92\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "105\n",
      "103\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "76\n",
      "118\n",
      "118\n",
      "100\n",
      "118\n",
      "118\n",
      "115\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "103\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "40\n",
      "118\n",
      "118\n",
      "84\n",
      "31\n",
      "119\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "55\n",
      "118\n",
      "118\n",
      "72\n",
      "72\n",
      "72\n",
      "118\n",
      "67\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "115\n",
      "113\n",
      "99\n",
      "118\n",
      "95\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "68\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "97\n",
      "118\n",
      "118\n",
      "118\n",
      "101\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "40\n",
      "118\n",
      "106\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "57\n",
      "67\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "59\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "113\n",
      "118\n",
      "113\n",
      "54\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "74\n",
      "118\n",
      "58\n",
      "70\n",
      "42\n",
      "74\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "100\n",
      "118\n",
      "118\n",
      "101\n",
      "118\n",
      "118\n",
      "118\n",
      "84\n",
      "118\n",
      "118\n",
      "89\n",
      "107\n",
      "103\n",
      "118\n",
      "120\n",
      "119\n",
      "118\n",
      "118\n",
      "62\n",
      "35\n",
      "118\n",
      "44\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "87\n",
      "66\n",
      "109\n",
      "95\n",
      "95\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "41\n",
      "41\n",
      "41\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "115\n",
      "115\n",
      "118\n",
      "85\n",
      "97\n",
      "59\n",
      "81\n",
      "47\n",
      "47\n",
      "118\n",
      "118\n",
      "54\n",
      "118\n",
      "118\n",
      "53\n",
      "93\n",
      "83\n",
      "54\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "118\n",
      "77\n",
      "118\n",
      "118\n",
      "118\n",
      "39\n",
      "118\n",
      "91\n",
      "109\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "55\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "65\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "113\n",
      "52\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "104\n",
      "75\n",
      "118\n",
      "71\n",
      "118\n",
      "118\n",
      "118\n",
      "61\n",
      "118\n",
      "118\n",
      "68\n",
      "118\n",
      "118\n",
      "113\n",
      "79\n",
      "74\n",
      "95\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "114\n",
      "72\n",
      "76\n",
      "119\n",
      "64\n",
      "118\n",
      "118\n",
      "118\n",
      "97\n",
      "46\n",
      "118\n",
      "58\n",
      "109\n",
      "54\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "85\n",
      "118\n",
      "118\n",
      "91\n",
      "91\n",
      "118\n",
      "61\n",
      "98\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "104\n",
      "118\n",
      "54\n",
      "118\n",
      "118\n",
      "77\n",
      "77\n",
      "47\n",
      "108\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "65\n",
      "118\n",
      "118\n",
      "69\n",
      "106\n",
      "92\n",
      "118\n",
      "111\n",
      "118\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "#look at align lengths that are less than 50, all aligned lengths > 31\n",
    "\n",
    "from Menadione_tnseq.ipynb:pylance-notebook-cell:Y144sZmlsZQ== import read_samfile\n",
    "\n",
    "\n",
    "sf = read_samfile(\"sorted_reads/mapped_test_5000_R1.sam\")[1]\n",
    "for read in sf:\n",
    "    x = align_len(read)\n",
    "    print(x)\n",
    "  \n",
    "    # x_al = x[0]\n",
    "    # x_sc = x[1]\n",
    "    # sc_num = int(x_sc.split(\"S\")[0])\n",
    "    # if sc_num < 30:\n",
    "    # #if \"S\" not in x_sc:\n",
    "    #     print(x_al, x_sc)\n",
    "    #     print(read)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All aligned lengths are greater than 31. All the ones checked appear to have transposon tag in the soft-clipped region. Could it be more efficient to just check this region for transposon tag in order to filter? This is less reliable because instead of sc, sometimes get D or other tags. Need to check later on that these aren't causing problems with align-length\n",
    "\n",
    "A01968:63:H77VYDSX5:4:1101:3314:1626_1:N:0:AACGTGAT_BC:ATCGTGAT\t16\tNC_002945.3\t3002949\t60\t78M1D40M33S\t*\t0\t0\tGCGCAGGTGTTGAACACCACGACGTCGGCCTCGGAACCGTCGGTCGCCCTCCGGTAGCCGGCCGCTTCCAGCAGACCCCCAGCCGCTCGGAGTCGTGGACGTTCATCTGACAGCCG*TAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\t\n",
    "\n",
    "In this case, on reverse strand, we want the most 5' aligned region to get start, so this will be closest to the soft clip. So we need to indicate the len(cigar_list)-1 to be the align len, NOT the 1st field. Have tested, and it works for this case, but this is only case in test file that has the D\n",
    "\n",
    "Shorter sc seem to indicated truncated transposon without the -GTTA, but is this reliable for narrowing search region for tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samfile(samfile):\n",
    "    \"\"\"\n",
    "    read sam_file and sort lines between header and reads\n",
    "    \"\"\"\n",
    "\n",
    "    header = []\n",
    "    reads  = []\n",
    "\n",
    "    with open(samfile, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line[0] == \"@\":\n",
    "                header.append(line)\n",
    "            else:\n",
    "                reads.append(line)\n",
    "    return header, reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = read_samfile(\"sorted_reads/mapped_test_5000_R1.sam\")[1]\n",
    "for read in heads:\n",
    "    sf = parse_samflag(read)\n",
    "    if sf not in [\"R\", \"F\"]:\n",
    "        print(sf)\n",
    "        print(read)\n",
    "        al = align_len(read)\n",
    "        print(al)\n",
    "# position = heads.split()[3]\n",
    "#bc_start = find_barcodes(heads)\n",
    "#print(bc_start)\n",
    "# rs = rev_start(position, al)\n",
    "# print(rs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re-do find_barcodes, find_tags and filter_mapped_reads to incorporate these functions and find correct start positions. Alignment length can be used to show if transposon tag in first part of read--aligned length needs to be >110 for it to be in first/last 40 bases regardless of strand (unlikely to be at end of aligned read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmfind1(G,n,H,m,max): # lengths; assume n>m\n",
    "  \"\"\"\n",
    "  A function to find matches of transposon sequence in read\n",
    "    Input               G                           sequence string\n",
    "                        n                           length of read sequence\n",
    "                        H                           pattern string\n",
    "                        m                           length of pattern\n",
    "                        max                         maximum mismatches\n",
    "    Output              i, -1                       start position of match, or -1 for no match\n",
    "  \"\"\"\n",
    "\n",
    "  a = G[:n].find(H[:m])\n",
    "  if a!=-1: return a # shortcut for perfect matches\n",
    "  for i in range(0,n-m):  # range of 0 to difference between seq length and len pattern\n",
    "    cnt = 0\n",
    "    for k in range(m):\n",
    "      if G[i+k]!=H[k]: cnt += 1\n",
    "      if cnt>max: break\n",
    "    if cnt<=max: return i\n",
    "  return -1\n",
    "\n",
    "def find_barcodes_starts(read):\n",
    "    \"\"\"\n",
    "    Find barcode and read alignment start position\n",
    "    Returns tuple of barcode and read start pos\n",
    "    \"\"\"\n",
    "    read_name  = read.split()[0]\n",
    "    barcode    = read_name.split(\"BC:\",1)[1]\n",
    "    rd_start = find_start(read)\n",
    "    bar_start = (barcode, rd_start) \n",
    "    return bar_start\n",
    "\n",
    "\n",
    "def find_start(read):\n",
    "    \"\"\"\n",
    "    Parse read for samflag (strand), start of read position and cigar string for alignment length\n",
    "    Returns aligned start position. Use this on filtered 'good reads' to identify start for quantification\n",
    "    \"\"\"\n",
    "    strand   = parse_samflag(read)\n",
    "    al_len   = align_len(read)\n",
    "    left_pos = read.split()[3]\n",
    "    if al_len > 0 and strand != \"*\":\n",
    "      if strand == \"F\":\n",
    "        read_start = left_pos\n",
    "      else:\n",
    "        read_start = rev_start(left_pos, al_len)\n",
    "    else:\n",
    "        read_start = \"-\"\n",
    "    return read_start\n",
    "\n",
    "def check_seq(str):\n",
    "    \"\"\"\n",
    "    Is the target tag made of acceptable nucleotide characters\n",
    "    \"\"\"\n",
    "    import re\n",
    "    chars = set('ACTG')\n",
    "    seq = set(str.upper())\n",
    "    if seq.issubset(chars):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def find_tags2(read, target_tag, max):\n",
    "    \"\"\"\n",
    "    Find transposon tag in sequence of each read.\n",
    "\n",
    "      Input           read              mapped read\n",
    "                      target_tag        string that matches transposon sequence\n",
    "                      max               maximum number of mismatches allowed (default=2)\n",
    "      Output          match             match of start of tag from left-most start position of read or -1 if no match\n",
    "    \"\"\"\n",
    "    from Bio.Seq import Seq\n",
    "    \n",
    "    # find strand of read\n",
    "    strand = parse_samflag(read)\n",
    "    seq = read.split()[9]\n",
    "    if strand == \"F\":\n",
    "        #search string for transposon seq with max num mismatches\n",
    "        match = mmfind1(seq, len(seq), target_tag, len(target_tag), max)\n",
    "    elif strand == \"R\":\n",
    "        # make rc of seq and search for transposonl tag with max num mismatches\n",
    "        dna     = Seq(seq)\n",
    "        rc_seq  = str(dna.reverse_complement())\n",
    "        match = mmfind1(rc_seq, len(rc_seq), target_tag, len(target_tag), max)\n",
    "    else: \n",
    "        match = -1\n",
    "    \n",
    "    return match\n",
    "\n",
    "def filter_mapped_reads2(sam_file, tag=\"ACTTATCAGCCAACCTGTTA\", mismatch_max=2):\n",
    "  \"\"\" \n",
    "  Revised filtering function\n",
    "  \"\"\"\n",
    "  import sys\n",
    "  import os\n",
    "  import re\n",
    "  from operator import itemgetter\n",
    "\n",
    "  #check tag has valid nucleotides\n",
    "  if check_seq(tag):\n",
    "     pass\n",
    "  else:\n",
    "     sys.exit(\"Invalid sequence tag\")\n",
    "\n",
    "  #read sam_file and sort lines between header and reads\n",
    "  data    = read_samfile(sam_file)\n",
    "  header  = data[0]\n",
    "  reads   = data[1]\n",
    "\n",
    "  barcode_list  = []\n",
    "  good_reads    = []\n",
    "  notags_reads  = []\n",
    "  dup_reads     = []\n",
    "  \n",
    "  for read in reads:\n",
    "    ftag = find_tags2(read, tag, mismatch_max)\n",
    "    #align length (will parse read align len from header)\n",
    "    al  = align_len(read)\n",
    "\n",
    "    if ftag != -1 and al > 0:   #(transposon tag in first 40, but this will discard shorter reads that have tag and have mapped, could make 0)\n",
    "      #find barcode/start combo\n",
    "      bc_start = find_barcodes_starts(read)\n",
    "      barcode_list.append(bc_start) \n",
    "      barcode_list.sort(key=itemgetter(0))  #maybe this will speed up search by barcode?\n",
    "     \n",
    "      # if hasn't been added before, add read to good_reads\n",
    "      if barcode_list.count(bc_start) < 2:\n",
    "        good_reads.append(read)\n",
    "      else:\n",
    "        dup_reads.append(read)\n",
    "\n",
    "    else:\n",
    "      notags_reads.append(read)\n",
    "  #save barcode_list as pickle object to use in quantifying reads at ta sites? temporary until read processed?  \n",
    "  print(\"Total number of mapped reads: \", len(reads))\n",
    "  print(\"number of good reads (with tag): \", len(good_reads))\n",
    "  print(\"number of bad reads (with no tag): \", len(notags_reads))\n",
    "  print(\"Number of reads with duplicate barcode/starts: \", len(dup_reads))\n",
    "  bn = os.path.basename(sam_file)\n",
    "  outfile = \"tag_filtered_\" + bn\n",
    "  with open(outfile, 'w') as f:\n",
    "    for line in header:\n",
    "      f.write(f\"{line}\\n\")\n",
    "    for line in good_reads:\n",
    "      f.write(f\"{line}\\n\")\n",
    "  return notags_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have filtered out unmapped reads from sorted.bam into mapped.sam. All reads should be assigned a strand in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def check_seq(str):\n",
    "    import re\n",
    "    chars = set('ACTG')\n",
    "    seq = set(str.upper())\n",
    "    if seq.issubset(chars):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(check_seq(\"ACTGGggG\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mapped reads:  1180\n",
      "number of good reads (with tag):  1105\n",
      "number of bad reads (with no tag):  7\n",
      "Number of reads with duplicate barcode/starts:  68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A01968:63:H77VYDSX5:4:1101:5267:1752_1:N:0:AACGTGAT_BC:GGGGGGGG\\t16\\tNC_002945.3\\t186581\\t60\\t119M32S\\t*\\t0\\t0\\tCGCTGTTCTACGCCGACGGCACCACTATGTTGTTCGGTGATGCGAAGAAATCGGTTACCGAAGTCTCCGAGGAACTCAAGGCGTTGTAGCGCGCGAGCGCTGGCTCAGACGGGCGGATAACAGGTTGGCTGATAGTCCTCGGTCTCTAGAC\\tF:FFF,FFFFFFFFFFFFFFF:FFF::FFFFFFFF:FFFFFF,FFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFF:FFFFF:FFFFF:FFFFFFFFFFF:FFFFFF:\\tNM:i:1\\tMD:Z:55G63\\tAS:i:114\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:21766:2300_1:N:0:AACGTGAT_BC:TATTTCAT\\t16\\tNC_002945.3\\t215396\\t60\\t119M32S\\t*\\t0\\t0\\tTGCAGCTCTTCGTGCTCATCGCCGCGGCCCACGACGTGCGCTGCGACGTGGCATCGAATTCGCCGTTCGTGTACGCCTACGGGTTCGCCGAGGACATCGACACCAGCCACGCCCTATACCCCGTGGGCTGATAAGTCCCCGGTCTCTAGAC\\tF::FF:F,F,:FFFF:,F:F,F,FFF,F,:F:FFF:,FFFF:,,FFFF,FFFFFFFFFFFF::FF,:F:FFF,FF:F::FFF::FFFFFF,FFF,FFFF,FFFF:,FF:FFFF:F:,F,F,,FF,,FFFF:FFFFFFFF:FFFFFFFFFFF\\tNM:i:3\\tMD:Z:22G6A32A56\\tAS:i:104\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:11559:2002_1:N:0:AACGTGAT_BC:ACACCACT\\t16\\tNC_002945.3\\t967861\\t60\\t132M19S\\t*\\t0\\t0\\tCACCGGGATCAGTTTCAAGTTGGCCAGTGCTAGCGGAATGCCGATGATCGTGACTGCCATTGCCGCGGCACTCACCAAATGCCCGAGGGCCAGCCAGATCCCGAACAGCAGCACCCAGATGACGTTGCTGATAAGTCCCCGGTCTCTAGAC\\t:FFFFFFF,FFFFFFFFFFFF:FF:F:FFF:FFFF,FFF:FFF,FFF:FFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFF:F:FFFFFFFFFFFFFFFFFFFFFF:FFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFF:F\\tNM:i:0\\tMD:Z:132\\tAS:i:132\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:15926:2018_1:N:0:AACGTGAT_BC:GCAACCGC\\t16\\tNC_002945.3\\t967861\\t60\\t132M19S\\t*\\t0\\t0\\tCACCGGGATCAGTTTCAAGTTGGCCAGTGCTAGCGGAATGCCGATGATCGTGACTGCCATTGCCGCGGCACTCACCAAATGCCCGAGGGCCAGCCAGATCCCGAACAGCAGCACCCAGATGACGTTGCTGATAAGTCCCCGGTCTCTAGAC\\tFFFFFFFFFFFF:FFF::FFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFF:FFFFFFFFF:F,FFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFF\\tNM:i:0\\tMD:Z:132\\tAS:i:132\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:24930:1611_1:N:0:AACGTGAT_BC:TAAGGCCT\\t16\\tNC_002945.3\\t1076613\\t60\\t142M9S\\t*\\t0\\t0\\tACCCGATCCTGCTGGCCCGGTGTCAGCGAATGCCAACACCGCTTGACCTCCTCAGGGTCGCTGTCCGGCGGCGGCATCTGCGGCATTGTGGGTGGCGCATGGCTGAGTTGGGCATGGACCTGCTCGCGTGATAAGTCCCCGGTCTCTAGAC\\tF:FFF:F:FFFFFFFFFFFF,:F,FF:F,F,FFF,,FFFFF,F,F,FFFFF:FFFFFFF:FFFFF:FFFFFFFFFFFFFF:FFFFFFFFFFFF:FF::FFFFFFFFFFF,FFFFF,FF:FFF:,F:FFFFFF:FFFF:FFFFFFFFFF:FF\\tNM:i:5\\tMD:Z:20G14C79T12C2C10\\tAS:i:117\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:12744:2957_1:N:0:AACGTGAT_BC:AAAACAAA\\t0\\tNC_002945.3\\t2547177\\t60\\t89S62M\\t*\\t0\\t0\\tCTCTAGACACCAGGCACATATAATCCAAACTGTTACTAGCTAGCACAAATCCGAAAAGCCCACGACTCACATCCACTCACAGACGCCACCAACCAGGTACATTTCTCGCTCGCACACCCTGATGCGCTCGAAGATCTGGTGCCGTTCGCCG\\tFFFF:FF,FF,,F,:,:,F,::,F:F,F,,F,F,F:::,F:,FF,F:,,,:FF,F,,FF,,:FF,F::,,,F::FFFF,F:F:::,:F,:,FFFF,:F:F,F:F,:FFFF,F:,FFF:FFFFFFFFF:FF::F:FFFFFFFFFFFFFFFFF\\tNM:i:1\\tMD:Z:24C37\\tAS:i:57\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:11677:2456_1:N:0:AACGTGAT_BC:TGTATAGA\\t16\\tNC_002945.3\\t3829092\\t60\\t118M33S\\t*\\t0\\t0\\tCACACGGCACGGGCCCGTGAGCTGATTGGTCCGTCGGCGTTCCTGGCGCCCGAACACAAGGTGGTGCTGACCACCGACTCGGCAAGGGCCCGTACGGTGGGACGCCAGGCGCTCGATAACCTGTTGGCTTATAAGTCCCCGGTCTCTAGAC\\tF,FFF,F,,,FFFF:FFF,,,FF:FFFFFFF:,FFFF:::F,FFFFF,F:F:FFFF::F,FF:FF,FFFF,FFF,:F::,F,,FF:F,F:FFF,:FFFFF,FFFF,FFF,::F,F:F,,,,,FF:,FFF,:FFFFFFF,,FFF,F:FFFFF\\tNM:i:0\\tMD:Z:118\\tAS:i:118\\tXS:i:0']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_reads = read_samfile(\"sorted_reads/mapped_test_5000_R1.sam\")[1][0:10]\n",
    "#for read in my_reads:   \n",
    "#    print(read)\n",
    " #   fb = find_barcodes2(read)\n",
    " #   print(fb)\n",
    " #   tg = find_tags2(read, \"ACTTATCAGCCAACCTGTTA\", 2)\n",
    " #   print(tg)\n",
    "    \n",
    "filter_mapped_reads2(\"sorted_reads/mapped_test_5000_R1.sam\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 of these 7 no tag reads have different part of the transposon tag in them, but not the part that ends in GTTA\n",
    "\n",
    "+  ACTTATCAGCCAACCTGTTA-gDNA\n",
    "-  gDNA-TAACAGGTTGGCTGATAAGT\n",
    "            gDNA?-GCTGATAAGTCCCCGGTCTCTAGAC\n",
    " \n",
    "\n",
    "These have parts of transposon tag, but either >2 matches, or missing the terminal end of Himar1 transposon which includes the TA site.\n",
    "\n",
    "View these on artemis. Artemis won't view .sam, so have to change to .bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda activate dna\n",
    "samtools view -b -o tag_filtered_mapped_test_5000_R1.bam tag_filtered_mapped_test_5000_R1.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This samtools view won't work with the filtered because the @SQ lines are removed when filtered for mapped reads.\n",
    "\n",
    "Can use -h to reset output with header--edited snakemake file for filtering by mapped reads.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Quantify reads at each TA site (need list of coordinates of TA sites for bovis to make wig files). Need to make sure start position parsed from the cigar string is accurate\n",
    "\n",
    "How do left-most positions from reads align with TA sites?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View on artemis\n",
    "\n",
    "Reverse strand mapped read with start position calculated as 39,859 (left-most position + aligned len from cigar string). This is actually one off from real start position which is one too far right (5' for reverse). REad actually begins at 39858. TA site on - strand starts at 39858. Positive read starts at 39857 (correct left-most position from cigar) which coincides with TA starting at 39857. These are same TA site and should be counted as 2 insertions for one TA site. The site is referenced by + strand T of the TA in the file of ta sites (bovis_TA_sites.txt) found with insertion_site_finder function. Therefore, reverse strand reads need to subtract one from + strand coordinate of start to line up with the correct TA site (which is really the 'A' of the TA site on the reverse compl strand)\n",
    "\n",
    "![Forward and reverse mapped to same TA site](images/F_R_sameTAsite.png)\n",
    "\n",
    "\n",
    "For forward strand:  start == TA site position (+ strand coordinate)\n",
    "for reverse strand:  start - 1 == TA site position (based on + strand coordinate)\n",
    "\n",
    "Instead of changing start position for reverse reads, will quantify strands separately, with TA sites adjusted for reverse orientation?\n",
    "\n",
    "Also, when TA site inserted before T in genomic, alignment len can be off by one (should have soft-clipped one more base as genomic DNA seq has TTA which matches transposon tag end -GTTA), should there be some leeway if start is within 1 base of TA site position?\n",
    "\n",
    "![soft-clip one early](images/softclip_error.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def motif_finder(seq, motif):\n",
    "    \"\"\"Finds location of motif (substring) in sequence.\n",
    "    Input       seq         sequence\n",
    "                motif       desired subsequence\n",
    "    Output      locations   list of sequence locations of substrings (one-indexed)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    locations = []\n",
    "    p = re.compile('(?='+motif+')')       # use lookahead to find all overlapping matches\n",
    "    matches = p.finditer(seq)\n",
    "    for match in matches:\n",
    "        start = match.span()\n",
    "        locations.append(start[0] + 1)   # add one to change from zero-indexing to sequence position   \n",
    "\n",
    "\n",
    "    return locations\n",
    "\n",
    "#**********************************************************************************\n",
    "\n",
    "def find_insertion_sites(seq):\n",
    "    \"\"\"Finds all the locations of TA insertion sites in sequence\n",
    "\n",
    "    Seems you only want TA positions from one strand for counting number of sites,\n",
    "    as they are same site on either strand. When quantifying, need to get reads that start \n",
    "    \"\"\"\n",
    "    from Bio.Seq import Seq\n",
    "    \n",
    "    seq_obj         = Seq(seq)\n",
    "    rec_seq         = str(Seq.reverse_complement(seq_obj))\n",
    "    fwd_positions = motif_finder(seq, \"TA\")\n",
    "    rev_positions = motif_finder(rec_seq, \"TA\")\n",
    "    \n",
    "    return fwd_positions, rev_positions\n",
    "\n",
    "#**********************************************************************************\n",
    "\n",
    "\n",
    "def open_fasta(refseq):\n",
    "    with open(refseq, 'r') as file:\n",
    "        seq = ''\n",
    "        for line in file:\n",
    "            if line[0] != '>':\n",
    "                seq += line.strip()\n",
    "    return seq\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make list of TA sites in reference genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73465\n",
      "[60, 72, 102, 188, 246, 333, 360, 426, 448, 471]\n",
      "73465\n",
      "[60, 72, 102, 188, 246, 333, 360, 426, 448, 471]\n",
      "73465\n",
      "[6, 24, 53, 92, 111, 120, 135, 195, 255, 259]\n",
      "[4345021, 4345044, 4345066, 4345132, 4345159, 4345246, 4345304, 4345390, 4345420, 4345432]\n"
     ]
    }
   ],
   "source": [
    "bovis_fasta = \"ref_seqs/Mbovis_AF2122-97.fasta\"\n",
    "bovis_seq   = open_fasta(bovis_fasta)\n",
    "# to look at only first 30,000 nts of genome\n",
    "bovis_30 = bovis_seq[0:30001]\n",
    "\n",
    "res = motif_finder(bovis_seq, \"TA\")\n",
    "print(len(res))\n",
    "print(res[0:10])\n",
    "\n",
    "with open('bovis_TA_sites.txt', 'w') as outfile:\n",
    "    results = find_insertion_sites(bovis_seq)\n",
    "    print(len(results[0]))\n",
    "    #73465\n",
    "    print(results[0][0:10])\n",
    "    print(len(results[1]))\n",
    "    #73465\n",
    "    #these will be from 5' end of rc strand (-), wont use these\n",
    "    print(results[1][0:10])\n",
    "    #these should coincide with 5' end of + strand\n",
    "    print(results[1][-10:])\n",
    "    #for site in results:\n",
    "     #   print(site, file=outfile)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_set = [1,2,3,4,5,5,5,5,6,7,8,9,10]\n",
    "print(len(whole_set))\n",
    "unique_set = set(whole_set)\n",
    "print(unique_set)\n",
    "len(unique_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use similar strategy to tpp_tools.py: read_counts(ref,sam,vars) template_counts(ref,sam,bcfile,vars) and increase_counts(pos,sites, strand)\n",
    "\n",
    "1. use find_barcode_starts to get list of barcode/starts for each read. should be same length as number of reads\n",
    "    - change filter function to only look for tranposon tags\n",
    "    - new function to identify starts/strand/barcode for each read and make into list(which will be iterated through)\n",
    "2. make dicts of ta-sites and read counts: ordered dictionary (default is ordered), new values will overwrite old values\n",
    "    - analyse each strand separately\n",
    "    - make TA:read dict: \n",
    "        - for each site key, append list barcodes that have starts within 1 nt of TA site\n",
    "        - add up total reads for each site (F and R)\n",
    "    - make TA:template dict:\n",
    "        - for each site key, reduce barcode list to unique sites\n",
    "        - add up total reads for each site (F and R)\n",
    "3. Write wig files for TA sites using TA:template dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new dictionary with start positions and strand? or re-use barcode/start--add strand info?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New sequence if I don't want to map first?\n",
    "\n",
    "1. quality control and trim illumina seqs\n",
    "2. add barcode to fastq sequence name \n",
    "3. filter fastq reads for transposon tag--have to look in both directions (won't know which way will map--search for both?)\n",
    "4. trim off transposon tag from reads--look for tag at both ends where there is short fragment (TACCACGACCA on other end) \n",
    "4. map reads\n",
    "5. remove duplicates with same barcode/start position (could do this after assigning to TA site)\n",
    "6. quantify reads at each ta site and generate .wig file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
