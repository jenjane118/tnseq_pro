{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menadione Tn-seq notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#download from genewiz sftp\n",
    "sftp jstien01_student_bbk@gweusftp.azenta.com\n",
    "lcd /d/in16/u/sj003/men_tnseq\n",
    "cd 40-842749567\n",
    "mget *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: A1_R1_001.fastq.gz: No such file or directory\n",
      "gzip: A1_R2_001.fastq.gz: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# files in /d/in16/u/sj003/men_tnseq/fastq\n",
    "\n",
    "#!ls fastq\n",
    "zcat A1_R1_001.fastq.gz | head -5\n",
    "zcat A1_R2_001.fastq.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File checks to make sure downloaded correctly--matching sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check length of files to see if reads same in all files?\n",
    "!cd fastq\n",
    "!FILES=*.fastq.gz\n",
    "!for file in $FILES; do wc -l $file; done >> sanity_check.txt\n",
    "# all 3 files have different line counts, R2 is index. R1 and R3 more similar, different because of line continuation?\n",
    "\n",
    "# check files downloaded correctly\n",
    "!for file in $FILES; do md5sum -c $file.md5; done >> md5_check.txt\n",
    "\n",
    "#all 'OK'\n",
    "\n",
    "#head\n",
    "!for file in $FILES; do echo $file; zcat $file | head -10 $file; done\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "line count was the same for R1 and R2 when we did cattle in vivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gzip: A1_R1_001.fastq.gz: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# for A1 sample see how many times header appears in file\n",
    "!cd fastq\n",
    "!zgrep -c 'N:0:AACGTGAT' A1_R1_001.fastq.gz\n",
    "#64796903\n",
    "!zgrep -c 'N:0:AACGTGAT' A1_R2_001.fastq.gz\n",
    "#64796903\n",
    "!zgrep -c 'N:0:AACGTGAT' A1_R3_001.fastq.gz\n",
    "#64796903"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ON mac the zgrep command doesn't seem to work (or zcat?) use gunzip?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates same number of reads in all files.\n",
    "\n",
    "Run fastqc and then can double-check read length for all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-a2cea0d7dfa0>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-a2cea0d7dfa0>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    FILES=/d/in16/u/sj003/men_tnseq/fastq/*.fastq.gz\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!cd fastq\n",
    "!module load fastqc\n",
    "!module load multiqc\n",
    "!FILES=*.fastq.gz\n",
    "!for f in $FILES; do fastqc ${f} -o fastqc; done\n",
    "!cd fastqc\n",
    "!multiqc .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-03ca3f647ae6>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-03ca3f647ae6>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    for file in $FILES; do echo $file; done\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#test bash command\n",
    "\n",
    "pwd\n",
    "FILES=*fastq.gz\n",
    "for file in $FILES; do echo $file; done"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look fine--reads of 151bp for R1 and R3 (corresponding to Read 1 and Read 3) and i7 read (R2) is 8bp which represents the barcode.\n",
    "\n",
    "Next step is to iterate through reads in R1 and R2 to 'demultiplex' by combining to one 'template' the reads that have the same barcode in R2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@A01968:63:H77VYDSX5:4:1101:28212:1031 2:N:0:AACGTGAT\r\n",
      "CGCAGTAT\r\n",
      "+\r\n",
      "FFFFFF:F\r\n",
      "@A01968:63:H77VYDSX5:4:1101:29369:1031 2:N:0:AACGTGAT\r\n",
      "GAGCACCT\r\n",
      "+\r\n",
      "FF:FFFFF\r\n",
      "@A01968:63:H77VYDSX5:4:1101:32027:1031 2:N:0:AACGTGAT\r\n",
      "CATACCAA\r\n",
      "+\r\n",
      "FFFFFFFF\r\n",
      "@A01968:63:H77VYDSX5:4:1101:3341:1047 2:N:0:AACGTGAT\r\n",
      "GTACATGC\r\n",
      "+\r\n",
      ":FFFF::F\r\n",
      "@A01968:63:H77VYDSX5:4:1101:9073:1047 2:N:0:AACGTGAT\r\n",
      "GCCGCCCC\r\n",
      "+\r\n",
      "FFFFFFFF\r\n",
      "\r\n",
      "gzip: stdout: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!zcat fastq/A1_R2_001.fastq.gz | head -20"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line 1 is the read identifier with the barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zcat: can't stat: fastq/A1_R1_001.fastq.gz (fastq/A1_R1_001.fastq.gz.Z): No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!zcat fastq/A1_R1_001.fastq.gz | head -20\n",
    "!zcat fastq/A1_R1_001.fastq.gz | head -1000 > test_1000_R1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on laptop need to install gnused before can use sed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all unique reads in R2 line 2 of each read\n",
    "#!head -1000 data/fastq/A1_R2_001.fastq > test_1000_R2.txt\n",
    "#!brew install gnu-sed\n",
    "#!sed -n '1~2p' test_1000_R2.txt | sort | uniq -c | sort -nr\n",
    "# need gsed installed for this\n",
    "#!awk 'NR % 2 == 0' data/fastq/test_1000_R2.txt | sort | uniq -c | sort -nr\n",
    "# print every 4 lines starting from line 2\n",
    "!awk 'NR % 4 == 2' data/fastq/test_1000_R2.txt > test.txt\n",
    "cat test.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12 GGGGGGGG\n",
      "   3 ACAAGAAT\n",
      "   2 GCCGCCCT\n",
      "   2 CGGCGTCA\n",
      "   2 ATTGAAGT\n",
      "   2 ATATACAA\n",
      "   2 ACTATCGT\n",
      "   2 AATTAACT\n",
      "   2 AAGGTTAA\n",
      "   1 TTTTGAGG\n",
      "   1 TTTTACCT\n",
      "   1 TTTAGTCA\n",
      "   1 TTGCACGA\n",
      "   1 TTCGTGCA\n",
      "   1 TTCGTACT\n",
      "   1 TTCCCAGT\n",
      "   1 TTCATAAG\n",
      "   1 TTCACGTC\n",
      "   1 TTAGTAAC\n",
      "   1 TTAGACAT\n",
      "   1 TTAAAACC\n",
      "   1 TGTTGTGA\n",
      "   1 TGTTGTCT\n",
      "   1 TGTCCATC\n",
      "   1 TGGTTCAA\n",
      "   1 TGCACTTT\n",
      "   1 TGCAACGG\n",
      "   1 TGATCTTA\n",
      "   1 TGAGCACA\n",
      "   1 TGACATGA\n",
      "   1 TGACATCT\n",
      "   1 TGACATAT\n",
      "   1 TCTTGTCA\n",
      "   1 TCTGCCTC\n",
      "   1 TCTACTTC\n",
      "   1 TCTAACGC\n",
      "   1 TCGTTCAG\n",
      "   1 TCGTCTTA\n",
      "   1 TCGTCCAG\n",
      "   1 TCATTGAA\n",
      "   1 TCATTCAT\n",
      "   1 TCATGTTA\n",
      "   1 TCACATGC\n",
      "   1 TCACAAGT\n",
      "   1 TCAATTAC\n",
      "   1 TCAACAGC\n",
      "   1 TATTAATT\n",
      "   1 TATTAAGC\n",
      "   1 TATGATCT\n",
      "   1 TAGTCAAG\n",
      "   1 TAGTATTT\n",
      "   1 TAGCTGAT\n",
      "   1 TAGCATAA\n",
      "   1 TAGACTCA\n",
      "   1 TACTTGAC\n",
      "   1 TACTATTC\n",
      "   1 TACGATAC\n",
      "   1 TAATCATC\n",
      "   1 TAAGACCC\n",
      "   1 TAAAATCA\n",
      "   1 GTTCTGCT\n",
      "   1 GTGCCGAG\n",
      "   1 GTGACCCA\n",
      "   1 GTGACACG\n",
      "   1 GTCTAATT\n",
      "   1 GTCCACAT\n",
      "   1 GTCCAACT\n",
      "   1 GTAGCATG\n",
      "   1 GTACCGAT\n",
      "   1 GTACATGC\n",
      "   1 GTAATGCA\n",
      "   1 GTAAATAG\n",
      "   1 GTAAAAGA\n",
      "   1 GGGCCGCA\n",
      "   1 GGCAACTC\n",
      "   1 GGCAAACA\n",
      "   1 GGACGACT\n",
      "   1 GCTAGTAG\n",
      "   1 GCGTCGAC\n",
      "   1 GCGCGCAT\n",
      "   1 GCGAAGAG\n",
      "   1 GCCTTAAA\n",
      "   1 GCCTATAT\n",
      "   1 GCCTAACA\n",
      "   1 GCCGCCCC\n",
      "   1 GCATATCG\n",
      "   1 GCACGCAA\n",
      "   1 GCAATCTA\n",
      "   1 GCAAAAAT\n",
      "   1 GATCCCTA\n",
      "   1 GAGCACCT\n",
      "   1 GACGGATT\n",
      "   1 GACGCAAT\n",
      "   1 GACCAGTG\n",
      "   1 GACCACAA\n",
      "   1 GACACGAG\n",
      "   1 GAATTTAG\n",
      "   1 GAAGGTCC\n",
      "   1 GAAAGACA\n",
      "   1 GAAACCAC\n",
      "   1 GAAACAAC\n",
      "   1 CTGGGCAG\n",
      "   1 CTGCGTAA\n",
      "   1 CTGCAAAT\n",
      "   1 CTGCAAAA\n",
      "   1 CTCCCTAA\n",
      "   1 CTACGGAC\n",
      "   1 CTACACTG\n",
      "   1 CTAATGAC\n",
      "   1 CGTTGTTT\n",
      "   1 CGTGCCCG\n",
      "   1 CGGTTGGC\n",
      "   1 CGGTATAG\n",
      "   1 CGGGCTTT\n",
      "   1 CGGACACT\n",
      "   1 CGCTCTGT\n",
      "   1 CGCTAAGG\n",
      "   1 CGCCTCGA\n",
      "   1 CGCCGTAC\n",
      "   1 CGCCGAAC\n",
      "   1 CGCCCCCC\n",
      "   1 CGCCCATA\n",
      "   1 CGCCCAAG\n",
      "   1 CGCAGTAT\n",
      "   1 CGCAACTT\n",
      "   1 CGAGACAT\n",
      "   1 CGAGACAG\n",
      "   1 CGAATGGG\n",
      "   1 CGAAGTAA\n",
      "   1 CGAACTAG\n",
      "   1 CCTGCGCC\n",
      "   1 CCGTTCAA\n",
      "   1 CCGGCACC\n",
      "   1 CCGCGCAT\n",
      "   1 CCGATTTA\n",
      "   1 CCGACCCT\n",
      "   1 CCCTGGTA\n",
      "   1 CCCTAGAT\n",
      "   1 CCCGGCAG\n",
      "   1 CCCCTAAC\n",
      "   1 CCCACTTT\n",
      "   1 CCATGGAC\n",
      "   1 CCATACGA\n",
      "   1 CCAAGACT\n",
      "   1 CCAACCTA\n",
      "   1 CCAAATAT\n",
      "   1 CATGCTCC\n",
      "   1 CATCCCAT\n",
      "   1 CATCAACC\n",
      "   1 CATATTAG\n",
      "   1 CATAGAAT\n",
      "   1 CATAGAAG\n",
      "   1 CATACCTA\n",
      "   1 CATACCAA\n",
      "   1 CAGTGGTG\n",
      "   1 CAGGCTCT\n",
      "   1 CACACTAA\n",
      "   1 CACAATAC\n",
      "   1 CAAGGTTA\n",
      "   1 CAAGATTT\n",
      "   1 CAACTGGC\n",
      "   1 CAACGCAC\n",
      "   1 CAACCGCT\n",
      "   1 CAAAGATG\n",
      "   1 CAAACCGA\n",
      "   1 ATTTCCTA\n",
      "   1 ATTACAAA\n",
      "   1 ATTAATGA\n",
      "   1 ATGTAAGG\n",
      "   1 ATGGCCAA\n",
      "   1 ATGATTGC\n",
      "   1 ATGATGAC\n",
      "   1 ATGATCCT\n",
      "   1 ATCTGACA\n",
      "   1 ATATTCAA\n",
      "   1 ATATCAAA\n",
      "   1 ATATAACT\n",
      "   1 ATAATTTT\n",
      "   1 AGTGGTTG\n",
      "   1 AGTCCCCA\n",
      "   1 AGTAACGT\n",
      "   1 AGGCCTAA\n",
      "   1 AGGAAATG\n",
      "   1 AGCTCTAA\n",
      "   1 AGCCTGTC\n",
      "   1 AGATGACA\n",
      "   1 AGATAACT\n",
      "   1 AGAGAACG\n",
      "   1 AGACCGGC\n",
      "   1 AGAAACCC\n",
      "   1 ACTCGCTA\n",
      "   1 ACTCGATA\n",
      "   1 ACTCCCAC\n",
      "   1 ACTACCAT\n",
      "   1 ACTAACAC\n",
      "   1 ACGGTCAT\n",
      "   1 ACGCTTCC\n",
      "   1 ACGACGCC\n",
      "   1 ACGACCGA\n",
      "   1 ACCGCCCG\n",
      "   1 ACCGATTT\n",
      "   1 ACCGACTT\n",
      "   1 ACCCTCCT\n",
      "   1 ACCCAAAA\n",
      "   1 ACCATCTC\n",
      "   1 ACCAGCAG\n",
      "   1 ACCAAGAC\n",
      "   1 ACCAAACA\n",
      "   1 ACATAACG\n",
      "   1 ACAGGTAG\n",
      "   1 ACAGGATC\n",
      "   1 ACAGCCCC\n",
      "   1 AATTGACT\n",
      "   1 AAGGAGCA\n",
      "   1 AAGGACGG\n",
      "   1 AAGCCGGA\n",
      "   1 AAGCAATG\n",
      "   1 AAGACGAA\n",
      "   1 AACCGCCA\n",
      "   1 AACCGAAA\n",
      "   1 AACCAGAA\n",
      "   1 AACATTAC\n",
      "   1 AACATCCT\n",
      "   1 AACATACT\n",
      "   1 AAATCGCA\n",
      "   1 AAAGACTT\n",
      "   1 AAAATGTA\n",
      "   1 AAAAGTTA\n",
      "   1 AAAAACGA\n",
      "   1 AAAAAAAT\n"
     ]
    }
   ],
   "source": [
    "!sort test.txt | uniq -c | sort -nr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows how many duplicates of certain barcodes. need to match barcodes with read name to eliminate ones that have repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do again with whole file (takes a really long time)\n",
    "!awk 'NR % 4 == 2' data/fastq/A1_R2_001.fastq | sort | uniq -c | sort -nr > count_barcodes.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top counts\n",
    "1312199 GGGGGGGG\n",
    "16805 CAAAACGA\n",
    "16371 GCAACATT\n",
    "14835 AATACAAC\n",
    "14507 CCAAAGAC\n",
    "14483 AATATAAC\n",
    "14427 TAACAACA\n",
    "14120 AAAACATA\n",
    "13989 TAAAACTA\n",
    "13544 CCGAAACA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find distribution of these reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which reads are associated with these really big duplications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thiss just puts pattern in file--not whole read\n",
    "!grep 'GGGGGGGG' data/fastq/A1_R1_001.fastq > gggg_dupl.fastq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paste two files together so barcode is added onto read header?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bash solution\n",
    "!paste -d \" \" file1.txt file2.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a new fastq file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python\n",
    "seq_list = []\n",
    "with open ('test_1000_R1.txt', 'r') as f1, open('test_1000_R2.txt', 'r') as f2:\n",
    "    #line1_f1 = f1.readline()\n",
    "    #for line in f1.read().split(\"\\n\")[0]:\n",
    "    #    print(line.rstrip())\n",
    "    # header and sequence for R1 reads\n",
    "    for lineno, line in enumerate(f1):  \n",
    "        if lineno % 4 == 0:\n",
    "            f1_head = line.rstrip()\n",
    "        # second line and every 4  (sequence)  \n",
    "        if lineno %4 == 1:\n",
    "            #print(line)\n",
    "            f1_seq = line\n",
    "        # spacer line\n",
    "        if lineno %4 ==2:\n",
    "            f1_spacer = line\n",
    "        #quality line\n",
    "        if lineno %4 ==3:\n",
    "            f1_quality = line\n",
    "\n",
    "        \n",
    "    #create header and barcode for index reads\n",
    "    for lineno, line in enumerate(f2):\n",
    "        if lineno % 4 == 0:\n",
    "            f2_head = line.rstrip()\n",
    "            \n",
    "        if lineno % 4 == 1:\n",
    "            #print(line)\n",
    "            f2_barcode = line.rstrip()\n",
    "        # add barcode to end of header in read1 file\n",
    "        new_head = f1_head + \" BC:\" + f2_barcode\n",
    "\n",
    "        list_entry = new_head + \"\\n\" + f1_seq + f1_spacer + f1_quality\n",
    "    \n",
    "        seq_list.append(list_entry)\n",
    "\n",
    "# write new file with barcodes\n",
    "with open('new_barcode_file_1000.txt', 'w') as outfile:\n",
    "    outfile.writelines(seq_list)\n",
    "outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sequences for the R1 reads that have the GGGGGGGG 'random barcode' in the first 1000 reads, these sequences all have huge runs of G's?\n",
    "\n",
    "\"For Illumina NextSeq/NovaSeq data, polyG can happen in read tails since G means no signal in the Illumina two-color systems. fastp can detect the polyG in read tails and trim them. This feature is enabled for NextSeq/NovaSeq data by default, and you can specify -g or --trim_poly_g to enable it for any data, or specify -G or --disable_trim_poly_g to disable it. NextSeq/NovaSeq data is detected by the machine ID in the FASTQ records. \" \n",
    "Need to map reads first because will exclude artefactual PCR amplicons that have identical P7 as well as same genomic insertion site. (from fastp documentation, https://github.com/OpenGene/fastp)\n",
    "\n",
    "But maybe need to add the barcode to the header of each read first. Adapt above to make python script that can work on entire files of reads and make new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_barcode(file1, file2):\n",
    "    \"\"\"\n",
    "    A function to extract the barcode from the associated p7 index read to the header of each read in the fastq file\n",
    "\n",
    "    Input               file1                       fastq file for sequence reads (R1 or R3)\n",
    "                        file2                       fastq file for i7 index reads (R2)\n",
    "    Output              barcoded<sample>.fastq      new fastq file which has barcode added to header   \n",
    "\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import os.path\n",
    "    import sys\n",
    "\n",
    "    # identify sample name\n",
    "    filename = str(file1)\n",
    "    bn_sample = os.path.basename(file1)\n",
    "    sample = re.sub(\".fastq\", \"\", bn_sample)\n",
    "    print(sample)\n",
    "\n",
    "    seq_list = []\n",
    "    with open (file1, 'r') as f1, open(file2, 'r') as f2:\n",
    "        #count lines in R1 and R2 and proceed if equal\n",
    "       # if sum(1 for _ in f1) != sum(1 for _ in f2):\n",
    "        #    sys.exit(\"Number of reads do not match\")\n",
    "            \n",
    "    # header and sequence for R1 reads\n",
    "        for index, (line1, line2) in enumerate(zip(f1, f2)):\n",
    "            if index % 4 == 0:\n",
    "                f1_head = line1.rstrip()\n",
    "                f2_head = line2.rstrip()\n",
    "            # second line and every 4  (sequence)  \n",
    "            if index %4 == 1:\n",
    "                f1_seq = line1.rstrip()\n",
    "                #create header and barcode for index reads\n",
    "                #>A01968:63:H77VYDSX5:4:1101:25455:1423 1:N:0:AACGTGAT BC:GGGGGGGG\n",
    "                f2_barcode = line2.rstrip()\n",
    "                # add barcode to end of header in read1 file\n",
    "                new_head = f1_head + \"_BC:\" + f2_barcode\n",
    "                #replace whitespace\n",
    "                new_head = new_head.replace(\" \", \"_\")\n",
    "                \n",
    "            # don't need these if do after trimming\n",
    "            #quality line\n",
    "            #if index %4 ==3:\n",
    "                #f1_quality = line1.rstrip()\n",
    "                #list_entry = new_head + \"\\n\" + f1_seq + \"\\n\" + \"+\" + \"\\n\" + f1_quality + \"\\n\"\n",
    "                list_entry = new_head + \"\\n\" + f1_seq + \"\\n\"\n",
    "                seq_list.append(list_entry)\n",
    "                seq_list.append(list_entry)\n",
    "    print(len(seq_list))\n",
    "    # write new file with barcodes\n",
    "    new_filename = \"barcode_\" + sample + \".fastq\"\n",
    "    with open(new_filename, 'w') as outfile:\n",
    "        outfile.writelines(seq_list)\n",
    "    outfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_5000_R1\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "add_barcode(\"test_5000_R1.fastq\", \"test_5000_R2.fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1_R1_001\n"
     ]
    }
   ],
   "source": [
    "add_barcode(\"data/fastq/A1_R1_001.fastq\", \"data/fastq/A1_R2_001.fastq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1_R3_001\n",
      "64796903\n",
      "barcode_A1_R3_001.fastq\n"
     ]
    }
   ],
   "source": [
    "# can also apply to read2, but no point as won't be using for mapping\n",
    "#add_barcode(\"data/fastq/A1_R3_001.fastq\", \"data/fastq/A1_R2_001.fastq\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map small file to see if header stays connected to read (installed new bwa-mem2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample first 10000 bases of barcoded fastq\n",
    "!head -10000 barcode_A1_R1_001.fastq > new_barcode_file_A1_10000.fastq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate tnseq\n",
    "# have to index file first for bwa-mem2\n",
    "!bwa-mem2 index ref_seqs/Mbovis_AF2122-97.fasta\n",
    "!bwa-mem2 mem -C ref_seqs/Mbovis_AF2122-97.fasta barcode_test_5000_R1.fastq > test_map_5000.sam\n",
    "# still getting error: ERROR! Unable to open the file: ref_seqs/Mbovis_AF212297.fasta.bwt.2bit.64\n",
    "# there were multiple fasta files, deleted all others and manually coppied and pasted text from ref file. changed to Mbovis_AF2122-97.fasta\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "barcode doesn't appear in sam header automatically, -C to append FASTA/FASTQ comment to SAM output (appends to end of file--column 12 in sam file) (-V            output the reference FASTA header in the XR tag--gives blank file)\n",
    "\n",
    "Column 4 in sam header is meant to be the start mapping position (should be insert position). in test file, all have start position as 0--probably didn't map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    starts  barcodes  size\n",
      "0  2736492  AAAAAAAT     1\n",
      "1  2736492  AAAAAAGG     1\n",
      "2  2736492  AAAAACAG     1\n",
      "3  2736492  AAAAACAT     1\n",
      "4  2736492  AAAAACCT     1\n"
     ]
    }
   ],
   "source": [
    "#parse sam file for start and barcode\n",
    "\n",
    "def parse_samfile(samfile):\n",
    "    import pandas as pd\n",
    "    \"\"\"\n",
    "    find molecular barcode and start of mapping in each read\n",
    "    \"\"\"\n",
    "\n",
    "    sam_list = []\n",
    "    barcodes = []\n",
    "    id_dups = []\n",
    "    data = pd.read_csv(samfile, sep=\"\\t\", skiprows=2, header=None)\n",
    "    #print(data.head())\n",
    "    ids = data[0]\n",
    "    starts = data[3]\n",
    "    # sometimes dividing up final lines? 13 for 1000 or 15 for 100000\n",
    "    barcodeLines = list(data[15])\n",
    "    #extract molecular barcode\n",
    "    for line in barcodeLines:\n",
    "        barcodes.append(line.split(\"BC:\",1)[1])\n",
    "    # remove non-unique rows?\n",
    "    #dup_tup = pd.DataFrame(ids)\n",
    "    dup_tup = pd.DataFrame(starts)\n",
    "    dup_tup.columns=[\"starts\"]\n",
    "    dup_tup['barcodes'] = barcodes\n",
    "    # get count of duplicates for each unique row\n",
    "    sorted_dup_tup = dup_tup.groupby(dup_tup.columns.tolist(), as_index=False).size()\n",
    "    \n",
    "    return sorted_dup_tup\n",
    "    \n",
    "    \n",
    "p = parse_samfile(\"test_map_10000.sam\")\n",
    "print(p.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column 3 (0 indexed) is start, column 13/15 has both barcodes, column 9 is sequence. No duplicate start/barcode combos apparent in the 10000bp test file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better approach may be to process reads and find duplicates using Picard find duplicates tool (Mark Duplicates)\n",
    "\n",
    "https://gatk.broadinstitute.org/hc/en-us/articles/360037051452-EstimateLibraryComplexity-Picard-\n",
    "\n",
    "can use argument --BARCODE_TAG 'BC' to identify barcode\n",
    "--READ_NAME_REGEX null if duplicate sets are very high and we are not trying to establish library complexity\n",
    "\n",
    "Reads must be mapped and sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c bioconda picard\n",
    "!java -jar picard.jar MarkDuplicates \\\n",
    "      I=input.bam \\\n",
    "      O=marked_duplicates.bam \\\n",
    "      M=marked_dup_metrics.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step before mapping is to remove the transposon sequence from each read\n",
    "## From TRANSIT/TPP docs:\n",
    "\n",
    "1. Convert .fastq files to .fasta format (.reads).\n",
    "    https://bioinformaticsworkbook.org/dataWrangling/fastaq-manipulations/converting-fastq-format-to-fasta.html#gsc.tab=0\n",
    "    This would only keep header and sequence.\n",
    "\n",
    "(creates .reads file using 'fastq2reads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "gsed -n '1~4s/^@/>/p;2~4p' new_barcode_file_1000.fastq > new_barcode_file_1000.fasta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Identify reads with the transposon prefix in R1 . The sequence searched for is ACTTATCAGCCAACCTGTTA (or TAAGAGACAG for Tn5), which must start between cycles 5 and 10 (inclusive). (Note that this ends in the canonical terminus of the Himar1 transposon, TGTTA.) The “staggered” position of this sequence is due to insertion a few nucleotides of variable length in the primers used in the Tn-Seq sample prep protocol (e.g. 4 variants of Sol_AP1_57, etc.). The number of msmatchen searching reads for the transposon sequence pattern can be adjusted as an option in the interface; the default is 1.\n",
    "\n",
    "    Use fastp for this? First try with TPP?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(thoth)\n",
    "#!tpp -bwa /s/software/bwa/bwa/bwa -ref $my_path/refseqs/Mtb/Mtb_H37Rv.fasta \\\n",
    "#    -reads1 $my_path/ncbi/files/dejesus/SRR4113428_1.fastq \\\n",
    "#        -reads2 $my_path/ncbi/files/dejesus/SRR4113428_2.fastq \\\n",
    "#            -output $my_path/dejesus_mtb/tpp_results/SRR4113428\n",
    "\n",
    "# paired-end (laptop)\n",
    "# this will look for barcode which isnt' there, but maybe useful to trim read 2 and extract genomic sequence?\n",
    "!tpp -bwa ~/anaconda3/envs/tnseq/bin/bwa-mem2 -ref ref_seqs/Mbovis_AF2122-97.fasta \\\n",
    "    -reads1 barcode_A1_R1_001.fastq -reads2 data/fastq/A1_R3_001.fastq \\\n",
    "        -output tpp_test_A1_paired\n",
    "\n",
    "# Error: unexpected format of headers in .fastq files\n",
    "#Assume this is due to adding barcode to header for read1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fastp can use index reads for UMI processing--can extract from index reads and append to first part of read names so it will be in sam/bam files, can specify prefix 'UMI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: fastp\n"
     ]
    }
   ],
   "source": [
    "!fastp -i data/fastq/A1_R1_001.fastq -I data/fastq/A1_R3_001.fastq -o data/fastp_out/A1_R1_001_trimmed.fastq -O data/fastp_out/A1_R3_001_trimmed.fastq \\\n",
    " -U --umi_loc=index1 --umi_prefix=UMI --unpaired1 -l=20 --adapter_fasta adapter_file.txt\n",
    "\n",
    "#It reads from index that is already in multiplexed header--not from index file, so not useful unless specify P7 reads as R2 and it can get umi from read2\n",
    "#--unpaired1 means will save unpaired read 1s, -l 20 means will require reads to be at least 20 bp after trimming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added an 'adapter file' to use with fastp: 'adapter_file.txt' which includes the transposon sequence pattern for R1 and R2.\n",
    "\n",
    "This is leading to very short R1 sequences which actually match the transposon sequence (no genomic?) Are R1 and R3 reversed and R3 is really read1?\n",
    "\n",
    "Do tests with shorter number of reads (100000)\n",
    "\n",
    "Use R2 index reads as read2?\n",
    "\n",
    "#temporarily disable adapter trimming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fastp -i data/fastq/A1_R1_001.fastq -I data/fastq/A1_R2_001.fastq -o data/fastp_out/A1_R1_001_trimmed.fastq -O data/fastp_out/A1_R3_001_trimmed.fastq \\\n",
    " #-U --umi_loc=read2 --umi_prefix=UMI --umi_len=8 --unpaired1 -l 20 --adapter_fasta adapter_file.txt --reads_to_process=100000\n",
    "\n",
    "fastp -i data/fastq/A1_R1_001.fastq -I data/fastp_out/A1_R2_001_trimmed.fastq \\ \n",
    "-U --umi_loc=read2 --umi_prefix=UMI --umi_len=8 --reads_to_process=100000 -o data/fastp_out/A1_R1_001_trimmed.fastq -O data/fastp_out/A1_R3_001_trimmed.fastq\n",
    " \n",
    " #this doesn't work--needs to have properly paired ends"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is new heading that fastp makes that includes UMI: @A01968:63:H77VYDSX5:4:1101:32027:1031:UMI_AACGTGAT 1:N:0:AACGTGAT\n",
    "I imagine this will work for TPP and still have heading included in mapped reads if I just move it to end of first field instead of randomly adding to end. MIght be easier to filter with picard then. but not sure it matters if I don't do tpp since I can parse from sam files myself--it is only using P5 index from the header of the reads--NOT UMI barcode from p7 index files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unpaired fastp for adapter trimming\n",
    "!fastp -i new_barcode_file_A1_10000.fastq -o data/fastp_out/test_A1_R1.fastq --adapter_fasta adapter_file.txt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just leaving super short reads like before (15 bp long--also still part of primer, no genomic sequence left). Maybe because I have both ends of inverted repeat sequence in there\n",
    "\n",
    "@A01641:207:HNJLJDSX3:1:2678:31331:37059 1:N:0:AACGTGAT BC:CGCAGTAT\n",
    "GTCTAGAGACCGGGG\n",
    "+\n",
    "F::FF:FFFFFFFFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without adapter fasta file--using transposon seq for R1 only\n",
    "!fastp -i data/fastq/A1_R1_001.fastq -o data/fastp_out/test_repeat_A1_R1.fastq --adapter_sequence=ACTTATCAGCCAACCTGTTA --reads_to_process=100000\n",
    "#results in mostly 15 bp identical sequences\n",
    "#reads with adapter trimmed: 98991 (trims nearly all)\n",
    "\n",
    "#try with R3\n",
    "!fastp -i data/fastq/A1_R3_001.fastq -o data/fastp_out/test_repeat_A1_R3.fastq --adapter_sequence=ACTTATCAGCCAACCTGTTA --reads_to_process=100000\n",
    "#reads with adapter trimmed: 17\n",
    "\n",
    "#using Read 2 transposon sequence:\n",
    "!fastp -i data/fastq/A1_R3_001.fastq -o data/fastp_out/test_repeat_A1_R3.fastq --adapter_sequence=TGGTCGTGGTAT --reads_to_process=100000\n",
    "#bases trimmed due to adapters: 7559\n",
    "\n",
    "# using rc of read 2 transposon sequence:\n",
    "!fastp -i data/fastq/A1_R3_001.fastq -o data/fastp_out/test_repeat_A1_R3.fastq --adapter_sequence=TACCACGACCA --reads_to_process=100000\n",
    "\n",
    "\n",
    "# using automatic detection\n",
    "!fastp -i data/fastq/A1_R1_001.fastq -o data/fastp_out/test_automatic_A1_R1.fastq --reads_to_process=100000\n",
    "#finds truseq adapter read1, but not many are actually trimmed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First trim with sequencing adapters. Then do again in separate step (cutadapt) to cut off transposon prefix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "added barcode info to both paired-end reads (R1 and R3). After mapping will try to remove duplicates and make wigs again? Will have to put into header of both reads as TPP needs these to match.\n",
    "\n",
    "Paired end takes hours for single sample. Not going to be useful as R2 contains empty barcodes. Use single-end to see if get anything useful. otherwise do manually and compare with trimmed files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tpp on paired ends with matched headers. doesn't map with no barcodes on 2nd read (set for paired end#)\n",
    "!tpp -bwa ~/anaconda3/envs/tnseq/bin/bwa-mem2 -ref ref_seqs/Mbovis_AF2122-97.fasta \\\n",
    "    -reads1 barcode_A1_R1_001.fastq -reads2 barcode_A1_R3_001.fastq -output tpp_test_A1_paired\n",
    "\n",
    "# try for single end \n",
    "#!tpp -bwa ~/anaconda3/envs/tnseq/bin/bwa-mem2 -ref ref_seqs/Mbovis_AF2122-97.fasta \\\n",
    "#    -reads1 barcode_A1_R1_001.fastq -output tpp_test_A1_single\n",
    "\n",
    "#on thoth\n",
    "!tpp -bwa /s/software/bwa/bwa/bwa -ref /d/in16/u/sj003/refseqs/mbovis/Mbovis_AF2122_97.fasta \\\n",
    "    -reads1 fastq/A1_R1_001.fastq.gz -output tpp/tpp_test_A1_single\n",
    "#TypeError: sequence item 1: expected str instance, bytes found\n",
    "#gunzipped file and tried again with unzipped file--seemed to work, but want to use barcoded fastq instead so aborted\n",
    "#!gunzip fastq/A1_R1_001.fastq.gz\n",
    "#!tpp -bwa /s/software/bwa/bwa/bwa -ref /d/in16/u/sj003/refseqs/mbovis/Mbovis_AF2122_97.fasta \\\n",
    "#    -reads1 fastq/A1_R1_001.fastq -output tpp/tpp_test_A1_single\n",
    "\n",
    "#zip file on laptop, send to thoth, try again\n",
    "!gzip barcode_A1_R1_001.fastq \n",
    "!scp barcode_A1_R1_001.fastq.gz sj003@ssh.cryst.bbk.ac.uk:/d/in16/u/sj003/men_tnseq/fastq/\n",
    "!gunzip fastq/barcode_A1_R1_001.fastq.gz\n",
    "!tpp -bwa /s/software/bwa/bwa/bwa -ref /d/in16/u/sj003/refseqs/mbovis/Mbovis_AF2122_97.fasta \\\n",
    "    -reads1 fastq/barcode_A1_R1_001.fastq -output tpp/tpp_test_A1_single\n",
    "\n",
    "#barcoded reads were wrong--all had same sequence information (bug in program)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "3. Extract genomic part of read 1. This is the suffix following the transposon sequence pattern above. However, for reads coming from fragments shorter than the read length, the adapter might appear at the other end of R1, TACCACGACCA. If so, the adapter suffix is stripped off. (These are referred to as “truncated” reads, but they can still be mapped into the genome just fine by BWA.) The length of the genomic part must be at least 20 bp.\n",
    "\n",
    "4. Extract barcodes from read 2. Read 2 is searched for GATGGCCGGTGGATTTGTGnnnnnnnnnnTGGTCGTGGTAT”. The length of the barcode is typically 10 bp, but can be varaible, and must be between 5-15 bp.\n",
    "    -can tpp skip this step (it takes absolutely forever)?\n",
    "    -can this be done manually and fed in? or can I use header from .sam files in step 8 instead of barcode\n",
    "\n",
    "5. Extract genomic portions of read 2. This is the part following TGGTCGTGGTAT…. It is often the whole suffix of the read. However, if the read comes from a short DNA fragment that is shorter than the read length, the adapter on the other end might appear, in which case it is stripped off and the nucleotides in the middle representing the genomic insert, TGGTCGTGGTATxxxxxxxTAACAGGTTGGCTGATAAG. The insert must be at least 20 bp long (inserts shorter than this are discarded, as they might map to spurious locations in the genome).\n",
    "    -can this work in paired end without barcodes? \n",
    "\n",
    "6. Map genomic parts of R1 and R2 into the genome using BWA. Mismatches are allowed, but indels are ignored. No trimming is performed. BWA is run in ‘sampe’ mode (treating reads as pairs). Both reads of a pair must map (on opposite strands) to be counted.\n",
    "    \n",
    "\n",
    "7. Count the reads mapping to each TA site in the reference genome (or all sites for Tn5).\n",
    "\n",
    "8. Reduce raw read counts to unique template counts. Group reads by barcode AND mapping location of read 2 (aka fragment “endpoints”).\n",
    "    -i can do this with mapped reads\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#original read1\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "GTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACCGAGGCATCCCAGAGCTTGCCGGCTTCCTCGCCGCGCGGAATCTTCGAGAGCACCGGCCCGAAGAACGCCACACCATTGACATGGATCGTCGGCGTACCGACGTCCTCGCCCAC\n",
    "\n",
    "#trimmed read1\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "ACCGAGGCATCCCAGAGCTTGCCGGCTTCCTCGCCGCGCGGAATCTTCGAGAGCACCGGCCCGAAGAACGCCACACCATTGACATGGATCGTCGGCGTACCGACGTCCTCGCCCAC\n",
    "\n",
    "#original read2\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "ACTCACCGCTGCTGACTCCAACGAGAGGACCGCGCCATGCTCGAGAAGGCCCCCCAGAAGTCTGGCGCCGATTTCTGGTTCGATCCGCTGTGCCCGTGGTGCTGGATCACGTCGCGCTGGATCCTCGAGGTGGCAAAGGTCCGCGACATCG\n",
    "\n",
    "#trimmed read2\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "ACTCACCGCTGCTGACTCCAACGAGAGGACCGCGCCATGCTCGAGAAGGCCCCCCAGAAGTCTGGCGCCGATTTCTGGTTCGATCCGCTGTGCCCGTGGTGCTGGATCACGTCGCGCTGGATCCTCGAGGTGGCAAAGGTCCGCGACATCG\n",
    "\n",
    "#paired.barcodes2\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "XXXXXXXXXX\n",
    "\n",
    "#paired.genomic2\n",
    ">A01641:207:HNJLJDSX3:1:2678:31331:37059_:N:0:AACGTGAT_BC:GCCGCCC\n",
    "XXXXXXXXXX"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read 1 is successfully trimmed using himar1 sequence. Read2 isn't trimmed (can't find himar1 sequence). Can use these files for ds analysis? (_paired.trimmed1). This is final fastq for read1 for tpp. _paired.genomic2 is final file for read 2.\n",
    "\n",
    "I should maybe map using .trimmed1 and .trimmed2? or run tpp in single-end mode only\n",
    "\n",
    "Mendum, smith, butler and that group only gets single-end reads from sequencing. OK to ignore read 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternate (non-tpp) strategy\n",
    "1. add barcodes to read 1 from index reads \n",
    "2. fastp for illumina trimming and QC (.fastq, filtered and trimmed)\n",
    "3. then map with bwa-mem2 (.sam/.bam)\n",
    "4. then filter reads for ...TGGTA in first 32 bp of read and start/barcode (filtered and mapped)\n",
    "5. then count template reads mapping to each TA site (generate .wig files)\n",
    "6. use transit for essentiality/resampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline\n",
    "\n",
    "1. add barcodes to Read1 from index reads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_barcode(READ1, INDEX)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. fastp for qc and illumina adapter trimming\n",
    "\n",
    "I believe the reads have already been trimmed by genewiz when multiplexed as fastp doesnt detect many of the sequencing adapters. Could skip trimming, but useful for shortening test file and also qc and catching any untrimmed adapters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automatic adapter trimming\n",
    "#!fastp -i barcode_A1_R1_001.fastq.gz -o trimmed/test_A1_R1_trimmed.fastq --reads_to_process=10000\n",
    "# this trims all reads and nothing left!\n",
    "#Detecting adapter sequence for read1...\n",
    "#GTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACCGAGGCATCCCAGAGCTTGCCGG\n",
    "\n",
    "#this is trasposon sequence and also genomic dna. skip trimming and map without? more aggressive trimming using barcode processed file\n",
    "\n",
    "#disable adapter trimming or indicate only truseq adapter\n",
    "\n",
    "!fastp -a GATCGGAAGAGCACACGTCTGAACTCCAGTCAC -i barcode_A1_R1_001.fastq -o trimmed/test_A1_R1_trimmed.fastq --reads_to_process=10000\n",
    "\n",
    "#on thoth\n",
    "!fastp -a GATCGGAAGAGCACACGTCTGAACTCCAGTCAC -i fastq/barcode_A1_R1_001.fastq -o trimmed/barcode_A1_R1_001.fastq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Map read 1 with bwa-mem2 and sort\n",
    "    -can use TPP single end for this?\n",
    "\n",
    "    Use snakemake mapping/sorting script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda activate tnseq\n",
    "# # have to index file first for bwa-mem2\n",
    "# !bwa-mem2 index ref_seqs/Mbovis_AF2122-97.fasta\n",
    "# !bwa-mem2 -t 3 -C ref_seqs/Mbovis_AF2122-97.fasta data/fastp_out/A1_R1_trimmed.fastq > mapped_reads/A1_R1.sam\n",
    "# # bwa samse?\n",
    "# #!samtools view -Sb mapped_reads/A1_R1.bam\n",
    "# !samtools sort -T -o sorted_reads/A1_R1_sorted.sam -O sam mapped_reads/A1_R1.sam \n",
    "\n",
    "\n",
    "# with snakemake (maps, sorts, indexes and creates flagstats report)\n",
    "#make config.yaml file\n",
    "\n",
    "\n",
    "!cd ~/tn_seq/menadione_tnseq/\n",
    "!conda activate snakemake\n",
    "!snakemake -np -s ~/snakemake/tnseq/snakefile.smk\n",
    "!snakemake --cores 2 -s ~/snakemake/tnseq/snakefile.smk\n",
    "#!snakemake -np -s $my_path/snakemake/tnseq/snakefile.smk\n",
    "#!nohup snakemake --cores 8 -s $my_path/snakemake/map_bwa/pe/snakefile.smk > nohup_map.out 2>&1 &"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "samtools doesn't recognise the .sam format made by bwa in the -C mode includes quality scores, etc? When transit adds barcodes, it does it to names when making read files, so I've moved barcode to name so I don't have to worry about keeping any fields after the sequence.\n",
    "\n",
    "More info about -C parameter and formation of sam headers:\n",
    "\"Append append FASTA/Q comment to SAM output. This option can be used to transfer read meta information (e.g. barcode) to the SAM output. Note that the FASTA/Q comment (the string after a space in the header line) must conform the SAM spec (e.g. BC:Z:CGTAC). Malformated comments lead to incorrect SAM output.\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "samtools sort mapped_reads/test_A1_R1.sam \n",
    "[E::aux_parse] unrecognized type ':'\n",
    "[W::sam_read1_sam] Parse error at line 3\n",
    "samtools sort: truncated file. Aborting\n",
    "\n",
    "# mapped reads file from bwa mem\n",
    "@SQ\tSN:NC_002945.3\tLN:4345492\n",
    "@PG\tID:bwa\tPN:bwa\tVN:0.7.17-r1188\tCL:bwa mem -t 2 -C ref_seqs/Mbovis_AF2122-97.fasta trimmed/test_A1_R1_trimmed.fastq\n",
    "A01641:207:HNJLJDSX3:1:2678:31331:37059\t0\tNC_002945.3\t2736492\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACCGAGGCATCCCAGAGCTTGCCGGCTTCCTCGCCGCGCGGAATCTTCGAGAGCACCGGCCCGAAGAACGCCACACCATTGACATGGATCGTCGGCGTACCGACGTCCTCGCCCAC\tF::FF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFF:FFFFFF:FFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\t1:N:0:AACGTGAT BC:CGCAGTAT\n",
    "A01641:207:HNJLJDSX3:1:2678:31331:37059\t0\tNC_002945.3\t2736492\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACCGAGGCATCCCAGAGCTTGCCGGCTTCCTCGCCGCGCGGAATCTTCGAGAGCACCGGCCCGAAGAACGCCACACCATTGACATGGATCGTCGGCGTACCGACGTCCTCGCCCAC\tF::FF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFF:FFFFFF:FFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\t1:N:0:AACGTGAT BC:GAGCACCT\n",
    "A01641:207:HNJLJDSX3:1:2678:31331:37059\t0\tNC_002945.3\t2736492\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACCGAGGCATCCCAGAGCTTGCCGGCTTCCTCGCCGCGCGGAATCTTCGAGAGCACCGGCCCGAAGAACGCCACACCATTGACATGGATCGTCGGCGTACCGACGTCCTCGCCCAC\tF::FF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFF:FFFFFF:FFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\t1:N:0:AACGTGAT BC:CATACCAA\n",
    "\n",
    "# mapped reads from previous project\n",
    "@SQ\tSN:NC_000962.3\tLN:4411532\n",
    "@PG\tID:bwa\tPN:bwa\tVN:0.7.17-r1188\tCL:/Users/jenniferstiens/anaconda3/envs/tnseq/bin/bwa mem /Users/jenniferstiens/tn_seq/data/Mtb_H37Rv.fasta hiseq_trimmed_tpp_MtbA022.trimmed1\n",
    "GWNJ-0957:701:GW201208000:7:1101:13210:1309\t16\tNC_000962.3\t2970926\t60\t113M\t*\t0\t0\tCAGCAGCACCTCTCCCCAGAGGGCCGCAAAACCTATCGCAGCACGTTGCGGGGCTTCTTCGTGTCGGCCTACGAAATGGACCGGGTGCGCGACTATGTCGCAGACTCCCTGCC\t*\tNM:i:1\tMD:Z:64G48\tAS:i:108\tXS:i:0\n",
    "GWNJ-0957:701:GW201208000:7:1101:10419:1327\t0\tNC_000962.3\t589184\t60\t111M\t*\t0\t0\tTGAACGCGTTCTTCACCACGGCGATGGCGCTGCGTCTTCTTCACTCTGATCCCGGCAGTCCGGCGTGCCGGGTTTTTGAAGGCGAGCTGTACGATCACTGGACCATCGGGC\t*\tNM:i:9\tMD:Z:10G23C1C0G3G4C10A31A4G16\tAS:i:66\tXS:i:0\n",
    "GWNJ-0957:701:GW201208000:7:1101:14742:1327\t16\tNC_000962.3\t2468660\t60\t114M\t*\t0\t0\tGTGCGCGACAAGCGCACCGATCAGGCCTTGGCTAAGCTGAGCAGCGACGCGTTTCTCAAGCAGTACTCCCAGGTCGCAGTTACCTCGATCGACAAAATCGCGTACTGGTCGCAA\t*\tNM:i:8\tMD:Z:15T16C20C1G8T11A3G19T13\tAS:i:74\tXS:i:0\n",
    "GWNJ-0957:701:GW201208000:7:1101:31974:1397\t4\t*\t0\t0\t*\t*\t0\t0\tCTAGAGGGCCCAATTCGCCCTATAGTGAGTTGGATTGCTCTTCACTGGCCGTCGTTTATCAACGTCGTGACTGGGAAAACCCTGGCGTTACCCCACTTAATCGCCTTGCAGCC\t*\tAS:i:0\tXS:i:0\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "except for barcode, all my reads are identical. maybe this is something wrong with the barcoded file--since fastp also removed whole read.\n",
    "\n",
    "Yes--barcode script was wrong. fixed bug. This works for downstream snakemake trimming/mapping pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter out unmapped reads--part of snakemake pipeline. Changed snakemake to include header (@SQ) in output .sam file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. filter reads for transposon sequence (in first 32 bp?)\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "@HD\tVN:1.6\tSO:coordinate\n",
    "@SQ\tSN:NC_002945.3\tLN:4345492\n",
    "@PG\tID:bwa\tPN:bwa\tVN:0.7.17-r1188\tCL:bwa mem -t 2 -C ref_seqs/Mbovis_AF2122-97.fasta trimmed/test_5000_R1_trimmed.fastq\n",
    "@PG\tID:samtools\tPN:samtools\tPP:bwa\tVN:1.16.1\tCL:samtools sort -T sorted_reads/test_5000_R1 -O SAM mapped_reads/test_5000_R1.sam\n",
    "A01968:63:H77VYDSX5:4:1101:8757:2503_1:N:0:AACGTGAT_BC:GAACAACT\t16\tNC_002945.3\t20232\t60\t118M33S\t*\t0\t0\tATACGCGTTCGATGACCTCGGTGCCGGCCGCCGTAATCGGCGACTTATTTCGTGGGCGGGTGCGCAGTGGGCGGCGGGCTCCGTGCGAGATGCGTGCCAGGATGGCCAGCAATATGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
    "A01968:63:H77VYDSX5:4:1101:15022:1611_1:N:0:AACGTGAT_BC:CGCCACCT\t0\tNC_002945.3\t26121\t60\t32S119M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGCACGCTCATCGTGTGTCCTTGCGGCCAGGGATAGCGCCGTAGCTGATCGTAGATAGTGGTGCGGCACATGTCGTCGCCAGTGGCCGCCAGCAGCGGGTCCCGCGCCTGCGGTGTG\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:119\tAS:i:11XS:i:0\n",
    "A01968:63:H77VYDSX5:4:1101:20184:2942_1:N:0:AACGTGAT_BC:CCGACTGA\t0\tNC_002945.3\t31581\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGCGTCTCGCTGAGCGAGGCGGCGATGGAGACCGACGCAGAAACCCTGGCGGAAGCCATCCTGCTCACCGCCGACGTGTCCTGCCTTAAAGCGTTGCTGGAAGTACGCAACGAGATC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFF\tNM:i:0\tMD:Z:118\tAS:i:11XS:i:0\n",
    "A01968:63:H77VYDSX5:4:1101:24089:1595_1:N:0:AACGTGAT_BC:TATTCTAA\t16\tNC_002945.3\t35118\t60\t118M33S\t*\t0\t0\tGTACCGGGCGGCGCTGATGCTCAGCCTAAAAGATGCGATCAGCCGAGATAAACGGCGAATGGAAATGGGTATTACGAACTATTTCACAAAACTTCGCATTCCGGGTGCCCGAGTCATAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:11XS:i:0\n",
    "A01968:63:H77VYDSX5:4:1101:15311:2644_1:N:0:AACGTGAT_BC:AATCACTC\t0\tNC_002945.3\t37337\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAATCAGCTATCAGGACCTCATCGCGCGCGCGGCGGCATGCATCCCCCCGCTACGGCGTCTTGACATCAAACGCGGTGAACCCGTGCTGATCACCGCCCCCACCAACCTGGAATTCCT\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFFF,F:FFFFFFFFFF,FFFFF\tNM:i:2\tMD:Z:65C33A18\tAS:i:10XS:i:0\n",
    "A01968:63:H77VYDSX5:4:1101:28682:1689_1:N:0:AACGTGAT_BC:TGAGGCGA\t16\tNC_002945.3\t39750\t60\t109M33S\t*\t0\t0\tTCAGCGCGTCGAGGTCGTCGCTTTCGGCACGCAGGTCTGCCACGAACGGCCCAGGATCCGCCATCACCACCTCCTGAGGTAACAGTTCGTCGGGAAAGGCATGTTTGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\t,F::FF::F:F,FFFFFF:FF::FFFFF,FFFFFFF,,,FFFF:F,FFF,FFFFFF,FFF:,FFFFFF::F:F,FFFFFFFF,:FFF::FFF,FFFFFF:FFFFFFFF,F:F:FFFF:FFFFFF:F:FF::,F:,F,FFFFF\tNM:i:1\tMD:Z:0C108\tAS:i:108\tXS:i:0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slice <first 32 bp of sequence line> | grep <transposon tag> > concatenate to .tag_positive file\n",
    "\n",
    "\n",
    "\n",
    "#filter reads in samfile for transposon tag in sequence\n",
    "#from tpp (https://github.com/mad-lab/transit/blob/master/src/pytpp/tpp_tools.py)\n",
    "# find index of H[1..m] in G[1..n] with up to max mismatches\n",
    "# note: this find first match, not necessarily the best (with min mismatches)\n",
    "\n",
    "def mmfind1(G,n,H,m,max): # lengths; assume n>m\n",
    "  \"\"\"\n",
    "  A function to find matches of transposon sequence in read\n",
    "    Input               G                           sequence string\n",
    "                        n                           length of read sequence\n",
    "                        H                           pattern string\n",
    "                        m                           length of pattern\n",
    "                        max                         maximum mismatches\n",
    "    Output              i, -1                       start position of match, or -1 for no match\n",
    "  \"\"\"\n",
    "\n",
    "  a = G[:n].find(H[:m])\n",
    "  if a!=-1: return a # shortcut for perfect matches\n",
    "  for i in range(0,n-m):  # range of 0 to difference between seq length and len pattern\n",
    "    cnt = 0\n",
    "    for k in range(m):\n",
    "      if G[i+k]!=H[k]: cnt += 1\n",
    "      if cnt>max: break\n",
    "    if cnt<=max: return i\n",
    "  return -1\n",
    "    \n",
    "def find_tags(samfile, target_tag, mismatch_max=2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Find transposon tag in sequence of each read.\n",
    "\n",
    "      Input           samfile           file of mapped reads in .sam format\n",
    "                      target_tag        string that matches transposon sequence\n",
    "                      mismatch_max      number of mismatches allowed\n",
    "      Output          match_list        list of length of number of reads in .sam file \n",
    "                                        with match start position or -1 if no match\n",
    "    \"\"\"\n",
    "\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    import os\n",
    "\n",
    "    match_list = []\n",
    "    # this is giving errors in parsing\n",
    "    data = pd.read_csv(samfile, sep=\"\\t\", comment=\"@\", header=None, usecols=[0,1,2,3,4,5,6,7,8,9,10])\n",
    "    seqs = data[9]\n",
    "    #search string for transposon seq with 2 mismatches\n",
    "    for i in range(0, len(seqs)):\n",
    "      seq = seqs[i]\n",
    "      res = mmfind1(seq, len(seq), target_tag, len(target_tag), mismatch_max)\n",
    "      match_list.append(res)\n",
    "    #print(len(match_list))\n",
    "    print(\"Number of reads with no transposon tag: \", match_list.count(-1))\n",
    "    print(\"Number of reads with tag within first 32 bases: \", sum(1 for i in match_list if i >0 and i < 32))\n",
    "    bn = os.path.basename(samfile)\n",
    "    file_name = bn + \"_tags\" + \".pkl\"\n",
    "    with open(file_name, 'wb') as f:\n",
    "      pickle.dump(match_list, f)\n",
    "    return match_list\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reads with no transposon tag:  611\n",
      "Number of reads with tag within first 32 bases:  635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 14,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 14,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 14,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 14,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 14,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 14,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 15,\n",
       " 15,\n",
       " 15,\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_tags(\"sorted_reads/test_5000_R1.sam\", \"ACTTATCAGCCAACCTGTTA\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_barcodes(read):\n",
    "    \"\"\"\n",
    "    find molecular barcode and start of mapping in each read\n",
    "    \"\"\"\n",
    "    #extract molecular barcode from a single read and return tuple\n",
    "    \n",
    "    read_name = read.split()[0]\n",
    "    barcode = read_name.split(\"BC:\",1)[1]\n",
    "    read_start = read.split()[3]\n",
    "    bar_start = (barcode, read_start) \n",
    "    return bar_start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def filter_mapped_reads(sam_file, tag=\"ACTTATCAGCCAACCTGTTA\", mis_max=2, position_pkl=[]):\n",
    "  \"\"\" \n",
    "  \"\"\"\n",
    "  import sys\n",
    "  import pickle\n",
    "  import os\n",
    "  import pandas as pd\n",
    "  from operator import itemgetter\n",
    "  if position_pkl == []:\n",
    "    #create pos_list\n",
    "    pos_list = find_tags(sam_file, tag, mis_max)\n",
    "  else:\n",
    "    with open(position_pkl, 'rb') as f:\n",
    "      pos_list = pickle.load(f)\n",
    "  #read sam_file and sort lines between header and reads\n",
    "  header = []\n",
    "  reads  = []\n",
    "  barcode_list = []\n",
    "  good_reads = []\n",
    "  notags_reads = []\n",
    "  bad_reads = []\n",
    "  with open(sam_file, 'r') as f:\n",
    "    for line in f:\n",
    "      line = line.strip()\n",
    "      if line[0] == \"@\":\n",
    "        header.append(line)\n",
    "      else:\n",
    "        reads.append(line)\n",
    "  #compare number of reads in sam file to positions in pos_list\n",
    "  if len(reads)==len(pos_list):\n",
    "    for i in range(0, len(pos_list)):\n",
    "      if pos_list[i] != -1:\n",
    "        #find barcode/start combo\n",
    "        bc_start = find_barcodes(reads[i])\n",
    "        #add to list \n",
    "        barcode_list.append(bc_start) #list will essentially be sorted by read start because reads are sorted?\n",
    "        barcode_list.sort(key=itemgetter(0))  #maybe this will speed up search by barcode?\n",
    "        # if hasn't been added before, add read to good_reads\n",
    "        if barcode_list.count(bc_start) < 2:\n",
    "          good_reads.append(reads[i])\n",
    "        else:\n",
    "          bad_reads.append(reads[i])\n",
    "      else:\n",
    "        notags_reads.append(reads[i])\n",
    "    print(\"number of good reads (with tag): \", len(good_reads))\n",
    "    print(\"number of bad reads (with no tag): \", len(notags_reads))\n",
    "    print(\"Number of reads with duplicate barcode/starts: \", len(bad_reads))\n",
    "    bn = os.path.basename(sam_file)\n",
    "    outfile = \"tag_filtered_\" + bn\n",
    "    with open(outfile, 'w') as f:\n",
    "      for line in header:\n",
    "        f.write(f\"{line}\\n\")\n",
    "      for line in good_reads:\n",
    "        f.write(f\"{line}\\n\")\n",
    "  else:\n",
    "    sys.exit(\"reads file length doesn't match position file\")\n",
    "  \n",
    "  return barcode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reads with no transposon tag:  611\n",
      "Number of reads with tag within first 32 bases:  635\n",
      "number of good reads (with tag):  603\n",
      "number of bad reads (with no tag):  611\n",
      "Number of reads with duplicate barcode/starts:  32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('AAAAACAG', '274566'),\n",
       " ('AAAAACGC', '3897925'),\n",
       " ('AAAACTAG', '224546'),\n",
       " ('AAAAGTCT', '3648873'),\n",
       " ('AAAATGTA', '1384138'),\n",
       " ('AAAATTAA', '3373285'),\n",
       " ('AAACAAAG', '1165185'),\n",
       " ('AAACAAGT', '475611'),\n",
       " ('AAACATTG', '3973761'),\n",
       " ('AAACCTTT', '2558063'),\n",
       " ('AAACTTCC', '985131'),\n",
       " ('AAAGCAAA', '2948421'),\n",
       " ('AAATAAAC', '0'),\n",
       " ('AAATAGTT', '1616431'),\n",
       " ('AAATAGTT', '1616431'),\n",
       " ('AAATCGCA', '2075169'),\n",
       " ('AAATGGAA', '3140968'),\n",
       " ('AACAACCA', '2773084'),\n",
       " ('AACAACCT', '3499990'),\n",
       " ('AACAATGC', '3143862'),\n",
       " ('AACACACA', '0'),\n",
       " ('AACACCTC', '1696057'),\n",
       " ('AACATACT', '1349442'),\n",
       " ('AACATTAC', '165903'),\n",
       " ('AACCAATC', '925154'),\n",
       " ('AACCAGAA', '87640'),\n",
       " ('AACTAACC', '0'),\n",
       " ('AACTATTC', '3859816'),\n",
       " ('AACTCCAA', '1775261'),\n",
       " ('AACTGTCA', '3905561'),\n",
       " ('AAGACATG', '2183427'),\n",
       " ('AAGACCAA', '2883723'),\n",
       " ('AAGACGCT', '837708'),\n",
       " ('AAGACGTC', '812555'),\n",
       " ('AAGATCAC', '307998'),\n",
       " ('AAGATGTA', '3847260'),\n",
       " ('AAGCAATA', '1480841'),\n",
       " ('AAGCCAGA', '1973979'),\n",
       " ('AAGCGCTT', '99975'),\n",
       " ('AAGGAGCA', '3152336'),\n",
       " ('AAGGCCCA', '2253087'),\n",
       " ('AAGGCGCA', '363748'),\n",
       " ('AAGGTAGT', '4142776'),\n",
       " ('AAGTTTGG', '3599231'),\n",
       " ('AATACGTT', '177547'),\n",
       " ('AATAGATA', '2787004'),\n",
       " ('AATAGATC', '4247668'),\n",
       " ('AATAGCCG', '1352287'),\n",
       " ('AATCACTC', '37337'),\n",
       " ('AATCTACA', '159782'),\n",
       " ('AATGTTTG', '970924'),\n",
       " ('AATTAACT', '176268'),\n",
       " ('AATTAACT', '176268'),\n",
       " ('AATTACAT', '3128164'),\n",
       " ('AATTGACT', '0'),\n",
       " ('AATTGCAT', '1043320'),\n",
       " ('AATTTCCC', '3376955'),\n",
       " ('AATTTGAT', '3829208'),\n",
       " ('ACAAACAG', '3254248'),\n",
       " ('ACAAATAA', '3494487'),\n",
       " ('ACAAATTT', '388628'),\n",
       " ('ACAACGAA', '4210229'),\n",
       " ('ACAAGAAT', '2040005'),\n",
       " ('ACAAGAAT', '2040005'),\n",
       " ('ACAAGAAT', '0'),\n",
       " ('ACAAGTGA', '395005'),\n",
       " ('ACAATCCT', '3046336'),\n",
       " ('ACAATGCA', '0'),\n",
       " ('ACACTCTT', '2186689'),\n",
       " ('ACAGAACC', '2933863'),\n",
       " ('ACAGCCCC', '301037'),\n",
       " ('ACAGGATC', '47375'),\n",
       " ('ACAGGTAG', '2122305'),\n",
       " ('ACAGGTTA', '3514410'),\n",
       " ('ACAGTCAT', '280048'),\n",
       " ('ACAGTGGT', '0'),\n",
       " ('ACATAACG', '3546160'),\n",
       " ('ACATAGAA', '231801'),\n",
       " ('ACATCCCC', '2374575'),\n",
       " ('ACATCCGA', '4116895'),\n",
       " ('ACATGCCA', '3667275'),\n",
       " ('ACCAACTA', '941152'),\n",
       " ('ACCAAGAC', '388520'),\n",
       " ('ACCACTCT', '1956286'),\n",
       " ('ACCAGCAG', '3865744'),\n",
       " ('ACCCATCC', '4302845'),\n",
       " ('ACCCATTT', '573414'),\n",
       " ('ACCCTCCT', '1071009'),\n",
       " ('ACCGACGG', '1357292'),\n",
       " ('ACCGACGG', '1357296'),\n",
       " ('ACCGATTT', '438799'),\n",
       " ('ACCGCACA', '3852128'),\n",
       " ('ACCGGACG', '3520380'),\n",
       " ('ACCTACTG', '1537685'),\n",
       " ('ACCTTCCT', '2789051'),\n",
       " ('ACGAAATG', '2601592'),\n",
       " ('ACGACCGA', '2293447'),\n",
       " ('ACGATGAT', '0'),\n",
       " ('ACGCAGTG', '1280025'),\n",
       " ('ACGCCAAC', '890509'),\n",
       " ('ACGCTCCC', '925690'),\n",
       " ('ACGGGGTA', '1208405'),\n",
       " ('ACGGTCAT', '2857059'),\n",
       " ('ACGTTACG', '2700413'),\n",
       " ('ACTAAACC', '1492941'),\n",
       " ('ACTAACTT', '1955656'),\n",
       " ('ACTACCAA', '2357305'),\n",
       " ('ACTACCAT', '3337019'),\n",
       " ('ACTACGCG', '4148395'),\n",
       " ('ACTATCGT', '3006598'),\n",
       " ('ACTATCGT', '3006598'),\n",
       " ('ACTCAATC', '4099789'),\n",
       " ('ACTCGCTA', '3524653'),\n",
       " ('ACTCTATG', '1232032'),\n",
       " ('ACTCTATG', '1232032'),\n",
       " ('ACTCTGCC', '1275900'),\n",
       " ('ACTGTGAT', '2207679'),\n",
       " ('ACTTACAC', '1964214'),\n",
       " ('ACTTCATT', '2017698'),\n",
       " ('AGAAAATC', '1193249'),\n",
       " ('AGAAAGCA', '558545'),\n",
       " ('AGAACTAA', '1773171'),\n",
       " ('AGACCTTT', '500641'),\n",
       " ('AGACTAGA', '2201803'),\n",
       " ('AGACTAGA', '2201803'),\n",
       " ('AGAGAACG', '1916722'),\n",
       " ('AGAGCACC', '1124741'),\n",
       " ('AGAGGCGG', '3723077'),\n",
       " ('AGATAACT', '3619982'),\n",
       " ('AGATACTG', '0'),\n",
       " ('AGATGACA', '1501009'),\n",
       " ('AGATGCAT', '583027'),\n",
       " ('AGCCGACG', '2725768'),\n",
       " ('AGCCTTCC', '0'),\n",
       " ('AGCGCAAC', '2362959'),\n",
       " ('AGCGCCCA', '4223560'),\n",
       " ('AGCGTAAC', '454023'),\n",
       " ('AGCTACAC', '3901948'),\n",
       " ('AGCTACAG', '3958909'),\n",
       " ('AGCTCCTA', '466100'),\n",
       " ('AGCTCGTT', '522787'),\n",
       " ('AGCTCTAA', '369058'),\n",
       " ('AGGAAATG', '896544'),\n",
       " ('AGGCAGCT', '1276930'),\n",
       " ('AGGCTGAC', '3386631'),\n",
       " ('AGTAAAGT', '2195162'),\n",
       " ('AGTAAATA', '3510220'),\n",
       " ('AGTAGCCC', '1004830'),\n",
       " ('AGTCCGAG', '925690'),\n",
       " ('AGTCGAGG', '1352283'),\n",
       " ('AGTCTCGC', '543795'),\n",
       " ('AGTGACTT', '3702549'),\n",
       " ('AGTGGCAT', '1352191'),\n",
       " ('ATAAACGG', '4093935'),\n",
       " ('ATAACGAG', '0'),\n",
       " ('ATAATTTT', '1162150'),\n",
       " ('ATACAACT', '2122205'),\n",
       " ('ATACACGC', '0'),\n",
       " ('ATATCAAA', '4303873'),\n",
       " ('ATATGCTT', '1510039'),\n",
       " ('ATATTCAA', '3444385'),\n",
       " ('ATATTTCT', '0'),\n",
       " ('ATCAACAC', '229915'),\n",
       " ('ATCCACAA', '2015049'),\n",
       " ('ATCCCATA', '2475182'),\n",
       " ('ATCCCTTT', '856144'),\n",
       " ('ATCCGAAC', '918008'),\n",
       " ('ATCCGAAC', '918008'),\n",
       " ('ATCCTCAT', '2788695'),\n",
       " ('ATCCTGTT', '0'),\n",
       " ('ATCCTGTT', '0'),\n",
       " ('ATCGCGGC', '3920426'),\n",
       " ('ATCGGAGA', '2048202'),\n",
       " ('ATCTACAT', '0'),\n",
       " ('ATCTGTGA', '3842501'),\n",
       " ('ATGAATAT', '3375747'),\n",
       " ('ATGATCCT', '3768899'),\n",
       " ('ATGCCCCC', '3784342'),\n",
       " ('ATGGCCAA', '3460259'),\n",
       " ('ATGGCGGG', '2557705'),\n",
       " ('ATGGGCAC', '3268475'),\n",
       " ('ATGGTCAA', '3353319'),\n",
       " ('ATGTAAGG', '3395920'),\n",
       " ('ATGTCAAA', '160624'),\n",
       " ('ATGTCGTG', '3822317'),\n",
       " ('ATTACAAA', '0'),\n",
       " ('ATTACATT', '1669127'),\n",
       " ('ATTACCAA', '1956777'),\n",
       " ('ATTAGGGT', '4315069'),\n",
       " ('ATTCAGTA', '2601625'),\n",
       " ('ATTCCTCG', '2691651'),\n",
       " ('ATTCGTGG', '923833'),\n",
       " ('ATTCGTGG', '923833'),\n",
       " ('ATTGAAGT', '2202647'),\n",
       " ('ATTGAAGT', '2202647'),\n",
       " ('ATTGCGAG', '918006'),\n",
       " ('ATTGTCCG', '548378'),\n",
       " ('ATTTAAGA', '176424'),\n",
       " ('ATTTCCTA', '2883723'),\n",
       " ('ATTTTGGA', '2976310'),\n",
       " ('ATTTTTAC', '2869305'),\n",
       " ('CAAAAAGA', '3069342'),\n",
       " ('CAAAGACG', '3723077'),\n",
       " ('CAAAGATG', '468017'),\n",
       " ('CAAAGATT', '475403'),\n",
       " ('CAAAGTAC', '627749'),\n",
       " ('CAAAGTAT', '573387'),\n",
       " ('CAAAGTTA', '4313507'),\n",
       " ('CAACCGTC', '3288129'),\n",
       " ('CAACGCAC', '514243'),\n",
       " ('CAACTGGC', '2616325'),\n",
       " ('CAAGCAGA', '0'),\n",
       " ('CAAGCCCA', '1517377'),\n",
       " ('CAAGGTTA', '3525866'),\n",
       " ('CAATATCT', '561373'),\n",
       " ('CAATATTG', '3444337'),\n",
       " ('CAATTCCT', '575415'),\n",
       " ('CAATTTAA', '3178462'),\n",
       " ('CACAAGGA', '1711155'),\n",
       " ('CACAATAC', '0'),\n",
       " ('CACACCGC', '4288641'),\n",
       " ('CACAGGTC', '121806'),\n",
       " ('CACATAAT', '2930793'),\n",
       " ('CACATGCC', '1760529'),\n",
       " ('CACCAAAC', '4170071'),\n",
       " ('CACCATTG', '2136794'),\n",
       " ('CACGCAGA', '3368646'),\n",
       " ('CACGCTAT', '1965457'),\n",
       " ('CACGTGAA', '1379737'),\n",
       " ('CACTCACG', '2596621'),\n",
       " ('CACTCTAT', '1234657'),\n",
       " ('CACTTAGG', '2883723'),\n",
       " ('CACTTGGG', '2999533'),\n",
       " ('CAGACCTA', '3722122'),\n",
       " ('CAGATCTA', '2072383'),\n",
       " ('CAGCATCA', '2269079'),\n",
       " ('CAGCCGCA', '2871085'),\n",
       " ('CAGTGACA', '977272'),\n",
       " ('CAGTGGTG', '824174'),\n",
       " ('CAGTGTAC', '57868'),\n",
       " ('CAGTGTTC', '3204130'),\n",
       " ('CATAACCA', '547822'),\n",
       " ('CATAATCG', '4283122'),\n",
       " ('CATACCAA', '4166393'),\n",
       " ('CATACTTT', '2102047'),\n",
       " ('CATATTAG', '2106474'),\n",
       " ('CATCCCCC', '3448247'),\n",
       " ('CATGCTCC', '0'),\n",
       " ('CATGTCAT', '1604510'),\n",
       " ('CATTATCA', '1274093'),\n",
       " ('CATTTTTC', '4066934'),\n",
       " ('CCAAATAT', '2071996'),\n",
       " ('CCAACAGC', '0'),\n",
       " ('CCAACCTA', '2883723'),\n",
       " ('CCAATACC', '3397372'),\n",
       " ('CCACCCTG', '1616130'),\n",
       " ('CCACCGAC', '0'),\n",
       " ('CCACTGCG', '3850241'),\n",
       " ('CCAGCCTT', '2529108'),\n",
       " ('CCATACGA', '1397158'),\n",
       " ('CCATCAAC', '2087073'),\n",
       " ('CCATCGCA', '660680'),\n",
       " ('CCATGGAC', '0'),\n",
       " ('CCCACTTT', '0'),\n",
       " ('CCCAGACT', '2207493'),\n",
       " ('CCCCACCT', '3367581'),\n",
       " ('CCCCTAAC', '645188'),\n",
       " ('CCCCTAAG', '3160898'),\n",
       " ('CCCCTGCT', '1991237'),\n",
       " ('CCCGGCAG', '1675020'),\n",
       " ('CCCTGAAG', '3320660'),\n",
       " ('CCCTGTTT', '853667'),\n",
       " ('CCCTTACT', '3398609'),\n",
       " ('CCGACCCT', '2387889'),\n",
       " ('CCGACTCT', '1307048'),\n",
       " ('CCGACTGA', '31581'),\n",
       " ('CCGCGACC', '943994'),\n",
       " ('CCGGCCAC', '3512077'),\n",
       " ('CCGGGAGA', '2056208'),\n",
       " ('CCGGGTTT', '3831850'),\n",
       " ('CCGTAAGC', '0'),\n",
       " ('CCGTAATG', '3200697'),\n",
       " ('CCTCATCC', '3904998'),\n",
       " ('CCTGATAA', '870742'),\n",
       " ('CCTGCAGT', '2953752'),\n",
       " ('CCTGCGCC', '2086116'),\n",
       " ('CCTTATCC', '2296940'),\n",
       " ('CGAACCTA', '1620795'),\n",
       " ('CGAAGCGG', '2005628'),\n",
       " ('CGAAGTAA', '2294833'),\n",
       " ('CGAATTGC', '1357470'),\n",
       " ('CGAGACAG', '39857'),\n",
       " ('CGAGACAT', '2729375'),\n",
       " ('CGAGACGC', '595088'),\n",
       " ('CGATGACT', '222102'),\n",
       " ('CGCAACTT', '3934575'),\n",
       " ('CGCAAGGC', '0'),\n",
       " ('CGCCAATC', '528347'),\n",
       " ('CGCCACCT', '26121'),\n",
       " ('CGCCAGCA', '2556188'),\n",
       " ('CGCCGAAC', '2674221'),\n",
       " ('CGCCGTAC', '852818'),\n",
       " ('CGCGTACG', '3540539'),\n",
       " ('CGCTAAGG', '2883723'),\n",
       " ('CGCTCCTC', '4053752'),\n",
       " ('CGGACACT', '162598'),\n",
       " ('CGGCCCGT', '420404'),\n",
       " ('CGGCGTCA', '1209806'),\n",
       " ('CGGCGTCA', '1209806'),\n",
       " ('CGGGCTTT', '2179617'),\n",
       " ('CGGTCAAT', '2367435'),\n",
       " ('CGGTTGGC', '3272799'),\n",
       " ('CGTAATAA', '551845'),\n",
       " ('CGTAATAA', '551845'),\n",
       " ('CGTATCCA', '4053752'),\n",
       " ('CGTCGCAA', '695889'),\n",
       " ('CGTGACAA', '3912539'),\n",
       " ('CGTGCCCG', '3003340'),\n",
       " ('CGTGCCCG', '3003340'),\n",
       " ('CGTGGGAG', '2191082'),\n",
       " ('CGTTGTTT', '268158'),\n",
       " ('CTAACAGT', '3462593'),\n",
       " ('CTAAGCGG', '3769375'),\n",
       " ('CTACGGAC', '2883723'),\n",
       " ('CTACTTAA', '647077'),\n",
       " ('CTAGCACT', '3828970'),\n",
       " ('CTAGCACT', '3828970'),\n",
       " ('CTAGTATG', '4141420'),\n",
       " ('CTCACTTC', '2700413'),\n",
       " ('CTCACTTC', '2700413'),\n",
       " ('CTCATCTA', '1314045'),\n",
       " ('CTCATCTA', '1314045'),\n",
       " ('CTCCCACT', '2476770'),\n",
       " ('CTCCCCCC', '3268312'),\n",
       " ('CTCCCTAC', '2349218'),\n",
       " ('CTCGAAAA', '3779515'),\n",
       " ('CTCGACGC', '496032'),\n",
       " ('CTCGCCGT', '0'),\n",
       " ('CTCGGCGA', '3651635'),\n",
       " ('CTCGTAAA', '1994326'),\n",
       " ('CTCGTGAC', '4027272'),\n",
       " ('CTGCAAAA', '1539502'),\n",
       " ('CTGCAAAA', '1539502'),\n",
       " ('CTGCGTAA', '3510905'),\n",
       " ('CTGCTCCT', '2323670'),\n",
       " ('CTTAGGCG', '3745192'),\n",
       " ('CTTCGTGG', '3926596'),\n",
       " ('CTTGCCCC', '1748657'),\n",
       " ('CTTGCTCC', '2551825'),\n",
       " ('CTTTAAAC', '2725529'),\n",
       " ('GAAACAAC', '2630679'),\n",
       " ('GAAACCAC', '4334043'),\n",
       " ('GAAACCAG', '1281722'),\n",
       " ('GAAAGCTA', '190977'),\n",
       " ('GAAATGTT', '491869'),\n",
       " ('GAACTATT', '4168829'),\n",
       " ('GAAGATAA', '0'),\n",
       " ('GAAGCCTA', '651696'),\n",
       " ('GAAGCTAT', '3521561'),\n",
       " ('GAAGGTCC', '1549263'),\n",
       " ('GAATAAAT', '1019979'),\n",
       " ('GAATATTT', '1779433'),\n",
       " ('GAATGTGA', '1878101'),\n",
       " ('GAATTCTT', '223131'),\n",
       " ('GAATTTAG', '2800426'),\n",
       " ('GACAAAAT', '2534983'),\n",
       " ('GACACAGC', '3575760'),\n",
       " ('GACACGAG', '1066421'),\n",
       " ('GACCAAGC', '51951'),\n",
       " ('GACCACAA', '2981237'),\n",
       " ('GACCAGTG', '702977'),\n",
       " ('GACCCTGC', '1741622'),\n",
       " ('GACCTAAT', '1923751'),\n",
       " ('GACCTGAA', '0'),\n",
       " ('GACCTTGT', '4234348'),\n",
       " ('GACCTTGT', '4234348'),\n",
       " ('GACGCCCA', '3238510'),\n",
       " ('GACGCGAA', '3237028'),\n",
       " ('GACGGAGA', '2046725'),\n",
       " ('GACGGCCC', '631073'),\n",
       " ('GACTAATT', '3796239'),\n",
       " ('GACTATAT', '3393264'),\n",
       " ('GACTGTGA', '2821958'),\n",
       " ('GACTTCGT', '2916736'),\n",
       " ('GAGAACCC', '138138'),\n",
       " ('GAGACTAG', '3859816'),\n",
       " ('GAGAGACA', '2425313'),\n",
       " ('GAGATTTG', '3721209'),\n",
       " ('GAGGGACA', '0'),\n",
       " ('GAGGGCTC', '1276944'),\n",
       " ('GAGGGTGC', '3863850'),\n",
       " ('GATATATA', '2762603'),\n",
       " ('GATATCAC', '0'),\n",
       " ('GATCACAC', '2074888'),\n",
       " ('GATCACCT', '4283460'),\n",
       " ('GATCCGCT', '341987'),\n",
       " ('GATTACGT', '382825'),\n",
       " ('GCAACCAA', '188101'),\n",
       " ('GCAATCTA', '0'),\n",
       " ('GCACACAC', '3038147'),\n",
       " ('GCACGCAA', '2677761'),\n",
       " ('GCACTAAC', '2083303'),\n",
       " ('GCACTTGT', '2609284'),\n",
       " ('GCATAACA', '1693625'),\n",
       " ('GCATGAAA', '3669913'),\n",
       " ('GCATTCCG', '2187030'),\n",
       " ('GCCATCAC', '737482'),\n",
       " ('GCCCAAAG', '4069407'),\n",
       " ('GCCCAAGG', '0'),\n",
       " ('GCCCAATT', '1971399'),\n",
       " ('GCCCACGC', '0'),\n",
       " ('GCCGACAA', '752423'),\n",
       " ('GCCGCCCT', '1111361'),\n",
       " ('GCCGCCCT', '1111361'),\n",
       " ('GCCGCCCT', '1111361'),\n",
       " ('GCCGGCGG', '1935084'),\n",
       " ('GCCTCGGC', '2369829'),\n",
       " ('GCCTTAAA', '2332345'),\n",
       " ('GCGAAGAG', '3657895'),\n",
       " ('GCGCGCAT', '0'),\n",
       " ('GCGCTATG', '2354053'),\n",
       " ('GCGTCGAC', '992103'),\n",
       " ('GCGTCGAC', '992103'),\n",
       " ('GCTAGACC', '4262890'),\n",
       " ('GCTCAGGA', '3670104'),\n",
       " ('GCTCCTGG', '1778019'),\n",
       " ('GGAATTCT', '878309'),\n",
       " ('GGACACGA', '4312798'),\n",
       " ('GGACAGCC', '3251794'),\n",
       " ('GGACCCTG', '1778730'),\n",
       " ('GGATAACG', '3852657'),\n",
       " ('GGATGTAC', '364277'),\n",
       " ('GGCAAACA', '4216050'),\n",
       " ('GGCACGAC', '154949'),\n",
       " ('GGCACTAA', '384582'),\n",
       " ('GGCATGTT', '2149568'),\n",
       " ('GGCCACTT', '2084754'),\n",
       " ('GGCCCAAT', '122870'),\n",
       " ('GGCCGCCC', '373583'),\n",
       " ('GGCGCGAA', '1166620'),\n",
       " ('GGCTAATC', '3566053'),\n",
       " ('GGCTTTTA', '1165724'),\n",
       " ('GGGCCTAT', '3638327'),\n",
       " ('GGGCTAAA', '0'),\n",
       " ('GGGGGGGG', '533657'),\n",
       " ('GGGGGGGG', '573428'),\n",
       " ('GGGGGGGG', '642706'),\n",
       " ('GGGGGGGG', '754840'),\n",
       " ('GGGGGGGG', '758941'),\n",
       " ('GGGGGGGG', '1124741'),\n",
       " ('GGGGGGGG', '1210940'),\n",
       " ('GGGGGGGG', '1283827'),\n",
       " ('GGGGGGGG', '1535492'),\n",
       " ('GGGGGGGG', '1598628'),\n",
       " ('GGGGGGGG', '2733020'),\n",
       " ('GGGGGGGG', '2744873'),\n",
       " ('GGGGGGGG', '3140337'),\n",
       " ('GGGGGGTG', '3254215'),\n",
       " ('GGGGTGCG', '4120066'),\n",
       " ('GGGGTGGT', '2730604'),\n",
       " ('GGTAAATC', '920700'),\n",
       " ('GGTAACTT', '4200966'),\n",
       " ('GGTATAAT', '3380597'),\n",
       " ('GGTATTAG', '3055285'),\n",
       " ('GGTGGCAG', '3002083'),\n",
       " ('GGTTCAAC', '3465963'),\n",
       " ('GGTTCAAC', '3465963'),\n",
       " ('GGTTCAAC', '3465963'),\n",
       " ('GGTTTACC', '0'),\n",
       " ('GTAAAAGA', '3896893'),\n",
       " ('GTAAATTT', '3569834'),\n",
       " ('GTAATAAA', '1531619'),\n",
       " ('GTAATCAG', '3527076'),\n",
       " ('GTAATGCA', '800222'),\n",
       " ('GTACATGC', '3096307'),\n",
       " ('GTACCGAT', '1779433'),\n",
       " ('GTATGAGC', '2819624'),\n",
       " ('GTATTACT', '1281000'),\n",
       " ('GTCCAACT', '3254215'),\n",
       " ('GTCCACAC', '4235791'),\n",
       " ('GTCCGTAA', '4279411'),\n",
       " ('GTCCGTTA', '938819'),\n",
       " ('GTCTAATT', '3523385'),\n",
       " ('GTCTACTT', '2193045'),\n",
       " ('GTCTCAAC', '3278585'),\n",
       " ('GTGACCCA', '169495'),\n",
       " ('GTGATAAC', '3416610'),\n",
       " ('GTGCACTG', '833502'),\n",
       " ('GTGCATGA', '925692'),\n",
       " ('GTGCCGAG', '3969507'),\n",
       " ('GTTAAGAC', '3681921'),\n",
       " ('GTTATTTT', '1400712'),\n",
       " ('GTTATTTT', '1400712'),\n",
       " ('GTTCATAT', '3337936'),\n",
       " ('GTTGGATA', '642706'),\n",
       " ('GTTTCATG', '0'),\n",
       " ('GTTTTACT', '1985715'),\n",
       " ('TAAAACTC', '3768560'),\n",
       " ('TAAAATCA', '165864'),\n",
       " ('TAAAATTA', '2816170'),\n",
       " ('TAAAGCAG', '0'),\n",
       " ('TAAAGTGC', '3263053'),\n",
       " ('TAAAGTGC', '3263053'),\n",
       " ('TAAATAGT', '3732352'),\n",
       " ('TAACAGTT', '0'),\n",
       " ('TAACATGA', '2954618'),\n",
       " ('TAACGGAA', '715727'),\n",
       " ('TAACGGCA', '2195180'),\n",
       " ('TAACTGCT', '0'),\n",
       " ('TAAGGTAC', '2597824'),\n",
       " ('TAAGTCCA', '1604277'),\n",
       " ('TAATCAAC', '2028715'),\n",
       " ('TAATCAGC', '2314015'),\n",
       " ('TAATCAGC', '2314015'),\n",
       " ('TAATCATC', '2887188'),\n",
       " ('TAATGCCA', '695523'),\n",
       " ('TAATGTCC', '3158760'),\n",
       " ('TAATGTTA', '738583'),\n",
       " ('TACCATGA', '2861954'),\n",
       " ('TACGATAC', '364282'),\n",
       " ('TACGCTGT', '991250'),\n",
       " ('TACTAATT', '1031012'),\n",
       " ('TACTACAA', '2967757'),\n",
       " ('TACTACAA', '0'),\n",
       " ('TACTTGAC', '3731089'),\n",
       " ('TACTTGCT', '2017115'),\n",
       " ('TACTTTTC', '108988'),\n",
       " ('TACTTTTC', '108988'),\n",
       " ('TAGAACCA', '2597440'),\n",
       " ('TAGACTCA', '2181115'),\n",
       " ('TAGCACCC', '0'),\n",
       " ('TAGCATAA', '454023'),\n",
       " ('TAGGCCGT', '134437'),\n",
       " ('TAGGCCGT', '134437'),\n",
       " ('TAGTCAAG', '1369436'),\n",
       " ('TAGTGTGT', '3718413'),\n",
       " ('TATACAGA', '1118720'),\n",
       " ('TATGATCT', '3373687'),\n",
       " ('TATTAATT', '3871092'),\n",
       " ('TCAAAATG', '934228'),\n",
       " ('TCAACAGC', '921005'),\n",
       " ('TCAATTAC', '3586266'),\n",
       " ('TCAATTGG', '3544928'),\n",
       " ('TCACAATG', '3511515'),\n",
       " ('TCACAGGT', '4200957'),\n",
       " ('TCACATGC', '1420823'),\n",
       " ('TCACCAAG', '0'),\n",
       " ('TCACCTGT', '2896113'),\n",
       " ('TCACTTAC', '4294475'),\n",
       " ('TCACTTCA', '361207'),\n",
       " ('TCATGCAA', '2857505'),\n",
       " ('TCATGTTA', '1872929'),\n",
       " ('TCATTCAC', '1349985'),\n",
       " ('TCATTCAT', '1057494'),\n",
       " ('TCCAAATA', '439737'),\n",
       " ('TCCACCCC', '1483152'),\n",
       " ('TCCACTCT', '0'),\n",
       " ('TCCATAAG', '4032275'),\n",
       " ('TCCATTCC', '0'),\n",
       " ('TCCCGAAT', '528347'),\n",
       " ('TCCGACAC', '3868104'),\n",
       " ('TCGAAAAA', '3343507'),\n",
       " ('TCGATCGC', '1331494'),\n",
       " ('TCGCAAAT', '1115790'),\n",
       " ('TCGCGATA', '3917018'),\n",
       " ('TCGCTCCC', '2367435'),\n",
       " ('TCGTCCAG', '854455'),\n",
       " ('TCGTCTTA', '3884021'),\n",
       " ('TCGTTGAC', '2335410'),\n",
       " ('TCGTTGAC', '2335410'),\n",
       " ('TCTAAAAA', '941152'),\n",
       " ('TCTAACGC', '3970721'),\n",
       " ('TCTACTAT', '0'),\n",
       " ('TCTATGAC', '0'),\n",
       " ('TCTCAACA', '2535730'),\n",
       " ('TCTGCTAA', '1991193'),\n",
       " ('TCTGGCCA', '1117262'),\n",
       " ('TGAAGTGA', '3998427'),\n",
       " ('TGACAAGC', '2672094'),\n",
       " ('TGACATAT', '389473'),\n",
       " ('TGACATCT', '1041105'),\n",
       " ('TGACATGA', '0'),\n",
       " ('TGATAAAA', '1708144'),\n",
       " ('TGATCGCC', '2911389'),\n",
       " ('TGATCTTA', '583027'),\n",
       " ('TGCACTTA', '1850708'),\n",
       " ('TGCATTAA', '3719320'),\n",
       " ('TGCCCCTT', '1357857'),\n",
       " ('TGCCGAAC', '331336'),\n",
       " ('TGCCTTCC', '1287953'),\n",
       " ('TGCGTCTC', '0'),\n",
       " ('TGCGTGCT', '2916021'),\n",
       " ('TGCTAAAA', '572609'),\n",
       " ('TGCTCACT', '3623533'),\n",
       " ('TGCTCACT', '3623533'),\n",
       " ('TGCTTGTG', '2354053'),\n",
       " ('TGGACCCA', '3082057'),\n",
       " ('TGGTTCAA', '1912477'),\n",
       " ('TGTAGATG', '0'),\n",
       " ('TGTCAGTA', '3444337'),\n",
       " ('TGTCCATC', '2426612'),\n",
       " ('TGTTCCAG', '87311'),\n",
       " ('TGTTCCTG', '3045250'),\n",
       " ('TGTTGTCT', '0'),\n",
       " ('TTAACAAC', '3722332'),\n",
       " ('TTAAGGGA', '4108672'),\n",
       " ('TTAATCTA', '3804604'),\n",
       " ('TTACCTGC', '3672454'),\n",
       " ('TTACGCAC', '1950073'),\n",
       " ('TTAGACAT', '181212'),\n",
       " ('TTATAACG', '3673147'),\n",
       " ('TTATGATT', '1121348'),\n",
       " ('TTCAACCC', '1302082'),\n",
       " ('TTCAACGC', '0'),\n",
       " ('TTCACGTC', '394906'),\n",
       " ('TTCCAAAA', '253845'),\n",
       " ('TTCCAGCA', '1307284'),\n",
       " ('TTCCATTT', '580902'),\n",
       " ('TTCCCACT', '3584406'),\n",
       " ('TTCCTCTA', '0'),\n",
       " ('TTCGTACT', '223235'),\n",
       " ('TTCGTGCA', '2286455'),\n",
       " ('TTGCAATT', '1779433'),\n",
       " ('TTGCACGA', '1994898'),\n",
       " ('TTGCATTC', '0'),\n",
       " ('TTGTGCTA', '592636'),\n",
       " ('TTTAAGCG', '2978486'),\n",
       " ('TTTATTGC', '1295441'),\n",
       " ('TTTCGTAG', '2318816'),\n",
       " ('TTTCTCTT', '547791'),\n",
       " ('TTTTAACT', '1731629'),\n",
       " ('TTTTAATT', '1536283'),\n",
       " ('TTTTAATT', '1536283'),\n",
       " ('TTTTAATT', '1536283'),\n",
       " ('TTTTTCCA', '3697348')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_mapped_reads(\"sorted_reads/test_5000_R1.sam\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Filter for PCR amplification reads--reduce to single template counts (do at same time as finding transposon tag)\n",
    "\n",
    "Will I be able to use the start position to do this?\n",
    "\n",
    "Need to use new program since barcode now in different place.\n",
    "\n",
    "1) function to identify barcode and start of each read\n",
    "2) when found good_read --> apply function and compare barcode:start to list\n",
    "3) if not in list --> append to good_reads, else to bad_reads\n",
    "\n",
    "Need to parse sam flag--position is strand specific (sam flag is 2nd field (index=1) and 0 is forward, 16 is reverse). Start position is left-most position of alignment, so for forward it is the start, but for reverse it is the end. To get reverse start, have to add to position the length of the alignment. This is given in the CIGAR string (6th field, so index=5). \n",
    "\n",
    "reverse strand (samflag=16): 118M33S  so first number up to 'M' is length of match (33S is softclip of transposon seq)\n",
    "forward strand (samflag=0): 33S118M so number AFTER S. But this wont need to be parsed, as left-most position is correct for forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse samflag \n",
    "\n",
    "def parse_samflag(read):\n",
    "    \"\"\"\n",
    "    Function to determine strand orientation of alignment from read header\n",
    "    \"\"\"\n",
    "    samflag = read.split()[1]\n",
    "    if samflag==\"16\":\n",
    "        strand = \"R\"\n",
    "    elif samflag==\"0\":\n",
    "        strand = \"F\"\n",
    "    else:\n",
    "        strand = \"*\"\n",
    "    return strand\n",
    "\n",
    "def align_len(read):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to determine read alignment length from start position of reads \n",
    "    filtered for transposon tag (should have soft-clipped ~30-33 before gDNA)\n",
    "\n",
    "    Some cases may exist like 29S89M33S--transposon at both ends? or adapter not trimmed? \n",
    "    if forward, can ignore last part of alignment, if reverse, can ignore first part\n",
    "    \n",
    "    \"\"\"\n",
    "    import re\n",
    "    cig = read.split()[5]\n",
    "    cig_list = parse_cigar(cig)\n",
    "    cig_last = len(cig_list) - 1\n",
    "    strand = parse_samflag(read)\n",
    "    if strand == \"R\":\n",
    "        # assume alignment without extra bases at 3' end \n",
    "        #make sure no soft-clipping at start\n",
    "        #if 'M' in cig_list[0]:  #this could be 3' end alignment\n",
    "        if \"S\" in cig_list[cig_last]:  #check for soft-clipping (tag) at 5' end of read\n",
    "            match_len = cig_list[cig_last - 1] #aligned part is next part\n",
    "            #soft_clip = cig_list[cig_last]\n",
    "            match_len = int(match_len.split(\"M\")[0])\n",
    "        else:\n",
    "            match_len = 0\n",
    "        \n",
    "    elif strand == \"F\":\n",
    "        #for F strand, softclipped bases at 3' end are cigar_list[2] and can be ignored?, if no soft-clipping at 5' end, will be no tag\n",
    "        match_len = cig_list[1]\n",
    "        match_len = int(match_len.split(\"M\")[0])\n",
    "        #soft_clip = cig_list[0]\n",
    "    else:\n",
    "        match_len = 0\n",
    "    return match_len\n",
    "\n",
    "def rev_start(pos, alignlen):\n",
    "    \"\"\"\n",
    "    Function to find start position for reverse aligned reads\n",
    "    \"\"\"\n",
    "    # need to subtract one from start to match actual start position of read on + strand\n",
    "    #  and -1 for zero-index adjustment when adding\n",
    "    rev_start = int(pos) + alignlen - 2\n",
    "    return rev_start\n",
    "\n",
    "def parse_cigar(cigar):\n",
    "    \"\"\"\n",
    "    Parse cigar string \n",
    "    regex: \\*|([0-9]+[MIDNSHPX=])+\n",
    "    \"\"\"\n",
    "    import re\n",
    "    #keep delimiter\n",
    "    #cig_list = re.split('([0-9]+M|[0-9]+S)', cigar)\n",
    "    #remove empty strings\n",
    "    #cig_list = [i for i in cig_list if i]\n",
    "\n",
    "    #find all number/letter combos in cigar string\n",
    "    cig_list = re.findall(r'[0-9]+[MIDNSHPX=]+', cigar)\n",
    "    return cig_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "109\n",
      "118\n",
      "95\n",
      "118\n",
      "69\n",
      "118\n",
      "79\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "121\n",
      "121\n",
      "116\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "72\n",
      "118\n",
      "119\n",
      "118\n",
      "113\n",
      "118\n",
      "114\n",
      "118\n",
      "88\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "89\n",
      "89\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "85\n",
      "118\n",
      "118\n",
      "87\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "111\n",
      "85\n",
      "118\n",
      "118\n",
      "86\n",
      "94\n",
      "118\n",
      "110\n",
      "118\n",
      "107\n",
      "118\n",
      "119\n",
      "118\n",
      "48\n",
      "93\n",
      "101\n",
      "118\n",
      "87\n",
      "93\n",
      "118\n",
      "119\n",
      "118\n",
      "93\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "47\n",
      "116\n",
      "116\n",
      "92\n",
      "59\n",
      "34\n",
      "118\n",
      "31\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "109\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "75\n",
      "118\n",
      "101\n",
      "122\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "91\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "118\n",
      "107\n",
      "118\n",
      "118\n",
      "119\n",
      "35\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "67\n",
      "118\n",
      "118\n",
      "113\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "109\n",
      "118\n",
      "118\n",
      "53\n",
      "97\n",
      "118\n",
      "85\n",
      "119\n",
      "105\n",
      "118\n",
      "85\n",
      "118\n",
      "118\n",
      "71\n",
      "77\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "59\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "115\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "77\n",
      "62\n",
      "118\n",
      "37\n",
      "118\n",
      "77\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "70\n",
      "118\n",
      "118\n",
      "73\n",
      "118\n",
      "118\n",
      "42\n",
      "45\n",
      "118\n",
      "118\n",
      "121\n",
      "118\n",
      "118\n",
      "118\n",
      "54\n",
      "118\n",
      "118\n",
      "80\n",
      "119\n",
      "59\n",
      "112\n",
      "118\n",
      "118\n",
      "113\n",
      "102\n",
      "75\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "47\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "114\n",
      "117\n",
      "118\n",
      "118\n",
      "104\n",
      "102\n",
      "52\n",
      "102\n",
      "118\n",
      "96\n",
      "60\n",
      "118\n",
      "118\n",
      "118\n",
      "55\n",
      "118\n",
      "110\n",
      "132\n",
      "132\n",
      "118\n",
      "66\n",
      "102\n",
      "118\n",
      "111\n",
      "120\n",
      "118\n",
      "83\n",
      "87\n",
      "87\n",
      "118\n",
      "63\n",
      "118\n",
      "54\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "36\n",
      "142\n",
      "118\n",
      "108\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "73\n",
      "113\n",
      "118\n",
      "34\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "37\n",
      "34\n",
      "118\n",
      "108\n",
      "108\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "120\n",
      "104\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "105\n",
      "118\n",
      "118\n",
      "118\n",
      "113\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "79\n",
      "79\n",
      "118\n",
      "118\n",
      "118\n",
      "95\n",
      "119\n",
      "118\n",
      "73\n",
      "83\n",
      "32\n",
      "119\n",
      "74\n",
      "70\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "103\n",
      "118\n",
      "89\n",
      "71\n",
      "71\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "104\n",
      "118\n",
      "118\n",
      "53\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "71\n",
      "108\n",
      "118\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "68\n",
      "68\n",
      "118\n",
      "118\n",
      "115\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "108\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "107\n",
      "65\n",
      "119\n",
      "119\n",
      "37\n",
      "118\n",
      "39\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "99\n",
      "99\n",
      "118\n",
      "42\n",
      "118\n",
      "118\n",
      "107\n",
      "118\n",
      "118\n",
      "118\n",
      "63\n",
      "121\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "76\n",
      "118\n",
      "54\n",
      "96\n",
      "73\n",
      "118\n",
      "118\n",
      "99\n",
      "118\n",
      "119\n",
      "89\n",
      "87\n",
      "120\n",
      "103\n",
      "119\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "105\n",
      "118\n",
      "110\n",
      "118\n",
      "99\n",
      "104\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "107\n",
      "113\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "101\n",
      "118\n",
      "48\n",
      "96\n",
      "118\n",
      "89\n",
      "118\n",
      "118\n",
      "103\n",
      "118\n",
      "118\n",
      "118\n",
      "49\n",
      "118\n",
      "118\n",
      "118\n",
      "117\n",
      "118\n",
      "45\n",
      "118\n",
      "59\n",
      "103\n",
      "113\n",
      "118\n",
      "114\n",
      "118\n",
      "122\n",
      "118\n",
      "118\n",
      "114\n",
      "96\n",
      "118\n",
      "118\n",
      "118\n",
      "91\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "64\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "113\n",
      "118\n",
      "119\n",
      "118\n",
      "48\n",
      "118\n",
      "118\n",
      "92\n",
      "118\n",
      "118\n",
      "118\n",
      "82\n",
      "118\n",
      "118\n",
      "118\n",
      "75\n",
      "118\n",
      "118\n",
      "118\n",
      "114\n",
      "118\n",
      "69\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "89\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "69\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "63\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "60\n",
      "72\n",
      "72\n",
      "36\n",
      "36\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "46\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "35\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "40\n",
      "113\n",
      "38\n",
      "118\n",
      "118\n",
      "118\n",
      "92\n",
      "63\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "92\n",
      "74\n",
      "118\n",
      "118\n",
      "83\n",
      "118\n",
      "118\n",
      "118\n",
      "64\n",
      "118\n",
      "75\n",
      "75\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "118\n",
      "73\n",
      "118\n",
      "52\n",
      "118\n",
      "118\n",
      "35\n",
      "114\n",
      "118\n",
      "79\n",
      "119\n",
      "118\n",
      "32\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "62\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "84\n",
      "118\n",
      "118\n",
      "58\n",
      "105\n",
      "118\n",
      "90\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "80\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "111\n",
      "118\n",
      "118\n",
      "118\n",
      "74\n",
      "59\n",
      "118\n",
      "119\n",
      "118\n",
      "114\n",
      "93\n",
      "66\n",
      "107\n",
      "90\n",
      "118\n",
      "118\n",
      "118\n",
      "85\n",
      "120\n",
      "118\n",
      "48\n",
      "118\n",
      "99\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "92\n",
      "119\n",
      "53\n",
      "47\n",
      "46\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "51\n",
      "120\n",
      "118\n",
      "118\n",
      "120\n",
      "118\n",
      "118\n",
      "43\n",
      "68\n",
      "105\n",
      "118\n",
      "118\n",
      "118\n",
      "121\n",
      "118\n",
      "118\n",
      "111\n",
      "83\n",
      "118\n",
      "95\n",
      "106\n",
      "118\n",
      "118\n",
      "48\n",
      "107\n",
      "118\n",
      "119\n",
      "118\n",
      "112\n",
      "120\n",
      "118\n",
      "40\n",
      "42\n",
      "121\n",
      "99\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "104\n",
      "118\n",
      "40\n",
      "100\n",
      "100\n",
      "41\n",
      "41\n",
      "95\n",
      "40\n",
      "118\n",
      "54\n",
      "106\n",
      "118\n",
      "49\n",
      "33\n",
      "118\n",
      "118\n",
      "118\n",
      "33\n",
      "85\n",
      "118\n",
      "105\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "57\n",
      "118\n",
      "118\n",
      "47\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "83\n",
      "118\n",
      "119\n",
      "118\n",
      "103\n",
      "119\n",
      "118\n",
      "120\n",
      "118\n",
      "113\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "119\n",
      "89\n",
      "119\n",
      "118\n",
      "76\n",
      "97\n",
      "118\n",
      "118\n",
      "36\n",
      "118\n",
      "118\n",
      "73\n",
      "119\n",
      "119\n",
      "92\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "105\n",
      "103\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "76\n",
      "118\n",
      "118\n",
      "100\n",
      "118\n",
      "118\n",
      "115\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "103\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "40\n",
      "118\n",
      "118\n",
      "84\n",
      "31\n",
      "119\n",
      "119\n",
      "119\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "55\n",
      "118\n",
      "118\n",
      "72\n",
      "72\n",
      "72\n",
      "118\n",
      "67\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "115\n",
      "113\n",
      "99\n",
      "118\n",
      "95\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "68\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "97\n",
      "118\n",
      "118\n",
      "118\n",
      "101\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "40\n",
      "118\n",
      "106\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "57\n",
      "67\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "59\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "113\n",
      "118\n",
      "113\n",
      "54\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "74\n",
      "118\n",
      "58\n",
      "70\n",
      "42\n",
      "74\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "100\n",
      "118\n",
      "118\n",
      "101\n",
      "118\n",
      "118\n",
      "118\n",
      "84\n",
      "118\n",
      "118\n",
      "89\n",
      "107\n",
      "103\n",
      "118\n",
      "120\n",
      "119\n",
      "118\n",
      "118\n",
      "62\n",
      "35\n",
      "118\n",
      "44\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "87\n",
      "66\n",
      "109\n",
      "95\n",
      "95\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "41\n",
      "41\n",
      "41\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "115\n",
      "115\n",
      "118\n",
      "85\n",
      "97\n",
      "59\n",
      "81\n",
      "47\n",
      "47\n",
      "118\n",
      "118\n",
      "54\n",
      "118\n",
      "118\n",
      "53\n",
      "93\n",
      "83\n",
      "54\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "118\n",
      "77\n",
      "118\n",
      "118\n",
      "118\n",
      "39\n",
      "118\n",
      "91\n",
      "109\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "55\n",
      "118\n",
      "118\n",
      "118\n",
      "119\n",
      "119\n",
      "65\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "113\n",
      "52\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "104\n",
      "75\n",
      "118\n",
      "71\n",
      "118\n",
      "118\n",
      "118\n",
      "61\n",
      "118\n",
      "118\n",
      "68\n",
      "118\n",
      "118\n",
      "113\n",
      "79\n",
      "74\n",
      "95\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "114\n",
      "72\n",
      "76\n",
      "119\n",
      "64\n",
      "118\n",
      "118\n",
      "118\n",
      "97\n",
      "46\n",
      "118\n",
      "58\n",
      "109\n",
      "54\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "85\n",
      "118\n",
      "118\n",
      "91\n",
      "91\n",
      "118\n",
      "61\n",
      "98\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "120\n",
      "104\n",
      "118\n",
      "54\n",
      "118\n",
      "118\n",
      "77\n",
      "77\n",
      "47\n",
      "108\n",
      "118\n",
      "119\n",
      "118\n",
      "118\n",
      "118\n",
      "118\n",
      "65\n",
      "118\n",
      "118\n",
      "69\n",
      "106\n",
      "92\n",
      "118\n",
      "111\n",
      "118\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "#look at align lengths that are less than 50, all aligned lengths > 31\n",
    "\n",
    "from Menadione_tnseq.ipynb:pylance-notebook-cell:Y144sZmlsZQ== import read_samfile\n",
    "\n",
    "\n",
    "sf = read_samfile(\"sorted_reads/mapped_test_5000_R1.sam\")[1]\n",
    "for read in sf:\n",
    "    x = align_len(read)\n",
    "    print(x)\n",
    "  \n",
    "    # x_al = x[0]\n",
    "    # x_sc = x[1]\n",
    "    # sc_num = int(x_sc.split(\"S\")[0])\n",
    "    # if sc_num < 30:\n",
    "    # #if \"S\" not in x_sc:\n",
    "    #     print(x_al, x_sc)\n",
    "    #     print(read)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All aligned lengths are greater than 31. All the ones checked appear to have transposon tag in the soft-clipped region. Could it be more efficient to just check this region for transposon tag in order to filter? This is less reliable because instead of sc, sometimes get D or other tags. Need to check later on that these aren't causing problems with align-length\n",
    "\n",
    "A01968:63:H77VYDSX5:4:1101:3314:1626_1:N:0:AACGTGAT_BC:ATCGTGAT\t16\tNC_002945.3\t3002949\t60\t78M1D40M33S\t*\t0\t0\tGCGCAGGTGTTGAACACCACGACGTCGGCCTCGGAACCGTCGGTCGCCCTCCGGTAGCCGGCCGCTTCCAGCAGACCCCCAGCCGCTCGGAGTCGTGGACGTTCATCTGACAGCCG*TAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\t\n",
    "\n",
    "In this case, on reverse strand, we want the most 5' aligned region to get start, so this will be closest to the soft clip. So we need to indicate the len(cigar_list)-1 to be the align len, NOT the 1st field. Have tested, and it works for this case, but this is only case in test file that has the D\n",
    "\n",
    "Shorter sc seem to indicated truncated transposon without the -GTTA, but is this reliable for narrowing search region for tags?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_samfile(samfile):\n",
    "    \"\"\"\n",
    "    read sam_file and sort lines between header and reads\n",
    "    \"\"\"\n",
    "\n",
    "    header = []\n",
    "    reads  = []\n",
    "\n",
    "    with open(samfile, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line[0] == \"@\":\n",
    "                header.append(line)\n",
    "            else:\n",
    "                reads.append(line)\n",
    "    return header, reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = read_samfile(\"sorted_reads/mapped_test_5000_R1.sam\")[1]\n",
    "for read in heads:\n",
    "    sf = parse_samflag(read)\n",
    "    if sf not in [\"R\", \"F\"]:\n",
    "        print(sf)\n",
    "        print(read)\n",
    "        al = align_len(read)\n",
    "        print(al)\n",
    "# position = heads.split()[3]\n",
    "#bc_start = find_barcodes(heads)\n",
    "#print(bc_start)\n",
    "# rs = rev_start(position, al)\n",
    "# print(rs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re-do find_barcodes, find_tags and filter_mapped_reads to incorporate these functions and find correct start positions. Alignment length can be used to show if transposon tag in first part of read--aligned length needs to be >110 for it to be in first/last 40 bases regardless of strand (unlikely to be at end of aligned read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmfind1(G,n,H,m,max): # lengths; assume n>m\n",
    "  \"\"\"\n",
    "  A function to find matches of transposon sequence in read\n",
    "    Input               G                           sequence string\n",
    "                        n                           length of read sequence\n",
    "                        H                           pattern string\n",
    "                        m                           length of pattern\n",
    "                        max                         maximum mismatches\n",
    "    Output              i, -1                       start position of match in sequence, or -1 for no match\n",
    "  \"\"\"\n",
    "\n",
    "  a = G[:n].find(H[:m])\n",
    "  if a!=-1: return a # shortcut for perfect matches\n",
    "  for i in range(0,n-m):  # range of 0 to difference between seq length and len pattern\n",
    "    cnt = 0\n",
    "    for k in range(m):\n",
    "      if G[i+k]!=H[k]: cnt += 1\n",
    "      if cnt>max: break\n",
    "    if cnt<=max: return i\n",
    "  return -1\n",
    "\n",
    "def find_barcodes_starts(read):\n",
    "    \"\"\"\n",
    "    Find barcode and read alignment start position\n",
    "    Returns tuple of barcode and read start pos\n",
    "    \"\"\"\n",
    "    read_name  = read.split()[0]\n",
    "    barcode    = read_name.split(\"BC:\",1)[1]\n",
    "    rd_start = find_start(read)\n",
    "    bar_start = (barcode, rd_start) \n",
    "    return bar_start\n",
    "\n",
    "\n",
    "def find_start(read):\n",
    "    \"\"\"\n",
    "    Parse read for samflag (strand), start of read position and cigar string for alignment length\n",
    "    Returns aligned start position. Use this on filtered 'good reads' to identify start for quantification\n",
    "    \"\"\"\n",
    "    strand   = parse_samflag(read)\n",
    "    al_len   = align_len(read)\n",
    "    left_pos = read.split()[3]\n",
    "    if al_len > 0 and strand != \"*\":\n",
    "      if strand == \"F\":\n",
    "        read_start = int(left_pos)\n",
    "      else:\n",
    "        read_start = rev_start(left_pos, al_len)\n",
    "    else:\n",
    "        read_start = \"-\"\n",
    "    return read_start\n",
    "\n",
    "def check_seq(str):\n",
    "    \"\"\"\n",
    "    Is the target tag made of acceptable nucleotide characters\n",
    "    \"\"\"\n",
    "    import re\n",
    "    chars = set('ACTG')\n",
    "    seq = set(str.upper())\n",
    "    if seq.issubset(chars):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def find_tags2(read, target_tag, max):\n",
    "    \"\"\"\n",
    "    Find transposon tag in sequence of each read.\n",
    "\n",
    "      Input           read              mapped read\n",
    "                      target_tag        string that matches transposon sequence\n",
    "                      max               maximum number of mismatches allowed (default=2)\n",
    "      Output          match             match of start of tag from left-most start position of read or -1 if no match\n",
    "    \"\"\"\n",
    "    from Bio.Seq import Seq\n",
    "    \n",
    "    # find strand of read\n",
    "    strand = parse_samflag(read)\n",
    "    seq = read.split()[9]\n",
    "    if strand == \"F\":\n",
    "        #search string for transposon seq with max num mismatches\n",
    "        match = mmfind1(seq, len(seq), target_tag, len(target_tag), max)\n",
    "    elif strand == \"R\":\n",
    "        # make rc of seq and search for transposonl tag with max num mismatches\n",
    "        dna     = Seq(seq)\n",
    "        rc_seq  = str(dna.reverse_complement())\n",
    "        match = mmfind1(rc_seq, len(rc_seq), target_tag, len(target_tag), max)\n",
    "    else: \n",
    "        match = -1\n",
    "    \n",
    "    return match\n",
    "\n",
    "def filter_mapped_reads2(sam_file, tag=\"ACTTATCAGCCAACCTGTTA\", mismatch_max=2):\n",
    "  \"\"\" \n",
    "  Revised filtering function\n",
    "  \"\"\"\n",
    "  import sys\n",
    "  import os\n",
    "  import re\n",
    "  from operator import itemgetter\n",
    "\n",
    "  #check tag has valid nucleotides\n",
    "  if check_seq(tag):\n",
    "     pass\n",
    "  else:\n",
    "     sys.exit(\"Invalid sequence tag\")\n",
    "\n",
    "  #read sam_file and sort lines between header and reads\n",
    "  data    = read_samfile(sam_file)\n",
    "  header  = data[0]\n",
    "  reads   = data[1]\n",
    "\n",
    "  barcode_list  = []\n",
    "  good_reads    = []\n",
    "  notags_reads  = []\n",
    "  dup_reads     = []\n",
    "  \n",
    "  for read in reads:\n",
    "    ftag = find_tags2(read, tag, mismatch_max)\n",
    "    #align length (will parse read align len from header)\n",
    "    al  = align_len(read)\n",
    "\n",
    "    if ftag != -1 and al > 0:   #(transposon tag in first 40, but this will discard shorter reads that have tag and have mapped, could make 0)\n",
    "      #find barcode/start combo\n",
    "      bc_start = find_barcodes_starts(read)\n",
    "      barcode_list.append(bc_start) \n",
    "      barcode_list.sort(key=itemgetter(0))  #maybe this will speed up search by barcode?\n",
    "     \n",
    "      # if hasn't been added before, add read to good_reads\n",
    "      if barcode_list.count(bc_start) < 2:\n",
    "        good_reads.append(read)\n",
    "      else:\n",
    "        dup_reads.append(read)\n",
    "\n",
    "    else:\n",
    "      notags_reads.append(read)\n",
    "  #save barcode_list as pickle object to use in quantifying reads at ta sites? temporary until read processed?  \n",
    "  print(\"Total number of mapped reads: \", len(reads))\n",
    "  print(\"number of good reads (with tag): \", len(good_reads))\n",
    "  print(\"number of bad reads (with no tag): \", len(notags_reads))\n",
    "  print(\"Number of reads with duplicate barcode/starts: \", len(dup_reads))\n",
    "  bn = os.path.basename(sam_file)\n",
    "  outfile = \"tag_filtered_\" + bn\n",
    "  with open(outfile, 'w') as f:\n",
    "    for line in header:\n",
    "      f.write(f\"{line}\\n\")\n",
    "    for line in good_reads:\n",
    "      f.write(f\"{line}\\n\")\n",
    "  return notags_reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have filtered out unmapped reads from sorted.bam into mapped.sam. All reads should be assigned a strand in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def check_seq(str):\n",
    "    import re\n",
    "    chars = set('ACTG')\n",
    "    seq = set(str.upper())\n",
    "    if seq.issubset(chars):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "print(check_seq(\"ACTGGggG\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mapped reads:  1180\n",
      "number of good reads (with tag):  1105\n",
      "number of bad reads (with no tag):  7\n",
      "Number of reads with duplicate barcode/starts:  68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A01968:63:H77VYDSX5:4:1101:5267:1752_1:N:0:AACGTGAT_BC:GGGGGGGG\\t16\\tNC_002945.3\\t186581\\t60\\t119M32S\\t*\\t0\\t0\\tCGCTGTTCTACGCCGACGGCACCACTATGTTGTTCGGTGATGCGAAGAAATCGGTTACCGAAGTCTCCGAGGAACTCAAGGCGTTGTAGCGCGCGAGCGCTGGCTCAGACGGGCGGATAACAGGTTGGCTGATAGTCCTCGGTCTCTAGAC\\tF:FFF,FFFFFFFFFFFFFFF:FFF::FFFFFFFF:FFFFFF,FFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFF:FFFFF:FFFFF:FFFFFFFFFFF:FFFFFF:\\tNM:i:1\\tMD:Z:55G63\\tAS:i:114\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:21766:2300_1:N:0:AACGTGAT_BC:TATTTCAT\\t16\\tNC_002945.3\\t215396\\t60\\t119M32S\\t*\\t0\\t0\\tTGCAGCTCTTCGTGCTCATCGCCGCGGCCCACGACGTGCGCTGCGACGTGGCATCGAATTCGCCGTTCGTGTACGCCTACGGGTTCGCCGAGGACATCGACACCAGCCACGCCCTATACCCCGTGGGCTGATAAGTCCCCGGTCTCTAGAC\\tF::FF:F,F,:FFFF:,F:F,F,FFF,F,:F:FFF:,FFFF:,,FFFF,FFFFFFFFFFFF::FF,:F:FFF,FF:F::FFF::FFFFFF,FFF,FFFF,FFFF:,FF:FFFF:F:,F,F,,FF,,FFFF:FFFFFFFF:FFFFFFFFFFF\\tNM:i:3\\tMD:Z:22G6A32A56\\tAS:i:104\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:11559:2002_1:N:0:AACGTGAT_BC:ACACCACT\\t16\\tNC_002945.3\\t967861\\t60\\t132M19S\\t*\\t0\\t0\\tCACCGGGATCAGTTTCAAGTTGGCCAGTGCTAGCGGAATGCCGATGATCGTGACTGCCATTGCCGCGGCACTCACCAAATGCCCGAGGGCCAGCCAGATCCCGAACAGCAGCACCCAGATGACGTTGCTGATAAGTCCCCGGTCTCTAGAC\\t:FFFFFFF,FFFFFFFFFFFF:FF:F:FFF:FFFF,FFF:FFF,FFF:FFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFF:F:FFFFFFFFFFFFFFFFFFFFFF:FFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFF:F\\tNM:i:0\\tMD:Z:132\\tAS:i:132\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:15926:2018_1:N:0:AACGTGAT_BC:GCAACCGC\\t16\\tNC_002945.3\\t967861\\t60\\t132M19S\\t*\\t0\\t0\\tCACCGGGATCAGTTTCAAGTTGGCCAGTGCTAGCGGAATGCCGATGATCGTGACTGCCATTGCCGCGGCACTCACCAAATGCCCGAGGGCCAGCCAGATCCCGAACAGCAGCACCCAGATGACGTTGCTGATAAGTCCCCGGTCTCTAGAC\\tFFFFFFFFFFFF:FFF::FFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFF:FFFFFFFFF:F,FFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFF\\tNM:i:0\\tMD:Z:132\\tAS:i:132\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:24930:1611_1:N:0:AACGTGAT_BC:TAAGGCCT\\t16\\tNC_002945.3\\t1076613\\t60\\t142M9S\\t*\\t0\\t0\\tACCCGATCCTGCTGGCCCGGTGTCAGCGAATGCCAACACCGCTTGACCTCCTCAGGGTCGCTGTCCGGCGGCGGCATCTGCGGCATTGTGGGTGGCGCATGGCTGAGTTGGGCATGGACCTGCTCGCGTGATAAGTCCCCGGTCTCTAGAC\\tF:FFF:F:FFFFFFFFFFFF,:F,FF:F,F,FFF,,FFFFF,F,F,FFFFF:FFFFFFF:FFFFF:FFFFFFFFFFFFFF:FFFFFFFFFFFF:FF::FFFFFFFFFFF,FFFFF,FF:FFF:,F:FFFFFF:FFFF:FFFFFFFFFF:FF\\tNM:i:5\\tMD:Z:20G14C79T12C2C10\\tAS:i:117\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:12744:2957_1:N:0:AACGTGAT_BC:AAAACAAA\\t0\\tNC_002945.3\\t2547177\\t60\\t89S62M\\t*\\t0\\t0\\tCTCTAGACACCAGGCACATATAATCCAAACTGTTACTAGCTAGCACAAATCCGAAAAGCCCACGACTCACATCCACTCACAGACGCCACCAACCAGGTACATTTCTCGCTCGCACACCCTGATGCGCTCGAAGATCTGGTGCCGTTCGCCG\\tFFFF:FF,FF,,F,:,:,F,::,F:F,F,,F,F,F:::,F:,FF,F:,,,:FF,F,,FF,,:FF,F::,,,F::FFFF,F:F:::,:F,:,FFFF,:F:F,F:F,:FFFF,F:,FFF:FFFFFFFFF:FF::F:FFFFFFFFFFFFFFFFF\\tNM:i:1\\tMD:Z:24C37\\tAS:i:57\\tXS:i:0',\n",
       " 'A01968:63:H77VYDSX5:4:1101:11677:2456_1:N:0:AACGTGAT_BC:TGTATAGA\\t16\\tNC_002945.3\\t3829092\\t60\\t118M33S\\t*\\t0\\t0\\tCACACGGCACGGGCCCGTGAGCTGATTGGTCCGTCGGCGTTCCTGGCGCCCGAACACAAGGTGGTGCTGACCACCGACTCGGCAAGGGCCCGTACGGTGGGACGCCAGGCGCTCGATAACCTGTTGGCTTATAAGTCCCCGGTCTCTAGAC\\tF,FFF,F,,,FFFF:FFF,,,FF:FFFFFFF:,FFFF:::F,FFFFF,F:F:FFFF::F,FF:FF,FFFF,FFF,:F::,F,,FF:F,F:FFF,:FFFFF,FFFF,FFF,::F,F:F,,,,,FF:,FFF,:FFFFFFF,,FFF,F:FFFFF\\tNM:i:0\\tMD:Z:118\\tAS:i:118\\tXS:i:0']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_reads = read_samfile(\"sorted_reads/mapped_test_5000_R1.sam\")[1][0:10]\n",
    "#for read in my_reads:   \n",
    "#    print(read)\n",
    " #   fb = find_barcodes2(read)\n",
    " #   print(fb)\n",
    " #   tg = find_tags2(read, \"ACTTATCAGCCAACCTGTTA\", 2)\n",
    " #   print(tg)\n",
    "    \n",
    "filter_mapped_reads2(\"sorted_reads/mapped_test_5000_R1.sam\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 of these 7 no tag reads have different part of the transposon tag in them, but not the part that ends in GTTA\n",
    "\n",
    "+  ACTTATCAGCCAACCTGTTA-gDNA\n",
    "-  gDNA-TAACAGGTTGGCTGATAAGT\n",
    "            gDNA?-GCTGATAAGTCCCCGGTCTCTAGAC\n",
    " \n",
    "\n",
    "These have parts of transposon tag, but either >2 matches, or missing the terminal end of Himar1 transposon which includes the TA site.\n",
    "\n",
    "View these on artemis. Artemis won't view .sam, so have to change to .bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "conda activate dna\n",
    "samtools view -b -o tag_filtered_mapped_test_5000_R1.bam tag_filtered_mapped_test_5000_R1.sam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This samtools view won't work with the filtered because the @SQ lines are removed when filtered for mapped reads.\n",
    "\n",
    "Can use -h to reset output with header--edited snakemake file for filtering by mapped reads.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Quantify reads at each TA site (need list of coordinates of TA sites for bovis to make wig files). Need to make sure start position parsed from the cigar string is accurate\n",
    "\n",
    "How do left-most positions from reads align with TA sites?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View on artemis\n",
    "\n",
    "Reverse strand mapped read with start position calculated as 39,859 (left-most position + aligned len from cigar string). This is actually one off from real start position which is one too far right (5' for reverse). REad actually begins at 39858. TA site on - strand starts at 39858. Positive read starts at 39857 (correct left-most position from cigar) which coincides with TA starting at 39857. These are same TA site and should be counted as 2 insertions for one TA site. The site is referenced by + strand T of the TA in the file of ta sites (bovis_TA_sites.txt) found with insertion_site_finder function. Therefore, reverse strand reads need to subtract one from + strand coordinate of start to line up with the correct TA site (which is really the 'A' of the TA site on the reverse compl strand)\n",
    "\n",
    "![Forward and reverse mapped to same TA site](images/F_R_sameTAsite.png)\n",
    "\n",
    "\n",
    "For forward strand:  start == TA site position (+ strand coordinate)\n",
    "for reverse strand:  start - 1 == TA site position (based on + strand coordinate) (this is in addition to -1 to calculate the start position from align len, subtract 2 from start in find_start/rev_start functions)\n",
    "\n",
    "Instead of changing start position for reverse reads, will quantify strands separately, with TA sites adjusted for reverse orientation? This causes problems when comparing lists--if starts are integers and ta_sites are strings. do all adjustments at start assignment BEFORE quantification\n",
    "\n",
    "Also, when TA site inserted before T in genomic, alignment len can be off by one (should have soft-clipped one more base as genomic DNA seq has TTA which matches transposon tag end -GTTA), should there be some leeway if start is within 1 base of TA site position?\n",
    "\n",
    "![soft-clip one early](images/softclip_error.png)\n",
    "\n",
    "Maybe can fix this when finding transposon tag--if there is a match, change start to end position of match? (returned value + transposon tag length). may be more accurate than start left most start of alignment? can still add alignment length to this for reverse reads. Essentially, \"virtually\" clip transposon when determining start site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def motif_finder(seq, motif):\n",
    "    \"\"\"Finds location of motif (substring) in sequence.\n",
    "    Input       seq         sequence\n",
    "                motif       desired subsequence\n",
    "    Output      locations   list of sequence locations of substrings (one-indexed)\n",
    "    \"\"\"\n",
    "    import re\n",
    "    locations = []\n",
    "    p = re.compile('(?='+motif+')')       # use lookahead to find all overlapping matches\n",
    "    matches = p.finditer(seq)\n",
    "    for match in matches:\n",
    "        start = match.span()\n",
    "        locations.append(start[0] + 1)   # add one to change from zero-indexing to sequence position   \n",
    "\n",
    "\n",
    "    return locations\n",
    "\n",
    "#**********************************************************************************\n",
    "\n",
    "def find_insertion_sites(seq):\n",
    "    \"\"\"Finds all the locations of TA insertion sites in sequence\n",
    "\n",
    "    Seems you only want TA positions from one strand for counting number of sites,\n",
    "    as they are same site on either strand. When quantifying, need to get reads that start \n",
    "    \"\"\"\n",
    "    from Bio.Seq import Seq\n",
    "    \n",
    "    seq_obj         = Seq(seq)\n",
    "    rec_seq         = str(Seq.reverse_complement(seq_obj))\n",
    "    fwd_positions = motif_finder(seq, \"TA\")\n",
    "    rev_positions = motif_finder(rec_seq, \"TA\")\n",
    "    \n",
    "    return fwd_positions\n",
    "\n",
    "#**********************************************************************************\n",
    "\n",
    "\n",
    "def open_fasta(refseq):\n",
    "    with open(refseq, 'r') as file:\n",
    "        seq = ''\n",
    "        for line in file:\n",
    "            if line[0] != '>':\n",
    "                seq += line.strip()\n",
    "    return seq\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make list of TA sites in reference genome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73465\n",
      "[60, 72, 102, 188, 246, 333, 360, 426, 448, 471]\n"
     ]
    }
   ],
   "source": [
    "bovis_fasta = \"ref_seqs/Mbovis_AF2122-97.fasta\"\n",
    "bovis_seq   = open_fasta(bovis_fasta)\n",
    "# to look at only first 30,000 nts of genome\n",
    "bovis_30 = bovis_seq[0:30001]\n",
    "\n",
    "res = motif_finder(bovis_seq, \"TA\")\n",
    "print(len(res))\n",
    "print(res[0:10])\n",
    "\n",
    "with open('bovis_TA_sites.txt', 'w') as outfile:\n",
    "    results = find_insertion_sites(bovis_seq)\n",
    "    #print(len(results))\n",
    "    #73465\n",
    "    #print(results[0:10])\n",
    "    #print(len(results[1]))\n",
    "    #73465\n",
    "    #these will be from 5' end of rc strand (-), wont use these\n",
    "    #print(results[1][0:10])\n",
    "    #these should coincide with 5' end of + strand\n",
    "    #print(results[1][-10:])\n",
    "    #for site in results:\n",
    "    print(results, file=outfile)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_set = [1,2,3,4,5,5,5,5,6,7,8,9,10]\n",
    "print(len(whole_set))\n",
    "unique_set = set(whole_set)\n",
    "print(unique_set)\n",
    "len(unique_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use similar strategy to tpp_tools.py: read_counts(ref,sam,vars) template_counts(ref,sam,bcfile,vars) and increase_counts(pos,sites, strand)\n",
    "\n",
    "1. use find_barcode_starts to get list of barcode/starts for each read. should be same length as number of reads\n",
    "    - change filter function to only look for tranposon tags\n",
    "    - new function to identify starts/strand/barcode for each read and make into list(which will be iterated through)\n",
    "2. make dicts of ta-sites and read counts: ordered dictionary (default is ordered), new values will overwrite old values\n",
    "    - analyse each strand separately\n",
    "    - make TA:read dict: \n",
    "        - for each site key, append list barcodes that have starts within 1 nt of TA site\n",
    "        - add up total reads for each site (F and R)\n",
    "    - make TA:template dict:\n",
    "        - for each site key, reduce barcode list to unique sites\n",
    "        - add up total reads for each site (F and R)\n",
    "3. Write wig files for TA sites using TA:template dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make new dictionary with start positions and strand? or re-use barcode/start--add strand info?\n",
    "\n",
    "\n",
    "\n",
    "def find_barcodes_starts_strand(reads):\n",
    "    \"\"\"\n",
    "    Find barcode and read alignment start position\n",
    "    Returns list of starts, strand, barcode\n",
    "    Input           read\n",
    "    Output          starts_list\n",
    "    \"\"\"\n",
    "\n",
    "    starts_list = []\n",
    "    for read in reads:\n",
    "        read_name  = read.split()[0]\n",
    "        barcode    = read_name.split(\"BC:\",1)[1]\n",
    "        strand     = parse_samflag(read) \n",
    "        rd_start   = find_start(read)\n",
    "        line = [rd_start, strand, barcode]\n",
    "        starts_list.append(line)\n",
    "    return starts_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180\n",
      "[20350, 'R', 'GAACAACT']\n",
      "[26121, 'F', 'CGCCACCT']\n",
      "[31581, 'F', 'CCGACTGA']\n",
      "[35236, 'R', 'TATTCTAA']\n",
      "[37337, 'F', 'AATCACTC']\n",
      "[39859, 'R', 'TGAGGCGA']\n",
      "[39857, 'F', 'CGAGACAG']\n",
      "[47375, 'F', 'ACAGGATC']\n",
      "[48094, 'R', 'CTGCTCCA']\n",
      "[51951, 'F', 'GACCAAGC']\n",
      "[52823, 'R', 'CATAGAAT']\n",
      "[57868, 'F', 'CAGTGTAC']\n",
      "[63369, 'R', 'TTACTATT']\n",
      "[69581, 'R', 'TTTAGTCA']\n",
      "[72388, 'R', 'TTACCATA']\n",
      "[72388, 'R', 'TTACCATA']\n",
      "[81203, 'R', 'ATATTCCT']\n",
      "[81968, 'R', 'AAGGTTAA']\n",
      "[81968, 'R', 'AAGGTTAA']\n",
      "[87311, 'F', 'TGTTCCAG']\n",
      "R\n"
     ]
    }
   ],
   "source": [
    "my_reads = read_samfile(\"sorted_reads/mapped_test_5000_R1.sam\")[1] #reads are second in return\n",
    "\n",
    "mlist = find_barcodes_starts_strand(my_reads)\n",
    "print(len(mlist))\n",
    "for x in mlist[0:20]:\n",
    "    print(x)\n",
    "\n",
    "print(mlist[0][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate through list and add to both dictionaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantify_sites(ta_sites, start_sites_list):\n",
    "    \"\"\"\n",
    "    Function to quantify reads at each ta site\n",
    "    \"\"\"\n",
    "\n",
    "    template_dict   = {}\n",
    "    #rev_template_dict   = {}\n",
    "    no_match        = []\n",
    "    read_count_dict     = {}\n",
    "\n",
    "    for site in start_sites_list:\n",
    "        #print(site)\n",
    "        start_site  = site[0]\n",
    "        #print(start_site)\n",
    "        strand      = site[1]\n",
    "        #print(strand)\n",
    "        barcode     = site[2]\n",
    "        # add to read dict\n",
    "        \n",
    "        # do separately for strands because easier to demultiplex (different reads if on different strands by def)\n",
    "        #if strand == 'F':\n",
    "        if str(start_site) in ta_sites:\n",
    "            site_list = template_dict.get(start_site, None)\n",
    "            print(site_list)\n",
    "            if site_list is not None:  #there are already entries at this site\n",
    "                read_count_dict[start_site] = read_count_dict[start_site] + 1\n",
    "                    #if barcode != site_list: #ie barcode is different than those in list \n",
    "                site_list.append(site)\n",
    "                template_dict[start_site] = site_list\n",
    "                    #else: \n",
    "                    #    pass\n",
    "            else: #no entries with this site key, create new one\n",
    "                read_count_dict[start_site] = 1\n",
    "                site_list = site\n",
    "                template_dict[start_site] = site_list\n",
    "        else:\n",
    "            no_match.append(site)\n",
    "\n",
    "        # if strand == 'R':\n",
    "        #     if str(start_site) in ta_sites:\n",
    "        #         site_list = rev_template_dict.get(start_site, None)\n",
    "        #         if site_list is not None: \n",
    "        #             read_count_dict[start_site] = read_count_dict[start_site] + 1\n",
    "        #             #if site not in site_list:\n",
    "        #             site_list.append(site)\n",
    "        #             rev_template_dict[start_site] = site_list\n",
    "        #             #else:\n",
    "        #             #    pass\n",
    "        #         else:\n",
    "        #                 read_count_dict[start_site] = 1\n",
    "        #                 site_list = site\n",
    "        #                 rev_template_dict[start_site] = site_list\n",
    "        #     else:\n",
    "        #         no_match.append(site)\n",
    "    \n",
    "    \n",
    "\n",
    "    return template_dict, read_count_dict, no_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "[39857, 'R', 'TGAGGCGA']\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "[72386, 'R', 'TTACCATA']\n",
      "None\n",
      "None\n",
      "[81966, 'R', 'AAGGTTAA']\n",
      "None\n",
      "templates {20348: [20348, 'R', 'GAACAACT'], 26121: [26121, 'F', 'CGCCACCT'], 31581: [31581, 'F', 'CCGACTGA'], 35234: [35234, 'R', 'TATTCTAA'], 37337: [37337, 'F', 'AATCACTC'], 39857: [39857, 'R', 'TGAGGCGA', [39857, 'F', 'CGAGACAG']], 47375: [47375, 'F', 'ACAGGATC'], 48092: [48092, 'R', 'CTGCTCCA'], 51951: [51951, 'F', 'GACCAAGC'], 52821: [52821, 'R', 'CATAGAAT'], 57868: [57868, 'F', 'CAGTGTAC'], 63367: [63367, 'R', 'TTACTATT'], 69579: [69579, 'R', 'TTTAGTCA'], 72386: [72386, 'R', 'TTACCATA', [72386, 'R', 'TTACCATA']], 81201: [81201, 'R', 'ATATTCCT'], 81966: [81966, 'R', 'AAGGTTAA', [81966, 'R', 'AAGGTTAA']], 87311: [87311, 'F', 'TGTTCCAG']}\n",
      "read counts for all sites: {20348: 1, 26121: 1, 31581: 1, 35234: 1, 37337: 1, 39857: 2, 47375: 1, 48092: 1, 51951: 1, 52821: 1, 57868: 1, 63367: 1, 69579: 1, 72386: 2, 81201: 1, 81966: 2, 87311: 1}\n",
      "no_match []\n"
     ]
    }
   ],
   "source": [
    "my_reads = read_samfile(\"sorted_reads/mapped_test_5000_R1.sam\")[1][0:20] #reads are second in return\n",
    "print(len(my_reads))\n",
    "mlist = find_barcodes_starts_strand(my_reads)\n",
    "with open('bovis_TA_sites.txt', 'r') as f:\n",
    "    ta_list = f.read()\n",
    "    \n",
    "test_dict = quantify_sites(ta_list, mlist)\n",
    "\n",
    "print(\"templates\", test_dict[0])\n",
    "#print(\"reverse_templates\", test_dict[1])\n",
    "print(\"read counts for all sites:\" , test_dict[1])\n",
    "print(\"no_match\", test_dict[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sites close to these:  134438 (F -1), 154950 (F -1), 163066 (R +4), 215512 (510) (R -1), 231802 (806,808) (F +1)\n",
    "248316 (R -1), 253846 (F +1)\n",
    "\n",
    "Barcode/template dedup not working here--too complicated. \n",
    "1. find more accurate start position using position of transposon tag (will eliminate some of the nomatches to ta sites that are off by a base or two)\n",
    "2. deduplicate barcode/start as before so quantification step using already deduped reads. won't have total read count, but will have count of deduped reads for each barcode/start?\n",
    "    - use find_barcodes_starts\n",
    "\n",
    "Alternatively, use genomic ranges in R to detect overlap between start of read and TA site with some flexibility in the read start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New filtering by transposon tag that uses location of tag to define start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_ta_position(read, tag_pos):\n",
    "    \"\"\"\n",
    "    Find the TA position relative to + strand\n",
    "    \"\"\"\n",
    "    strand = parse_samflag(read)\n",
    "    left_pos = read.split()[3]\n",
    "    if tag_pos != -1:\n",
    "        if strand == 'F':\n",
    "            #TA should be the left most position == soft-clipped bases + tag_pos\n",
    "            ta_position = int(left_pos)\n",
    "        else:\n",
    "            ta_position = int(left_pos) + tag_pos\n",
    "    else:\n",
    "        ta_position = \"-\"\n",
    "    return ta_position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tags2(read, target_tag, max):\n",
    "    \"\"\"\n",
    "    Find transposon tag in sequence of each read.\n",
    "\n",
    "      Input           read              mapped read\n",
    "                      target_tag        string that matches transposon sequence\n",
    "                      max               maximum number of mismatches allowed (default=2)\n",
    "      Output          start             calculation of start of read/insertion point \n",
    "                                        from left-most start position of tag or -1 if no match\n",
    "    \"\"\"\n",
    "    from Bio.Seq import Seq\n",
    "    #find rec of tag\n",
    "    tag_seq = Seq(target_tag)\n",
    "    rc_tag  = str(tag_seq.reverse_complement())\n",
    "    # find strand of read\n",
    "    strand = parse_samflag(read)\n",
    "    seq = read.split()[9]\n",
    "    if strand == \"F\":\n",
    "        #search string for transposon seq with max num mismatches\n",
    "        match = mmfind1(seq, len(seq), target_tag, len(target_tag), max)\n",
    "        if match != -1:\n",
    "            start = match + len(target_tag) #this gives start position of read (start of ta site)\n",
    "        else:\n",
    "            start = match\n",
    "    else: #strand is reverse\n",
    "        #this is too time consuming to find rc for each read\n",
    "        # make rc of seq and search for transposon tag with max num mismatches ()\n",
    "        #dna     = Seq(seq)\n",
    "        ##rc_seq  = str(dna.reverse_complement())\n",
    "        #rev_match   = mmfind1(rc_seq, len(rc_seq), target_tag, len(target_tag), max)\n",
    "        #match   = len(seq) - rev_match #gives match coordinate (start of tag)\n",
    "        match = mmfind1(seq, len(seq), rc_tag, len(rc_tag), max)\n",
    "        if match != -1:\n",
    "            start = match  #to get to start of 'TA' in reverse read\n",
    "        else: \n",
    "            start = match\n",
    "    \n",
    "    return start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n",
      "20348\n",
      "A01968:63:H77VYDSX5:4:1101:8757:2503_1:N:0:AACGTGAT_BC:GAACAACT\t16\tNC_002945.3\t20232\t60\t118M33S\t*\t0\t0\tATACGCGTTCGATGACCTCGGTGCCGGCCGCCGTAATCGGCGACTTATTTCGTGGGCGGGTGCGCAGTGGGCGGCGGGCTCCGTGCGAGATGCGTGCCAGGATGGCCAGCAATATGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
      "35\n",
      "26121\n",
      "A01968:63:H77VYDSX5:4:1101:15022:1611_1:N:0:AACGTGAT_BC:CGCCACCT\t0\tNC_002945.3\t26121\t60\t32S119M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGCACGCTCATCGTGTGTCCTTGCGGCCAGGGATAGCGCCGTAGCTGATCGTAGATAGTGGTGCGGCACATGTCGTCGCCAGTGGCCGCCAGCAGCGGGTCCCGCGCCTGCGGTGTG\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:119\tAS:i:119\tXS:i:0\n",
      "35\n",
      "31581\n",
      "A01968:63:H77VYDSX5:4:1101:20184:2942_1:N:0:AACGTGAT_BC:CCGACTGA\t0\tNC_002945.3\t31581\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGCGTCTCGCTGAGCGAGGCGGCGATGGAGACCGACGCAGAAACCCTGGCGGAAGCCATCCTGCTCACCGCCGACGTGTCCTGCCTTAAAGCGTTGCTGGAAGTACGCAACGAGATC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
      "116\n",
      "35234\n",
      "A01968:63:H77VYDSX5:4:1101:24089:1595_1:N:0:AACGTGAT_BC:TATTCTAA\t16\tNC_002945.3\t35118\t60\t118M33S\t*\t0\t0\tGTACCGGGCGGCGCTGATGCTCAGCCTAAAAGATGCGATCAGCCGAGATAAACGGCGAATGGAAATGGGTATTACGAACTATTTCACAAAACTTCGCATTCCGGGTGCCCGAGTCATAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
      "35\n",
      "37337\n",
      "A01968:63:H77VYDSX5:4:1101:15311:2644_1:N:0:AACGTGAT_BC:AATCACTC\t0\tNC_002945.3\t37337\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAATCAGCTATCAGGACCTCATCGCGCGCGCGGCGGCATGCATCCCCCCGCTACGGCGTCTTGACATCAAACGCGGTGAACCCGTGCTGATCACCGCCCCCACCAACCTGGAATTCCT\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFFF,F:FFFFFFFFFF,FFFFF\tNM:i:2\tMD:Z:65C33A18\tAS:i:108\tXS:i:0\n",
      "107\n",
      "39857\n",
      "A01968:63:H77VYDSX5:4:1101:28682:1689_1:N:0:AACGTGAT_BC:TGAGGCGA\t16\tNC_002945.3\t39750\t60\t109M33S\t*\t0\t0\tTCAGCGCGTCGAGGTCGTCGCTTTCGGCACGCAGGTCTGCCACGAACGGCCCAGGATCCGCCATCACCACCTCCTGAGGTAACAGTTCGTCGGGAAAGGCATGTTTGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\t,F::FF::F:F,FFFFFF:FF::FFFFF,FFFFFFF,,,FFFF:F,FFF,FFFFFF,FFF:,FFFFFF::F:F,FFFFFFFF,:FFF::FFF,FFFFFF:FFFFFFFF,F:F:FFFF:FFFFFF:F:FF::,F:,F,FFFFF\tNM:i:1\tMD:Z:0C108\tAS:i:108\tXS:i:0\n",
      "35\n",
      "39857\n",
      "A01968:63:H77VYDSX5:4:1101:8802:1204_1:N:0:AACGTGAT_BC:CGAGACAG\t0\tNC_002945.3\t39857\t60\t33S118M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACCCTAGCGACCGATCACAGGCTGGCCGCGGCGCCCGACGATGGTGTGCACCACCAGCCCGGCTAGGTAGATCGCCGACCCGAACAGCACAAATACGGGCGCGTGCCCGTGCTCGGG\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFF:FFFFFFFFFFFFFFFFFFFFFF,F:FFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
      "35\n",
      "47375\n",
      "A01968:63:H77VYDSX5:4:1101:23231:1172_1:N:0:AACGTGAT_BC:ACAGGATC\t0\tNC_002945.3\t47375\t60\t33S95M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGCTTCTGGCGAAGCCAGGGATCGGCGCCCCAAACGGGCCGGGACAAGCGCCCTCGGGCGGGACCAATACTCGGCGGCGGAACAGTTCGGCCAA\tFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFF::FFFFFFFFF,FFF::FFFFFFFFFFFFFFFFF:FF:FFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFF,F\tNM:i:1\tMD:Z:94G0\tAS:i:94\tXS:i:0\n",
      "116\n",
      "48092\n",
      "A01968:63:H77VYDSX5:4:1101:21875:2957_1:N:0:AACGTGAT_BC:CTGCTCCA\t16\tNC_002945.3\t47976\t60\t118M33S\t*\t0\t0\tTTTCATTGCGGTCGACGCGGTCCCCGCTGCGCAGTTTGCCGGTCAATAGCAGGTTGAGGATGTGGGCGACAACCTGGTCCTTTTCCTTAACCCCGTACTTTTTTGGCATCGGTATCTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tF:F,F,F,F,FFFFF,F:FFFFF,:F,FFF::FFF:FFFFF:FFFFF,FF:FFFF:FFF:FFFFFFF,FF,F::FFFF,FFFFF,FFF:::::,FF:FFFFFFFFF,:FFFFFFFFF:F:FFFFFFF:F,FF:FFF:F:F:FF,F:FF,:F\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
      "35\n",
      "51951\n",
      "A01968:63:H77VYDSX5:4:1101:11595:2409_1:N:0:AACGTGAT_BC:GACCAAGC\t0\tNC_002945.3\t51951\t60\t33S69M\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACATGCTTTTGACGTCGGATTGTTTGAGGCCGAGGGTTTCCGGGGCGCCGCGCATGATGCTCACAGCA\tFFF:FFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFF::FFFFFFFFFF,FFFF::FFFF::FFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:1\tMD:Z:68G0\tAS:i:68\tXS:i:0\n",
      "116\n",
      "52821\n",
      "A01968:63:H77VYDSX5:4:1101:13738:1078_1:N:0:AACGTGAT_BC:CATAGAAT\t16\tNC_002945.3\t52705\t60\t118M33S\t*\t0\t0\tTGGGAATCTGAACTCGATCCAGCCGTACCCGCGCAACAACGGCGCCGGTTGCGTATCGGTGGTGTGGATGGCGTCGTACTCTGGTCCGCGTGCGACTGCAGCGACAGGTAGTGGACTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFFFFF:FFF:FFFFFFFFF,FFFFF:FFFFFF,F,F,FFFF:FFFFFF,F::FFFFFF:F::F:FFF:FF:F:FFFFFFF:FFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFF:FFF\tNM:i:2\tMD:Z:20C65A31\tAS:i:108\tXS:i:0\n",
      "34\n",
      "57868\n",
      "A01968:63:H77VYDSX5:4:1101:15347:2644_1:N:0:AACGTGAT_BC:CAGTGTAC\t0\tNC_002945.3\t57868\t60\t32S79M3S\t*\t0\t0\tGTCTAAGACCGGGGACTTATCAGCCAACCTGTTATGACCCGCAGCCACCGGTGGATGCCGGGTCCCTGGCCAAGGCCTCGCCGGCTACCCATCGGCGCGCGTTTGAGTTCTGTA\tFF,FFF:FFFFF:FFFFFFFFF,FFFFFFFFFFFFFFF:FFFFFFF:FFFFFFF:F,,FFF,::F,,,FFFF:FFFF,FFFF:F,:FFFF:,FF:F,::F,FF,F,,FF::FFF\tNM:i:3\tMD:Z:29C34T6G7\tAS:i:64\tXS:i:0\n",
      "116\n",
      "63367\n",
      "A01968:63:H77VYDSX5:4:1101:31611:2722_1:N:0:AACGTGAT_BC:TTACTATT\t16\tNC_002945.3\t63251\t60\t118M33S\t*\t0\t0\tAGCGTCCCCATGACTGGATCGTTTGGCACTTCACCCATGCCGACAATCTCCCTGGGATCATCACCGCTGGCCGTCTGCTGGCCGATTCAGCAGTCACCCCGACGACCGAGGTTGCATAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFF,FFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:1\tMD:Z:112G5\tAS:i:113\tXS:i:0\n",
      "116\n",
      "69579\n",
      "A01968:63:H77VYDSX5:4:1101:9046:1188_1:N:0:AACGTGAT_BC:TTTAGTCA\t16\tNC_002945.3\t69463\t60\t118M33S\t*\t0\t0\tTTCCGGCGATGGCCGCCGCACTGCTGGTGCTGTCGGCGATCCTGGTCGGTGGACTGTGGCCGCTGCTGATGGAGCAGTTCTCGGTGCGTCCCAACGCCGCCGATGTCGAACGCCCATAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
      "116\n",
      "72386\n",
      "A01968:63:H77VYDSX5:4:1101:26955:2832_1:N:0:AACGTGAT_BC:TTACCATA\t16\tNC_002945.3\t72270\t60\t118M33S\t*\t0\t0\tATGGCGCCGTCGTAGCCGGGCCAGCCCGTCGTCAGCCTTGGACAGCCTCCAGCGCTGCATTGAACGTCTTGCTGGGCCGCATCACCGCCGTAGTCATGTCGCTGTCCGGCGCGTAGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFFFFFFFFFFF:FFFFFFFFFFFFFFFF,FFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFF,FFFFFFFFFFFFFF:FFFF:FFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFF::FF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
      "116\n",
      "72386\n",
      "A01968:63:H77VYDSX5:4:1101:26684:3082_1:N:0:AACGTGAT_BC:TTACCATA\t16\tNC_002945.3\t72270\t60\t118M33S\t*\t0\t0\tATGGCGCCGTCGTAGCCGGGCCAGCCCGTCGTCAGCCTTGGACAGCCTCCAGCGCTGCATTGAACGTCTTGCTGGGCCGCATCACCGCCGTAGTCATGTCGCTGTCCGGCGCGTAGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tF:FFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFF,FFFFFFFFFF:FFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
      "116\n",
      "81201\n",
      "A01968:63:H77VYDSX5:4:1101:24361:3098_1:N:0:AACGTGAT_BC:ATATTCCT\t16\tNC_002945.3\t81085\t60\t118M33S\t*\t0\t0\tCCGACACCTCGGTGACACTGTCGAGGTCGGCGCGCGCAGATTGCGGGTCGTTGGCATTGTGCCGAATTCCACCGCGCTGGCCAAGATCCCCAATGTCTTCCTCACGACCGAGGGCTTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:118\tAS:i:118\tXS:i:0\n",
      "116\n",
      "81963\n",
      "A01968:63:H77VYDSX5:4:1101:7798:1282_1:N:0:AACGTGAT_BC:AAGGTTAA\t16\tNC_002945.3\t81847\t60\t121M30S\t*\t0\t0\tAAGACGACACTGCTTTCCTGTCTGGGCGGCATTCTGCGCCCGAAGTCTGGGGCGATCAAGTTCGACGAAGTCGACATCACGACGCTACAAGGCGCCGAGCTGGCGAACTACCGGCGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFFF:FFFFFFFFFFFFFFFFF,:FFFFFFFFFFFFF::FFFFFFFFFFFFFFF,F,,FFFFFFFF:FFF:FFFFFF:FF,FFF:FFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFF:F\tNM:i:0\tMD:Z:121\tAS:i:121\tXS:i:0\n",
      "116\n",
      "81963\n",
      "A01968:63:H77VYDSX5:4:1101:7762:1313_1:N:0:AACGTGAT_BC:AAGGTTAA\t16\tNC_002945.3\t81847\t60\t121M30S\t*\t0\t0\tAAGACGACACTGCTTTCCTGTCTGGGCGGCATTCTGCGCCCGAAGTCTGGGGCGATCAAGTTCGACGAAGTCGACATCACGACGCTACAAGGCGCCGAGCTGGCGAACTACCGGCGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\t:FFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFF,FFFFFFFFFFFFFFFFF:FF,FF:FFFFFFFFFFFF:,F:FFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF::FFFFFFF\tNM:i:0\tMD:Z:121\tAS:i:121\tXS:i:0\n",
      "35\n",
      "87311\n",
      "A01968:63:H77VYDSX5:4:1101:20085:1924_1:N:0:AACGTGAT_BC:TGTTCCAG\t0\tNC_002945.3\t87311\t60\t33S116M2S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAATCAGCTCGATAGCTTTGCGCGCATCTTGGATATCTTGAGGCGATGCGGCGTCCACGAGCGCACGTAGATCACTGCGATCCTGGGGTCGCCGATCATCATCTCTCGCAAGAAGTAG\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFF\tNM:i:0\tMD:Z:116\tAS:i:116\tXS:i:0\n"
     ]
    }
   ],
   "source": [
    "my_reads = read_samfile(\"sorted_reads/mapped_test_5000_R1.sam\")[1][0:20] #reads are second in return\n",
    "for read in my_reads:\n",
    "    x = find_tags2(read, \"ACTTATCAGCCAACCTGTTA\", 2)\n",
    "    print(x)\n",
    "    print(find_ta_position(read, x))\n",
    "    print(read)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_mapped_reads3(sam_file, tag=\"ACTTATCAGCCAACCTGTTA\", mismatch_max=2):\n",
    "  \"\"\" \n",
    "  Revised filtering function\n",
    "  \"\"\"\n",
    "  import sys\n",
    "  import os\n",
    "  from operator import itemgetter\n",
    "\n",
    "  #check tag has valid nucleotides\n",
    "  if check_seq(tag):\n",
    "     pass\n",
    "  else:\n",
    "     sys.exit(\"Invalid sequence tag\")\n",
    "\n",
    "  #read sam_file and sort lines between header and reads\n",
    "  data    = read_samfile(sam_file)\n",
    "  header  = data[0]\n",
    "  reads   = data[1]\n",
    "\n",
    "\n",
    "  barcode_list    = []\n",
    "  template_reads  = []\n",
    "  template_barcodes = []\n",
    "  notags_reads    = []\n",
    "  dup_reads       = []\n",
    "  \n",
    "  for read in reads:\n",
    "    tag_pos   = find_tags2(read, tag, mismatch_max)\n",
    "    strand    = parse_samflag(read)\n",
    "    \n",
    "    #align length (will parse read align len from header)\n",
    "    #al      = align_len(read)\n",
    "    #if strand == \"F\":\n",
    "\n",
    "    if tag_pos != -1:  #there is match for tranposon tag\n",
    "        insertion_coord = find_ta_position(read, tag_pos)\n",
    "        #find barcode\n",
    "        read_name  = read.split()[0]\n",
    "        barcode    = read_name.split(\"BC:\",1)[1]\n",
    "        pos_barcode = [insertion_coord, strand, barcode]\n",
    "        #add to list of all reads with insertions\n",
    "        barcode_list.append(pos_barcode) \n",
    "        barcode_list.sort(key=itemgetter(0))  #maybe this will speed up search ordering by position--but sorting will add time\n",
    "        # if hasn't been added before, add read to unique template reads\n",
    "        if barcode_list.count(pos_barcode) < 2:\n",
    "          template_reads.append(read)\n",
    "          template_barcodes.append(pos_barcode)\n",
    "        else:\n",
    "          dup_reads.append(read)\n",
    "    else:\n",
    "      notags_reads.append(read)\n",
    "  \n",
    "  print(\"Total number of mapped reads: \", len(reads))\n",
    "  print(\"number of unique templates (with tag): \", len(template_reads))\n",
    "  print(\"number of bad reads (with no tag): \", len(notags_reads))\n",
    "  print(\"Number of reads with duplicate barcode/starts: \", len(dup_reads))\n",
    "  bn = os.path.basename(sam_file)\n",
    "  #write template reads\n",
    "  outfile = \"tag_filtered_templates_\" + bn\n",
    "  with open(outfile, 'w') as f:\n",
    "    for line in header:\n",
    "      f.write(f\"{line}\\n\")\n",
    "    for line in template_reads:\n",
    "      f.write(f\"{line}\\n\")\n",
    "  outfile_dups = \"duplicate_reads_\" + bn\n",
    "  with open(outfile_dups, 'w') as f:\n",
    "    for line in header:\n",
    "      f.write(f\"{line}\\n\")\n",
    "    for line in dup_reads:\n",
    "      f.write(f\"{line}\\n\")\n",
    "  outfile_notag = \"notag_filtered_mapped_\" + bn\n",
    "  with open(outfile_notag, 'w') as f:\n",
    "    for line in header:\n",
    "      f.write(f\"{line}\\n\")\n",
    "    for line in notags_reads:\n",
    "      f.write(f\"{line}\\n\")\n",
    "\n",
    "  return template_barcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mapped reads:  1180\n",
      "number of unique templates (with tag):  1107\n",
      "number of bad reads (with no tag):  7\n",
      "Number of reads with duplicate barcode/starts:  66\n",
      "1107\n",
      "1043\n",
      "1107\n",
      "1107\n"
     ]
    }
   ],
   "source": [
    "samfile = \"sorted_reads/mapped_test_5000_R1.sam\"\n",
    "template_list = filter_mapped_reads3(samfile)\n",
    "#check that unique templates are actually unique\n",
    "template_barcodes = []\n",
    "for line in template_list:\n",
    "    template_barcodes.append(line[0])\n",
    "    \n",
    "print(len(template_barcodes))\n",
    "unique_barcodes = set(template_barcodes)\n",
    "print(len(unique_barcodes))\n",
    "\n",
    "# for x in set(template_barcodes):\n",
    "#     count_x = template_barcodes.count(x)\n",
    "#     if count_x >1:\n",
    "#         print(x, count_x)\n",
    "\n",
    "#print([[x,template_barcodes.count(x)] for x in set(template_barcodes)])\n",
    "\n",
    "print(len(template_list))\n",
    "template_set = set()\n",
    "for item in template_list:\n",
    "    template_set.add(str(item))\n",
    "print(len(template_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are duplicate barcodes in templates--do these have different starts/strands? All unique combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOw many of these starts are in TA-site list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def take_closest(myList, myNumber):\n",
    "    \"\"\"\n",
    "    Assumes myList is sorted. Returns closest value to myNumber.\n",
    "\n",
    "    If two numbers are equally close, return the smallest number.\n",
    "\n",
    "    from https://stackoverflow.com/questions/12141150/from-list-of-integers-get-number-closest-to-a-given-value/12141511#12141511\n",
    "    \"\"\"\n",
    "    from bisect import bisect_left\n",
    "    pos = bisect_left(myList, myNumber)\n",
    "    if pos == 0:\n",
    "        return myList[0]\n",
    "    if pos == len(myList):\n",
    "        return myList[-1]\n",
    "    before = myList[pos - 1]\n",
    "    after = myList[pos]\n",
    "    if after - myNumber < myNumber - before:\n",
    "        return after\n",
    "    else:\n",
    "        return before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mapped reads:  1180\n",
      "number of unique templates (with tag):  1107\n",
      "number of bad reads (with no tag):  7\n",
      "Number of reads with duplicate barcode/starts:  66\n",
      "Number of reads with tags (total):  1107\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'take_closest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m insert_start \u001b[39m=\u001b[39m line[\u001b[39m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[39m#closest_ta = min(ta_sites, key=lambda x:abs(x-insert_start))  #too slow\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m closest_ta \u001b[39m=\u001b[39m take_closest(ta_sites, insert_start)\n\u001b[1;32m     17\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mabs\u001b[39m(closest_ta \u001b[39m-\u001b[39m insert_start) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m     18\u001b[0m     count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'take_closest' is not defined"
     ]
    }
   ],
   "source": [
    "import scripts.tnseq_pro as tn\n",
    "samfile = \"tests/mapped_test_5000_R1.sam\"\n",
    "bovis_fasta = \"ref_seqs/Mbovis_AF2122-97.fasta\"\n",
    "bovis_seq   = tn.open_fasta(bovis_fasta)\n",
    "ta_sites = tn.find_insertion_sites(bovis_seq)\n",
    "results = tn.filter_mapped_reads3(samfile)\n",
    "barcode_list = results\n",
    "\n",
    "print(\"Number of reads with tags (total): \", len(barcode_list))\n",
    "#print(barcode_list[0:5])\n",
    "\n",
    "count = 0\n",
    "for line in barcode_list:\n",
    "    insert_start = line[0]\n",
    "    #closest_ta = min(ta_sites, key=lambda x:abs(x-insert_start))  #too slow\n",
    "    closest_ta = take_closest(ta_sites, insert_start)\n",
    "    if abs(closest_ta - insert_start) < 2:\n",
    "        count +=1\n",
    "    else:\n",
    "        print(line)\n",
    "print(\"number of starts found in TA site list: \", count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives ok results--1113 reads mapped to a TA site out of 1180 reads total and 1107 unique templates, so seems all templates are mapped to ta site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to use the 'take_closest' function to quantify sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assign_counts_to_sites(ta_sites, barcode_position_list):\n",
    "    \"\"\"\n",
    "    Function to tally ta sites with insertions\n",
    "    \"\"\"\n",
    "\n",
    "    insertions_list = [line[0] for line in barcode_position_list]\n",
    "    read_count_dict = {}\n",
    "    no_match        = []\n",
    "\n",
    "    for site in insertions_list:\n",
    "        #check if ins_site is in list of ta sites (or within 1 nt of ta position)\n",
    "        closest_ta = take_closest(ta_sites, site)\n",
    "        if abs(closest_ta - site) < 2:\n",
    "            if closest_ta not in read_count_dict:\n",
    "                read_count_dict[closest_ta] = 1\n",
    "            else:\n",
    "                read_count_dict[closest_ta] += 1\n",
    "        else:\n",
    "            no_match.append(site)\n",
    "\n",
    "    return read_count_dict, no_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mapped reads:  1180\n",
      "number of unique templates (with tag):  1107\n",
      "number of bad reads (with no tag):  7\n",
      "Number of reads with duplicate barcode/starts:  66\n",
      "1107\n",
      "988\n",
      "{20348: 1, 26122: 1, 31581: 1, 35234: 1, 37337: 1, 39857: 2, 47375: 1, 48092: 1, 51951: 1, 52821: 1, 57868: 1, 63367: 1, 69579: 1, 72386: 1, 81201: 1, 81963: 1, 87311: 1, 87650: 1, 87640: 1, 99975: 1, 105899: 1, 108988: 1, 121806: 1, 122140: 1, 122870: 1, 126866: 1, 129020: 1, 134438: 1, 138138: 1, 154950: 1, 159782: 1, 160624: 1, 162598: 1, 163066: 1, 165864: 1, 165903: 1, 166239: 1, 169495: 1, 176268: 1, 176424: 1, 177547: 1, 178448: 1, 181212: 1, 186606: 1, 188101: 1, 190977: 1, 210469: 1, 222102: 1, 223131: 1, 223235: 1, 224546: 1, 225688: 1, 229915: 1, 231802: 1, 248316: 1, 253846: 1, 263856: 1, 268159: 1, 274566: 1, 274871: 1, 275021: 1, 280048: 1, 295715: 1, 300409: 1, 301037: 1, 307998: 1, 318230: 1, 321350: 1, 325804: 1, 329185: 1, 331336: 1, 337402: 1, 341987: 1, 361190: 1, 361207: 1, 363748: 1, 364277: 1, 364283: 1, 365298: 1, 366912: 1, 369058: 1, 378137: 1, 382825: 1, 384582: 1, 388520: 1, 388628: 1, 389473: 1, 393546: 1, 394906: 1, 395005: 1, 397128: 1, 411237: 1, 413442: 1, 420404: 2, 426005: 1, 426185: 1, 426569: 1, 429170: 1, 435236: 1, 438799: 1, 454023: 2, 455276: 1, 463674: 2, 466100: 1, 468017: 1, 475404: 1, 475611: 1, 491869: 1, 492214: 1, 493374: 1, 496032: 1, 505940: 1, 514243: 1, 518200: 1, 519423: 1, 522787: 1, 523738: 1, 525266: 1, 528347: 2, 529344: 1, 533590: 1, 536316: 2, 537246: 1, 541282: 1, 543796: 1, 544664: 1, 544888: 1, 547791: 1, 547822: 1, 548378: 1, 550585: 1, 551846: 1, 555602: 1, 558545: 1, 558965: 1, 561373: 1, 566280: 1, 566399: 1, 572609: 1, 573387: 1, 573414: 1, 573428: 1, 573615: 1, 575415: 1, 580902: 1, 583027: 3, 592636: 1, 594288: 1, 595088: 1, 595375: 1, 597800: 1, 608016: 1, 627749: 1, 631073: 1, 638795: 1, 642706: 2, 645188: 1, 647078: 1, 651696: 1, 660350: 1, 660680: 1, 666787: 1, 666912: 1, 669688: 1, 692773: 1, 695523: 1, 695909: 1, 695889: 1, 702977: 1, 707398: 1, 707810: 1, 709319: 1, 715727: 1, 718964: 1, 729829: 1, 737482: 1, 738583: 1, 740403: 1, 747419: 1, 752423: 1, 752699: 1, 754840: 1, 758941: 1, 790536: 1, 800222: 1, 812239: 1, 820791: 1, 824237: 1, 824239: 1, 824718: 1, 826282: 1, 833502: 1, 834165: 1, 834988: 1, 837708: 1, 852818: 1, 853050: 1, 853667: 1, 854455: 1, 856144: 1, 864919: 1, 870742: 1, 870947: 1, 878309: 1, 890509: 1, 896544: 1, 901323: 1, 918006: 1, 918008: 1, 920700: 1, 921005: 1, 921443: 1, 923833: 1, 925154: 2, 925691: 3, 933840: 1, 934228: 1, 938702: 1, 938819: 1, 941152: 2, 942173: 1, 943994: 1, 950324: 1, 952437: 1, 958520: 1, 966467: 1, 966860: 1, 970703: 2, 970924: 1, 972121: 1, 977273: 1, 991333: 1, 991250: 1, 992103: 1, 1004830: 1, 1019979: 1, 1022194: 1, 1030412: 1, 1031012: 1, 1041105: 1, 1043320: 1, 1044713: 1, 1053446: 1, 1057494: 1, 1059309: 1, 1064533: 1, 1066421: 1, 1071009: 1, 1088294: 1, 1092969: 1, 1100946: 2, 1111362: 1, 1115790: 1, 1117262: 1, 1118720: 1, 1121348: 1, 1124741: 2, 1144912: 1, 1159260: 1, 1162150: 1, 1165185: 1, 1165724: 1, 1166620: 1, 1167433: 1, 1168698: 1, 1171882: 1, 1193249: 1, 1198690: 1, 1202845: 2, 1207839: 1, 1208405: 1, 1209806: 1, 1210941: 1, 1232032: 1, 1234657: 1, 1242973: 1, 1273718: 1, 1274094: 1, 1275141: 1, 1275456: 1, 1275901: 1, 1276366: 1, 1276930: 1, 1277057: 1, 1276944: 1, 1278048: 1, 1279198: 1, 1280025: 1, 1281000: 1, 1281722: 1, 1283827: 1, 1287953: 1, 1294377: 1, 1295441: 1, 1300768: 1, 1302082: 1, 1307048: 1, 1314045: 1, 1317164: 1, 1331494: 1, 1335173: 1, 1349442: 1, 1349986: 1, 1350963: 1, 1352191: 1, 1352283: 1, 1352287: 1, 1354941: 1, 1357292: 1, 1357470: 1, 1357857: 1, 1367127: 1, 1369436: 1, 1370731: 1, 1377783: 1, 1379737: 1, 1384138: 1, 1394643: 1, 1397158: 1, 1400712: 1, 1407991: 1, 1410325: 1, 1417485: 1, 1418940: 1, 1420823: 1, 1429737: 1, 1440088: 1, 1457599: 1, 1480841: 1, 1481098: 1, 1482562: 1, 1483152: 1, 1492941: 2, 1495339: 1, 1501009: 1, 1510039: 1, 1517377: 1, 1529370: 1, 1531619: 1, 1532299: 1, 1533207: 1, 1535492: 1, 1536283: 1, 1537685: 1, 1539502: 1, 1542791: 1, 1547459: 1, 1548010: 1, 1549263: 1, 1571055: 1, 1582922: 1, 1598629: 1, 1604277: 1, 1604510: 1, 1616106: 1, 1616130: 1, 1616431: 1, 1620795: 1, 1637768: 1, 1658100: 1, 1669128: 1, 1675020: 1, 1675387: 1, 1683953: 1, 1687076: 1, 1693625: 1, 1696057: 1, 1708144: 1, 1711155: 1, 1726840: 1, 1728077: 1, 1731629: 1, 1741622: 1, 1747064: 1, 1748657: 1, 1760323: 1, 1760413: 1, 1760529: 1, 1773171: 1, 1778019: 1, 1778730: 1, 1779433: 4, 1784074: 1, 1791364: 1, 1803282: 1, 1807645: 1, 1815722: 1, 1825866: 1, 1831748: 1, 1841085: 1, 1843892: 1, 1850708: 1, 1878101: 1, 1891571: 1, 1912477: 2, 1914579: 1, 1916722: 1, 1918036: 1, 1921739: 1, 1923751: 1, 1933768: 1, 1935084: 1, 1936113: 1, 1950073: 1, 1954165: 1, 1955656: 1, 1956286: 1, 1964214: 1, 1964378: 1, 1965457: 1, 1971399: 1, 1971700: 1, 1971980: 1, 1973979: 1, 1977176: 1, 1979453: 1, 1984131: 2, 1985715: 1, 1991193: 1, 1991237: 1, 1994326: 1, 1994898: 1, 2004372: 1, 2005181: 1, 2005628: 1, 2006328: 1, 2007873: 1, 2015049: 1, 2017115: 1, 2017698: 2, 2018441: 1, 2028715: 1, 2032733: 1, 2032912: 1, 2034541: 1, 2034543: 1, 2040005: 1, 2041137: 1, 2042421: 1, 2046137: 1, 2046725: 1, 2047903: 1, 2048202: 1, 2055913: 1, 2056208: 1, 2064688: 1, 2065145: 1, 2068542: 1, 2068640: 1, 2070170: 1, 2071996: 2, 2072383: 2, 2074888: 1, 2075170: 1, 2076153: 1, 2081723: 1, 2083303: 1, 2083667: 1, 2084754: 1, 2086116: 1, 2086921: 1, 2087073: 1, 2096012: 1, 2096634: 1, 2099317: 1, 2102041: 1, 2102047: 2, 2106577: 1, 2106474: 1, 2110597: 1, 2111344: 1, 2122205: 1, 2122306: 1, 2123321: 1, 2125074: 1, 2129279: 2, 2132541: 1, 2136794: 1, 2137919: 1, 2139883: 1, 2140472: 1, 2146316: 1, 2149568: 1, 2149747: 1, 2157906: 1, 2165156: 2, 2181115: 1, 2183427: 1, 2186689: 1, 2187030: 1, 2191082: 1, 2193045: 1, 2195162: 1, 2195180: 1, 2195883: 1, 2199729: 1, 2200413: 1, 2201268: 1, 2201503: 1, 2201803: 1, 2202647: 1, 2202863: 1, 2207493: 1, 2207679: 1, 2211656: 1, 2211715: 1, 2215716: 1, 2223696: 1, 2230493: 1, 2242213: 1, 2253087: 1, 2268242: 1, 2269079: 1, 2273928: 1, 2275252: 1, 2277152: 1, 2286455: 1, 2293447: 1, 2294833: 1, 2296940: 1, 2297547: 1, 2305076: 1, 2312176: 1, 2314011: 1, 2314015: 1, 2318816: 1, 2319929: 2, 2323670: 1, 2332345: 2, 2335410: 3, 2339070: 1, 2340873: 1, 2349219: 1, 2354053: 2, 2354355: 1, 2357305: 1, 2362959: 1, 2367435: 2, 2369829: 1, 2374066: 1, 2374575: 1, 2383131: 1, 2387890: 1, 2411033: 1, 2425313: 2, 2426212: 1, 2426612: 1, 2455781: 1, 2456503: 1, 2466636: 1, 2475182: 1, 2476770: 1, 2507501: 1, 2529108: 1, 2534983: 1, 2535731: 1, 2536990: 1, 2543934: 1, 2556188: 1, 2556528: 1, 2557705: 1, 2558063: 1, 2589720: 1, 2590026: 1, 2596621: 1, 2597440: 1, 2597824: 1, 2600060: 1, 2601592: 1, 2601625: 1, 2609316: 1, 2609284: 1, 2616325: 1, 2630679: 1, 2635148: 1, 2636287: 1, 2647943: 1, 2651023: 1, 2651134: 1, 2651456: 1, 2656474: 2, 2661054: 1, 2667627: 1, 2672094: 1, 2672322: 1, 2673606: 1, 2674221: 1, 2675896: 1, 2677761: 1, 2688598: 1, 2691652: 1, 2700413: 2, 2725529: 2, 2725768: 1, 2729375: 1, 2730605: 1, 2733020: 1, 2735813: 1, 2736653: 1, 2743381: 1, 2744873: 1, 2755070: 1, 2762603: 1, 2773084: 1, 2787004: 1, 2788695: 1, 2789051: 1, 2790873: 1, 2795181: 1, 2800426: 1, 2816170: 1, 2819624: 1, 2821958: 1, 2824659: 1, 2833353: 1, 2833734: 1, 2846108: 1, 2857059: 1, 2857505: 1, 2861955: 1, 2862198: 1, 2869227: 1, 2869305: 1, 2871126: 1, 2871086: 1, 2877915: 1, 2877918: 1, 2884283: 1, 2887188: 1, 2888790: 1, 2895266: 1, 2895279: 1, 2896113: 1, 2898140: 1, 2901178: 1, 2911389: 1, 2914540: 1, 2914946: 1, 2916021: 1, 2930793: 1, 2932507: 1, 2933863: 1, 2936941: 1, 2937137: 1, 2948421: 1, 2953752: 1, 2954618: 1, 2956615: 1, 2957286: 1, 2967757: 1, 2976310: 1, 2977726: 1, 2977950: 1, 2978486: 1, 2980896: 1, 2980948: 1, 2981237: 1, 2983247: 1, 2991353: 1, 2993206: 1, 2993464: 1, 2994599: 1, 2994660: 1, 2998578: 1, 2999533: 1, 3002083: 1, 3003066: 1, 3003340: 1, 3006598: 1, 3008584: 1, 3014642: 1, 3032149: 1, 3035181: 1, 3035651: 1, 3038147: 1, 3044215: 1, 3045250: 2, 3046336: 1, 3055286: 1, 3057824: 1, 3069342: 1, 3082057: 1, 3095181: 2, 3096307: 1, 3128164: 1, 3131538: 1, 3134850: 1, 3140337: 1, 3140968: 1, 3143863: 1, 3152336: 1, 3158760: 1, 3160898: 1, 3164777: 1, 3168200: 1, 3178462: 1, 3200697: 1, 3204131: 1, 3230855: 1, 3237028: 1, 3238511: 1, 3244877: 1, 3248376: 1, 3251794: 1, 3254215: 2, 3254248: 1, 3258047: 1, 3259391: 1, 3263053: 1, 3264288: 1, 3268312: 1, 3268476: 1, 3272227: 1, 3272799: 1, 3273721: 1, 3274973: 1, 3278585: 1, 3288129: 1, 3308887: 1, 3312921: 1, 3320660: 1, 3322638: 1, 3322891: 2, 3337019: 1, 3337895: 1, 3337936: 1, 3343507: 1, 3353319: 1, 3355970: 1, 3357900: 1, 3366367: 2, 3367581: 1, 3368646: 1, 3371577: 1, 3373285: 1, 3373590: 2, 3373687: 1, 3374151: 1, 3375747: 1, 3376955: 1, 3380597: 1, 3383229: 1, 3386631: 1, 3393265: 1, 3395920: 1, 3397372: 1, 3398609: 1, 3412521: 1, 3416610: 1, 3422677: 1, 3429042: 1, 3437477: 1, 3444337: 2, 3444385: 1, 3446902: 1, 3447322: 1, 3448248: 2, 3455559: 1, 3460259: 1, 3462593: 1, 3462829: 2, 3462970: 1, 3466008: 1, 3465963: 1, 3488913: 1, 3491646: 1, 3494487: 1, 3499990: 1, 3505393: 1, 3510220: 1, 3510906: 1, 3511515: 1, 3512077: 1, 3514410: 1, 3518747: 1, 3520380: 2, 3521561: 1, 3521691: 1, 3523385: 1, 3524653: 1, 3525866: 1, 3527077: 1, 3534868: 1, 3540539: 1, 3543398: 1, 3544928: 1, 3546160: 1, 3549053: 1, 3553628: 1, 3553681: 2, 3554933: 1, 3561867: 1, 3564579: 1, 3565205: 1, 3565722: 1, 3566053: 1, 3566571: 1, 3568736: 1, 3569834: 1, 3571375: 1, 3575760: 1, 3581227: 1, 3584406: 1, 3585172: 1, 3586266: 1, 3594864: 1, 3599231: 1, 3608228: 1, 3615285: 1, 3619982: 1, 3623533: 1, 3624486: 1, 3638327: 1, 3648873: 1, 3651635: 1, 3653349: 1, 3654056: 1, 3657895: 1, 3667154: 1, 3667276: 1, 3669913: 1, 3670104: 1, 3672454: 1, 3673147: 1, 3678061: 1, 3678459: 1, 3681921: 1, 3697348: 1, 3699731: 1, 3701728: 1, 3701811: 1, 3702549: 1, 3705212: 1, 3705601: 1, 3711313: 1, 3711966: 1, 3718413: 1, 3721209: 1, 3722122: 1, 3722332: 2, 3723077: 2, 3725388: 1, 3727437: 1, 3731167: 1, 3731089: 1, 3732352: 1, 3738537: 1, 3745192: 1, 3759553: 1, 3761512: 1, 3768561: 1, 3768899: 1, 3769375: 1, 3772102: 1, 3774278: 1, 3775787: 1, 3779515: 1, 3780997: 1, 3784342: 1, 3787719: 1, 3795350: 1, 3796239: 1, 3796748: 1, 3798436: 1, 3804604: 1, 3818169: 1, 3822216: 1, 3822317: 1, 3828967: 1, 3829208: 1, 3831850: 1, 3839079: 1, 3842501: 1, 3846107: 1, 3847260: 2, 3850241: 1, 3851047: 1, 3852128: 1, 3852658: 1, 3859816: 2, 3862678: 1, 3863850: 1, 3865678: 1, 3865744: 1, 3871092: 1, 3876823: 1, 3884021: 1, 3890410: 1, 3890413: 1, 3896893: 2, 3897925: 1, 3901948: 1, 3904999: 1, 3905561: 1, 3912539: 1, 3912777: 1, 3912916: 1, 3917018: 1, 3920426: 1, 3927667: 1, 3930130: 1, 3934575: 1, 3948834: 1, 3958909: 1, 3965179: 1, 3969507: 1, 3970721: 1, 3973761: 1, 3991361: 1, 3998427: 1, 4000484: 1, 4008913: 1, 4017394: 1, 4027272: 1, 4032275: 1, 4038609: 1, 4039572: 1, 4040402: 1, 4053752: 2, 4056049: 1, 4061943: 2, 4062737: 1, 4064103: 1, 4066935: 1, 4068221: 1, 4069407: 1, 4085184: 1, 4087676: 1, 4091084: 1, 4093935: 1, 4098460: 1, 4099789: 1, 4100063: 1, 4108672: 1, 4110229: 1, 4114395: 2, 4116895: 1, 4120066: 2, 4127489: 1, 4132972: 1, 4135744: 1, 4136632: 1, 4141420: 1, 4142776: 1, 4148395: 1, 4156478: 1, 4166393: 1, 4168930: 1, 4168829: 1, 4170071: 1, 4200957: 1, 4200966: 1, 4210296: 1, 4210229: 1, 4214020: 1, 4216050: 1, 4223560: 1, 4232485: 1, 4234348: 1, 4235791: 1, 4236788: 1, 4247668: 1, 4247941: 1, 4261838: 1, 4262890: 1, 4273116: 1, 4281358: 1, 4283122: 1, 4283460: 1, 4288641: 1, 4291262: 2, 4292507: 1, 4294418: 1, 4294475: 1, 4298352: 1, 4302845: 1, 4303873: 1, 4306948: 1, 4310703: 1, 4312798: 1, 4313507: 2, 4315069: 1, 4320528: 1, 4320580: 1, 4320694: 1, 4334043: 1, 4341422: 1, 4342163: 1}\n"
     ]
    }
   ],
   "source": [
    "samfile = \"sorted_reads/mapped_test_5000_R1.sam\"\n",
    "bovis_fasta = \"ref_seqs/Mbovis_AF2122-97.fasta\"\n",
    "bovis_seq   = open_fasta(bovis_fasta)\n",
    "ta_sites = find_insertion_sites(bovis_seq)\n",
    "# make list of the positions of every insertion from unique templates\n",
    "template_positions = filter_mapped_reads3(samfile)\n",
    "print(len(template_positions))\n",
    "#count number of reads per ta site\n",
    "res=assign_counts_to_sites(ta_sites, template_positions)\n",
    "print(len(res[0]))\n",
    "print(res[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to go through all possible ta sites and create .wig file of insertions\n",
    "\n",
    "def ta_sites_to_dict(ta_sites, read_count_dict):\n",
    "    \"\"\" \n",
    "    Function to create wig file of insertions\n",
    "    Input               ta_sites                ordered list of all possible ta sites in Mbovis genome\n",
    "                        read_count_dict         dictionary of ta sites with insertions and read counts\n",
    "    Output              wig_dict                dict of insertions\n",
    "\n",
    "    \"\"\"\n",
    "    wig_dict = {}\n",
    "    for site in ta_sites:\n",
    "        if site in read_count_dict:\n",
    "            count = read_count_dict[site]\n",
    "        else:\n",
    "            count = 0\n",
    "        wig_dict[site] = count\n",
    "    return wig_dict\n",
    "\n",
    "def write_wig_from_dict(wig_dict, sample_name, genome):\n",
    "    \"\"\"\n",
    "    Function to write wig file from dict of insertions\n",
    "    Input               wig_dict            dict of insertions\n",
    "                        outfile_name        str of outfile name\n",
    "    Output              outfile             wig file of insertions\n",
    "    \"\"\"\n",
    "   \n",
    "    #write template reads\n",
    "    outfile = \"output/\" + sample_name + \"_insertions.wig\"\n",
    "    with open(outfile, 'w') as f:\n",
    "        f.write(\"#generated by tnseq_pro from \" + sample_name + \"\\n\")\n",
    "        f.write(\"variableStep chrom=\" + genome + \"\\n\")\n",
    "        for key, value in wig_dict.items():\n",
    "            f.write(f\"{key} {value}\\n\")\n",
    "    return outfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samfile = \"sorted_reads/mapped_test_5000_R1.sam\"\n",
    "bovis_fasta = \"ref_seqs/Mbovis_AF2122-97.fasta\"\n",
    "bovis_seq   = open_fasta(bovis_fasta)\n",
    "ta_sites = find_insertion_sites(bovis_seq)\n",
    "# make list of the positions of every insertion from unique templates\n",
    "template_positions = filter_mapped_reads3(samfile)\n",
    "print(len(template_positions))\n",
    "#count number of reads per ta site\n",
    "res=assign_counts_to_sites(ta_sites, template_positions)\n",
    "ta_dict = ta_sites_to_dict(ta_sites, res[0])\n",
    "for key, value in ta_dict.items():\n",
    "    if value > 0:\n",
    "        print(key, value)\n",
    "\n",
    "print(len(ta_dict))\n",
    "write_wig_from_dict(ta_dict, \"test\", \"Mbovis_AF2122-97\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper for creating wig file from sam file\n",
    "\n",
    "def sam_to_wig(samfile, genome_fasta, sample_name):\n",
    "    \"\"\"\n",
    "    Wrapper function to create wig file from sam file\n",
    "    Input           samfile         sam file of mapped reads   \n",
    "                    genome_fasta    path to fasta file of genome        str\n",
    "                    sample_name     name of sample                      str\n",
    "    Output          wig file        wig file of insertions\n",
    "    \"\"\"\n",
    "    import os\n",
    "    fasta       = genome_fasta\n",
    "    #get genome name from fasta file\n",
    "    genome      = os.path.basename(fasta).split(\".\")[0]\n",
    "    fasta_seq   = open_fasta(genome_fasta)\n",
    "    ta_sites    = find_insertion_sites(fasta_seq)\n",
    "    # make list of the positions of every insertion from unique templates\n",
    "    template_positions = filter_mapped_reads3(samfile)\n",
    "    #count number of reads per ta site\n",
    "    res=assign_counts_to_sites(ta_sites, template_positions)\n",
    "    ta_dict = ta_sites_to_dict(ta_sites, res[0])\n",
    "    wig_file = write_wig_from_dict(ta_dict, sample_name, genome)\n",
    "    return wig_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mapped reads:  1180\n",
      "number of unique templates (with tag):  1107\n",
      "number of bad reads (with no tag):  7\n",
      "Number of reads with duplicate barcode/starts:  66\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'output/test_insertions.wig'"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam = \"sorted_reads/mapped_test_5000_R1.sam\"\n",
    "bovis_fasta = \"ref_seqs/Mbovis_AF2122-97.fasta\"\n",
    "\n",
    "sam_to_wig(sam, bovis_fasta, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map and test entire sample\n",
    "\n",
    "moved scripts to thoth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "scp ~/tn_seq/menadione_tnseq/scripts/tnseq_pro.py sj003@ssh.cryst.bbk.ac.uk:/d/in16/u/sj003/men_tnseq/scripts/\n",
    "\n",
    "scp ~/tn_seq/menadione_tnseq/Menadione_tnseq.ipynb sj003@ssh.cryst.bbk.ac.uk:/d/in16/u/sj003/men_tnseq/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write script to apply add_barcode.py to each R1/R2 combination\n",
    "\n",
    "Updating (tnseq_process_pipeline.ipynb) to summarise steps and code.\n",
    "\n",
    "Uses module 'tnseq_pro.py'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through samples and run sam_to_wig script\n",
    "\n",
    "# iterate through files\n",
    "import scripts.tnseq_pro as tn\n",
    "\n",
    "# make list of .sam files in directory\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "sam_files = glob.glob(\"sorted_reads\" + \"/*.sam\")\n",
    "print(sam_files)\n",
    "bovis_fasta = \"ref_seqs/Mbovis_AF2122-97.fasta\"\n",
    "for file in sam_files:\n",
    "    #find sample name from file\n",
    "    sample_filename = os.path.basename(file).split(\".\")[0]\n",
    "    sample_name = re.findall(r'mapped_(\\w*)_R1_001', sample_filename)[0]\n",
    "    print(sample_name)\n",
    "    print(file)\n",
    "    tn.sam_to_wig(file, bovis_fasta, sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "On laptop, this takes a long time: stopped after 210 min for single file\n",
    "\n",
    "this step seems to be the really long one: barcode_list.sort(key=itemgetter(0))  \n",
    "Where it filters by barcodes and starts.\n",
    "-find tag\n",
    "-find start position\n",
    "-eliminate duplicates for each start position\n",
    "\n",
    "searching by TA site (only ~73K) and eliminate extra barcodes will be much faster\n",
    "\n",
    "find_closest might also take a while...\n",
    "Perhaps only run find closest on the ones that are don't match a ta site. Majority of those with tag matched a TA site."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "KeyboardInterrupt                         Traceback (most recent call last)\n",
    "Cell In[15], line 19\n",
    "     17 print(sample_name)\n",
    "     18 print(file)\n",
    "---> 19 tn.sam_to_wig(file, bovis_fasta, sample_name)\n",
    "\n",
    "File ~/tn_seq/menadione_tnseq/scripts/tnseq_pro.py:440, in sam_to_wig(samfile, genome_fasta, sample_name)\n",
    "    438 ta_sites    = find_insertion_sites(fasta_seq)\n",
    "    439 # make list of the positions of every insertion from unique templates\n",
    "--> 440 template_positions = filter_mapped_reads3(samfile)\n",
    "    441 #count number of reads per ta site\n",
    "    442 res=assign_counts_to_sites(ta_sites, template_positions)\n",
    "\n",
    "File ~/tn_seq/menadione_tnseq/scripts/tnseq_pro.py:194, in filter_mapped_reads3(sam_file, tag, mismatch_max)\n",
    "    192 #add to list of all reads with insertions\n",
    "    193 barcode_list.append(pos_barcode) \n",
    "--> 194 barcode_list.sort(key=itemgetter(0))  #maybe this will speed up search ordering by position--but sorting will add time\n",
    "    195 # if hasn't been added before, add read to unique template reads\n",
    "    196 if barcode_list.count(pos_barcode) < 2:\n",
    "\n",
    "KeyboardInterrupt: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### speed optimisation\n",
    "\n",
    "filter_mapped_reads3(sam_file, tag, mismatch_max) is taking way too long, mainly at point where it has to sort all the reads by barcode.\n",
    "\n",
    "break down into different parts with duplicates in final part--searching each ta sit si"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scripts.tnseq_pro as tn\n",
    "\n",
    "def filter_mapped_reads_tag(sam_file, tag=\"ACTTATCAGCCAACCTGTTA\", mismatch_max=2):\n",
    "  \"\"\" \n",
    "  Revised filtering function: filters reads for presence of transposon tag and for duplicate barcode/ta-position\n",
    "\n",
    "  Input             sam_file                            .sam file of mapped reads\n",
    "                    tag                                 str of transposon tag used, default is Himar1\n",
    "                    mismatch_max                        integer of maximum mismatches allowed in search for tag\n",
    "  Output            read barcodes                       list of 3 values: insert_start, strand, barcode from reads with insertions\n",
    "                                                        \n",
    "  \"\"\"\n",
    "  import sys\n",
    "  import os\n",
    "\n",
    "  #check tag has valid nucleotides\n",
    "  if tn.check_seq(tag):\n",
    "     pass\n",
    "  else:\n",
    "     sys.exit(\"Invalid sequence tag\")\n",
    "\n",
    "  tagged_reads    = []\n",
    "  barcode_list    = []\n",
    "  notag_count = 0\n",
    "\n",
    "  #read sam_file and sort lines between header and reads\n",
    "  data    = tn.read_samfile(sam_file)\n",
    "  header  = data[0]\n",
    "  reads   = data[1]\n",
    "\n",
    "  for read in reads:\n",
    "    tag_pos   = tn.find_tags2(read, tag, mismatch_max)\n",
    "    strand    = tn.parse_samflag(read)\n",
    "    if tag_pos != -1:  #there is match for tranposon tag\n",
    "        insertion_coord = tn.find_ta_position(read, tag_pos)\n",
    "        #find barcode\n",
    "        read_name  = read.split()[0]\n",
    "        barcode    = read_name.split(\"BC:\",1)[1]\n",
    "        pos_barcode = [insertion_coord, strand, barcode]\n",
    "        #add to list of all reads with insertions\n",
    "        barcode_list.append(pos_barcode) \n",
    "        #append read to tagged reads\n",
    "        tagged_reads.append(read)\n",
    "    else:\n",
    "      notag_count += 1\n",
    "  \n",
    "  print(\"Total number of mapped reads: \", len(reads))\n",
    "  print(\"number of reads with tag: \", len(barcode_list))\n",
    "  print(\"number of reads with no tag: \", notag_count)\n",
    "  \n",
    "  #write read files\n",
    "  bn = os.path.basename(sam_file)\n",
    "  outfile = \"tag_filtered_templates_\" + bn\n",
    "  with open(outfile, 'w') as f:\n",
    "    for line in header:\n",
    "      f.write(f\"{line}\\n\")\n",
    "    for line in tagged_reads:\n",
    "      f.write(f\"{line}\\n\")\n",
    "  \n",
    "  \n",
    "\n",
    "  return barcode_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mapped reads:  1180\n",
      "number of reads with tag:  1173\n",
      "number of reads with no tag:  7\n",
      "[[20348, 'R', 'GAACAACT'], [26121, 'F', 'CGCCACCT'], [31581, 'F', 'CCGACTGA'], [35234, 'R', 'TATTCTAA'], [37337, 'F', 'AATCACTC'], [39857, 'R', 'TGAGGCGA'], [39857, 'F', 'CGAGACAG'], [47375, 'F', 'ACAGGATC'], [48092, 'R', 'CTGCTCCA'], [51951, 'F', 'GACCAAGC']]\n"
     ]
    }
   ],
   "source": [
    "sam = \"tests/mapped_test_5000_R1.sam\"\n",
    "bc_list = filter_mapped_reads_tag(sam)\n",
    "print(bc_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_sites(ta_sites, barcode_list):\n",
    "\n",
    "    \"\"\"\n",
    "    Function to assign \n",
    "    Input               ta_sites                ordered list of all possible ta sites in Mbovis genome\n",
    "                        site_barcode_dict       dictionary of ta sites:[list of all barcodes at site]\n",
    "    Output              tasite_barcode_dict     dictionary of ta sites with insertions: [list of barcodes at site]\n",
    "                        no_match                list of barcodes with no match to ta site\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    tasite_barcode_dict = {}\n",
    "    no_match        = []\n",
    "\n",
    "    for line in barcode_list:\n",
    "        site = line[0]\n",
    "        strand = line[1]\n",
    "        barcode = line[2]\n",
    "        #check if site is in list of ta sites  \n",
    "        if site in ta_sites:\n",
    "            #check tasite_barcode_dict for site\n",
    "            if site in tasite_barcode_dict:\n",
    "                #get list of barcodes at site\n",
    "                bc_list = tasite_barcode_dict[site]\n",
    "                #add barcode to existing list of barcodes\n",
    "                bc_list.append(barcode)\n",
    "                #update dictionary\n",
    "                tasite_barcode_dict[site]=bc_list\n",
    "            else:\n",
    "                tasite_barcode_dict[site] = [barcode]\n",
    "        else:\n",
    "            # if no ta site matched, look for ta sites up  to 2 nt away to account for matching gtta or tta\n",
    "            closest_ta = tn.take_closest(ta_sites, site)\n",
    "            if abs(closest_ta - site) < 3:\n",
    "                #is closest_ta in dict\n",
    "                if closest_ta in tasite_barcode_dict:\n",
    "                    bc_list = tasite_barcode_dict[closest_ta]\n",
    "                    bc_list.append(barcode)\n",
    "                    tasite_barcode_dict[closest_ta] = bc_list\n",
    "                else:\n",
    "                    tasite_barcode_dict[closest_ta] = [barcode]\n",
    "            else:\n",
    "                no_match_str = site, strand, barcode\n",
    "                no_match.append(no_match_str)\n",
    "\n",
    "    return tasite_barcode_dict, no_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of mapped reads:  1180\n",
      "number of reads with tag:  1173\n",
      "number of reads with no tag:  7\n",
      "1001\n",
      "45\n",
      "(500641, 'F', 'AGACCTTT')\n",
      "(812555, 'F', 'AAGACGTC')\n",
      "(824174, 'F', 'CAGTGGTG')\n",
      "(1357296, 'F', 'ACCGACGG')\n",
      "(1775261, 'F', 'AACTCCAA')\n",
      "(2883723, 'F', 'CGCTAAGG')\n",
      "(2883723, 'F', 'CTACGGAC')\n",
      "(2883723, 'F', 'ATTTCCTA')\n",
      "(2883723, 'F', 'CCAACCTA')\n",
      "(2883723, 'F', 'CACTTAGG')\n",
      "(2883723, 'F', 'AAGACCAA')\n",
      "(2916736, 'F', 'GACTTCGT')\n",
      "(3719320, 'F', 'TGCATTAA')\n",
      "(3828970, 'F', 'CTAGCACT')\n",
      "(3828970, 'F', 'CTAGCACT')\n",
      "(3926596, 'F', 'CTTCGTGG')\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "sam = \"tests/mapped_test_5000_R1.sam\"\n",
    "bc_list = filter_mapped_reads_tag(sam)\n",
    "bovis_fasta = \"ref_seqs/Mbovis_AF2122-97.fasta\"\n",
    "bovis_seq   = tn.open_fasta(bovis_fasta)\n",
    "ta_sites = tn.find_insertion_sites(bovis_seq)[0]\n",
    "bc_dict = assign_sites(ta_sites, bc_list)\n",
    "#for k,v in bc_dict[0].items()#:\n",
    "#    print(k,v)\n",
    "print(len(bc_dict[0]))\n",
    "print(len(bc_dict[1]))\n",
    "no_match = bc_dict[1]\n",
    "count = 1\n",
    "for line in no_match:\n",
    "    if line[1] == 'F':\n",
    "        count += 1\n",
    "        print(line)\n",
    "print(count)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check these no match sites on Artemis. is threshold of 2 nt reasonable?\n",
    "These sites are stil being calculated wrong:\n",
    "179561, R  -28 179533\n",
    "336343, R -31 should be 336312  \n",
    "373583, F should be 373585  -2 but matched -gtta so this is ok (if threshold <3 will be included)\n",
    "429332, R, should be 429331 -1 (matched -gtta)\n",
    "431924, R, should be 431921 -3\n",
    "435857, R should be 435790, -33 nowhere near read start (or end)\n",
    "439737, F should be at 439739 -2\n",
    "500641, F should be 500645 + matches \"CTGTTA\"\n",
    "566691, R should be 566641 -50\n",
    "\n",
    "\n",
    "30 reads in R direction didn't match a TA site. maybe have extra step to catch these? seems like the transposon wasn't soft-clipped? Look at some of these reads\n",
    "\n",
    "\n",
    "17 forward reads mostly didn't match at all--some from same position:\n",
    "2883723 F: these reads don't seem to map to this coordinate though this is where they are aligned? no TA site in vicinity 6 reads show on artemis\n",
    "2916736 also mis-mapped\n",
    "3719320 and this one\n",
    "500641 not a TA site\n",
    "812555 not a TA site\n",
    "3828970 this one looks like should be a TA site but read doesn't seem to match genome here\n",
    "\n",
    "\n",
    "\n",
    "Maybe some of the R reads are still including soft-clipping in the alignment start. seems about the length of the transposon. Other reads have sequences that match the end of the transposon, so they are matched earlier than the TA site. I assume the insertion still occurs at TA site, so need to allow some flexibility for these reads.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A01968:63:H77VYDSX5:4:1101:16857:1438_1:N:0:AACGTGAT_BC:CAAAACTA\t16\tNC_002945.3\t566575\t60\t51S67M33S\t*\t0\t0\tGTGGTGTATTCGTAGCTGGATTATCCAGCCTATGGCCGGCCCGGCAAAACCGCGACTCCGTCGATGACGTGCAGCAAAGGAGACATGTAGTGACCGGATCAGCTGGGCCTGACATCTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\t,:,:,,F,:,F,,F,,,,,,,,,,:,,F:,,FF,F,F,,,,,,,,,,FF::,,F,,F::,FFFFFF,FFFFF,FFFFFFFFFFF,FFFFFFFF:FFF:F::F:,F,,:,,F,FFFFFFFF:FFF:FFFFFFFF,F::,FF:FF,FFFFFFF\tNM:i:0\tMD:Z:67\tAS:i:67\tXS:i:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import scripts.tnseq_pro as tn\n",
    "samfile = \"tests/mapped_test_5000_R1.sam\"\n",
    "data    = tn.read_samfile(samfile)\n",
    "header  = data[0]\n",
    "reads   = data[1]\n",
    "\n",
    "def find_reads(samfile, value):\n",
    "    for read in reads:\n",
    "        #print(read.split()[3])\n",
    "        if int(read.split()[3]) > value -200 and int(read.split()[3]) < value + 200:\n",
    "            print(read)\n",
    "\n",
    "find_reads(samfile, 566691)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read left most position is 336227. TA starts at 336312. start of gDNA insert is other end of read (right-most position) which is left position + len of alignment (336227 + 86 = 336313) but should also correspond to tag position. However, maybe because tag is present at both ends--soft clipping at both ends, the prediction is 336343 which is pretty close  to adding TA site 336312 + 32 sc bases from left end of read? \n",
    "\n",
    "\n",
    "\n",
    "179561 also two soft clips, 5' soft clip is maybe rc tranposon sequence but too many mismatches?\n",
    "\n",
    "\n",
    "435857 in read   68S34M33S maps to TA at 435790 which is length of first SC\n",
    "This one has adapter P7 sequence still attached\n",
    "\n",
    "\n",
    "566575\tpredicted to be 566691, 51S67M33S, TA starts at 566641 which is prediction - 51 soft-clipping +1; or left most - align length + 1\n",
    "\n",
    "GTGGTGTATTCGTAGCTGGATTATCCAGCCTATGGCCGGCCCGGCAAAACC this is 51 clipped from 3' end of read. \n",
    "\n",
    "If there are two soft clips, could first subtract first one from prediction before calculating start.\n",
    "\n",
    "Or if we go back to finding transposon tag in reverse complement of read\n",
    "\n",
    "\n",
    "Try the complement of read instead of reverse complement--we are searching reverse strand reads backwards, 3' to 5' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create small set of reads with soft-clipping on both ends to see how the tranposon tag position is identified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['29S', '89M', '33S']\n",
      "['32S', '86M', '33S']\n",
      "['2S', '116M', '33S']\n",
      "['2S', '116M', '33S']\n",
      "['4S', '92M', '33S']\n",
      "['68S', '34M', '33S']\n",
      "['51S', '67M', '33S']\n",
      "['13S', '105M', '33S']\n",
      "['33S', '85M', '33S']\n",
      "['2S', '102M', '32S']\n",
      "['16S', '102M', '33S']\n",
      "['3S', '115M', '33S']\n",
      "['5S', '113M', '33S']\n",
      "['69S', '49M', '33S']\n",
      "['4S', '114M', '33S']\n",
      "['4S', '114M', '33S']\n",
      "['4S', '114M', '33S']\n",
      "['34S', '84M', '33S']\n",
      "['25S', '93M', '33S']\n",
      "['78M', '1D', '40M', '33S']\n",
      "['78S', '40M', '33S']\n",
      "['35S', '83M', '33S']\n",
      "['3S', '97M', '33S']\n",
      "['19S', '105M', '27S']\n",
      "['21S', '103M', '27S']\n",
      "['3S', '115M', '33S']\n",
      "['19S', '99M', '33S']\n",
      "['51S', '67M', '33S']\n",
      "['5S', '113M', '33S']\n",
      "['65S', '54M', '32S']\n",
      "['20S', '100M', '31S']\n",
      "['31S', '87M', '33S']\n",
      "['52S', '66M', '33S']\n",
      "['7S', '59M', '33S']\n",
      "['64S', '54M', '33S']\n",
      "['43S', '75M', '33S']\n",
      "A01968:63:H77VYDSX5:4:1101:15347:2644_1:N:0:AACGTGAT_BC:CAGTGTAC\t0\tNC_002945.3\t57868\t60\t32S79M3S\t*\t0\t0\tGTCTAAGACCGGGGACTTATCAGCCAACCTGTTATGACCCGCAGCCACCGGTGGATGCCGGGTCCCTGGCCAAGGCCTCGCCGGCTACCCATCGGCGCGCGTTTGAGTTCTGTA\tFF,FFF:FFFFF:FFFFFFFFF,FFFFFFFFFFFFFFF:FFFFFFF:FFFFFFF:F,,FFF,::F,,,FFFF:FFFF,FFFF:F,:FFFF:,FF:F,::F,FF,F,,FF::FFF\tNM:i:3\tMD:Z:29C34T6G7\tAS:i:64\tXS:i:0 \n",
      " 34 57868\n",
      "A01968:63:H77VYDSX5:4:1101:20085:1924_1:N:0:AACGTGAT_BC:TGTTCCAG\t0\tNC_002945.3\t87311\t60\t33S116M2S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAATCAGCTCGATAGCTTTGCGCGCATCTTGGATATCTTGAGGCGATGCGGCGTCCACGAGCGCACGTAGATCACTGCGATCCTGGGGTCGCCGATCATCATCTCTCGCAAGAAGTAG\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFF\tNM:i:0\tMD:Z:116\tAS:i:116\tXS:i:0 \n",
      " 35 87311\n",
      "A01968:63:H77VYDSX5:4:1101:13386:2472_1:N:0:AACGTGAT_BC:TAGGCCGT\t0\tNC_002945.3\t134437\t60\t32S72M47S\t*\t0\t0\tGTCTAGAGACCCGGGACTTATCAGCCAACCTGTTACTTCACCCAGGGCGCTCCACCCTCGCCCCTACAGCACACCTGGTCGTTGGGGGGGGGGGATCAGTATTAAGTTCTATGGCCCCAGTTGCATATACGGGCGACCCTACAGTTGGCGG\tFFFF:::FFFF,F:F:,FFF::FFFF:F,FF,F::FF:FFFF:F,,FFFF,FFFFF,F:FF:F,,FFFFF:FF:::FFF::F,,,F,F,:F,F,F,FFF,,,FF,,,F,F,,,,F,,,:F,:,F,,:F,,:F,F:FF,::FF,,,F::FFF\tNM:i:3\tMD:Z:56T2A3G8\tAS:i:57\tXS:i:0 \n",
      " 35 134437\n",
      "A01968:63:H77VYDSX5:4:1101:14172:2487_1:N:0:AACGTGAT_BC:ATGTCAAA\t0\tNC_002945.3\t160624\t60\t33S113M5S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTATATGGGACAGGTCACCCCGGTGCCGAACAAGATGCTGCTGGTGCTGTTGCCCACCAACGCCAGGGTCAGCCAGGGAAACATCGAATGGCCCTCGTTGTCTTGCCATGGCTGAGATC\tFFF:F:FFFFFFF:FFFF:FF:F:FFFFF,FF,,FFF:FFFF:FFF,FFFF::FFFFFF:FFF,F:FF,:FFFFFFF,:FF:F:FFFFFFFFFFFFFF:FFFFF:FFFFFF,FFF,FFFF::F,:FFFFF:FFF:F,FFFF:FFFFFFFFF\tNM:i:0\tMD:Z:113\tAS:i:113\tXS:i:0 \n",
      " 35 160624\n",
      "A01968:63:H77VYDSX5:4:1101:14136:1391_1:N:0:AACGTGAT_BC:AGCTCTAA\t0\tNC_002945.3\t369058\t60\t33S93M25S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGTCACCGGAGTTGAAGCTGCCGGTCTTGTCAAAGCCGAGGTTGAAGCTGCCGGTATTGTTGCTGCCAGTGTTGAAGCTGCCGGTGATGATGATGAACGTGTTGCCCACGCCGCTCT\t,FFFF:FF:FFF,:FFF:F::,F,::FFFF,::FF,F,F:F,FFF:,,FF,FFFFFFFF,:,F,,F,,FF,:F,,,F:F,,:,,:F,:FF,,F,F:F,,,:FFF,F,,:F::,,FF,F,F,FFF,F,F,,F,,,:,FF,,,,F:,F,,:,F\tNM:i:6\tMD:Z:26G4G1T0C25G26T5\tAS:i:63\tXS:i:0 \n",
      " 35 369058\n",
      "A01968:63:H77VYDSX5:4:1101:18774:1846_1:N:0:AACGTGAT_BC:GGGGGGGG\t0\tNC_002945.3\t642706\t60\t33S71M47S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGTAGAAGCCATCGGAGTCCTCGCGGGCCAGGTCGCCGGTGTGCAGCCAGCCGTCTTTAAAAGTCCGCGAGATCGGAAGAGCACACGTCTGAACTCAGTCCAAAGGGCGATCGCGGA\tFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFF:FF:FFFFFFFFFFF:FFFFFFFFFFFFFFFFFF:F,FFFFFFFFFFFFFF,FF::FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFF:F,F,FFFFFFFFFFFFF,F:,,\tNM:i:0\tMD:Z:71\tAS:i:71\tXS:i:24 \n",
      " 35 642706\n",
      "A01968:63:H77VYDSX5:4:1101:1127:2660_1:N:0:AACGTGAT_BC:TAATGCCA\t0\tNC_002945.3\t695523\t60\t33S115M3S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACTGCAGGATTCCGATGGACGCGAGCAATGTGGTTCGTGGAGCACGCAACAACCCGTGTGTAGACGTGCCCGGCAAGCGGGCGGCGACCCCGCGGGAATGCCGCAGCAATGAAGATC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:115\tAS:i:115\tXS:i:0 \n",
      " 35 695523\n",
      "A01968:63:H77VYDSX5:4:1101:4607:2425_1:N:0:AACGTGAT_BC:ACGCTCCC\t0\tNC_002945.3\t925690\t60\t32S114M5S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACACTGCTTCCTGCTGATTCCTCCCTGCGATCGGTCGATCGCAGGATCGGTTGGCATCGAGGTCATGTCGCTGTGGGAGGAGATGTCGCGTGTCTTATGTGAGCGTGTTGCCAGATC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:F:FFF:FFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFF\tNM:i:0\tMD:Z:114\tAS:i:114\tXS:i:0 \n",
      " 35 925690\n",
      "A01968:63:H77VYDSX5:4:1101:20193:2550_1:N:0:AACGTGAT_BC:ACCAACTA\t0\tNC_002945.3\t941152\t17\t32S96M23S\t*\t0\t0\tGTCAGAGACCGGGGACATATCAGCCAACCTGTTAGATGTACGTTGGCCGCGGACAACCCGGACGTGACATGCCTACTCCCCGCGGCCCCAGCTAGTACAGCAGCCACTTCCGCATCGACTCGGCGACCTCCCCCGCTCGGTCGCGCACCCG\tF,FFF::FFFFF:FF:,:F,FF,FF,:FF,F,F:FFFFFFFF,FF,,,F:,::FFFF,:F,,FF,,FFF,,,F,:F,F,FF,,,FFFFFF,F,,,,,,FFFF,,,,FF,FF:FFF,,,,,:,F,FFFF,F:,F,F,FFF,,:F:,,F:,:,\tNM:i:13\tMD:Z:25G3T2G0T7G0T1G1G17C0G7T2G8C10\tAS:i:31\tXS:i:0 \n",
      " 34 941152\n",
      "A01968:63:H77VYDSX5:4:1101:6623:1125_1:N:0:AACGTGAT_BC:AAAATGTA\t0\tNC_002945.3\t1384138\t60\t33S103M15S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACTTGCCGGGCAGTCGCTGGCCGCAGACGTAGACACCGCAGTCGACCAAGGCTTGGGCCGGTGGCTGGGCAACGGGGTGTGCGTTCGGCGGCTGGGTTCGCGGGGGCGGGCGCCGCC\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,,FFFF,F:,:F::,F,,:,:\tNM:i:1\tMD:Z:97G5\tAS:i:98\tXS:i:0 \n",
      " 35 1384138\n",
      "A01968:63:H77VYDSX5:4:1101:26106:2675_1:N:0:AACGTGAT_BC:CACAAGGA\t0\tNC_002945.3\t1711155\t60\t33S99M19S\t*\t0\t0\tGTCTAAAGACCAGGAACTTATCAGCCAACATGTTACGACGACACCGCCGCGCCCCGGGGCGTTTCTTGATCGGCGCCGGGCCCCGGCGTAAACACATGCGGCAAAAAAAGGTTGGTTTCGAGTGCGGCAAGCAGGTCAGCAAGGCCCCTGA\tF:F:::FFF,:FFFFF,F:::,F:F,F,F,:,FFFFF:FF,FFFF:FF,FF:,F:FF,FF,,,:,FFFF:F,F,,FF,,FFF,:F:FF,FF,,:,,:,FFFFF,:F,,,:F:,FF:FF:F:,:,FF,FFF,F:F,F,FF,,,,F,:F,F,,\tNM:i:8\tMD:Z:19G29A8T1T1T7C3C13T10\tAS:i:59\tXS:i:0 \n",
      " 35 1711155\n",
      "A01968:63:H77VYDSX5:4:1101:26883:1110_1:N:0:AACGTGAT_BC:GGGGGGGG\t0\tNC_002945.3\t2733020\t60\t33S59M59S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACGACTGCAGTTCGGCGTCGACGTCGTCCTCACCGATTTCGATCGGATCCACCGAGAAGATCGGAAGAGCAACGTCTGAACTCCAGTCCTAGAAACCATCTCGTATGCCGTCTTCT\tFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF::FFFFFFFFFFFFF,FFFFFFFFFFFFFFFFFFFFFFFFFF:F,FFFFFF,FF::FF,,\tNM:i:0\tMD:Z:59\tAS:i:59\tXS:i:0 \n",
      " 35 2733020\n",
      "A01968:63:H77VYDSX5:4:1101:22833:1360_1:N:0:AACGTGAT_BC:GGGGGGGG\t0\tNC_002945.3\t2744873\t60\t33S114M4S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGCCCGAGGCATCCGTTGTGGCCAGACCGTGCGCGAGTGACCGTGCGGCCACCGACAGATCGTCGGGTCGCACCTTGCCGGCCACCACCCGACTGGCCAGCACGTCGAAGCCCAGAT\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FF:FFFFFFFFFF:FFFFFF\tNM:i:0\tMD:Z:114\tAS:i:114\tXS:i:0 \n",
      " 35 2744873\n",
      "A01968:63:H77VYDSX5:4:1101:19506:1047_1:N:0:AACGTGAT_BC:CGCTAAGG\t0\tNC_002945.3\t2883723\t60\t40S51M43S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACTAGACCGTCCAGTCTGGCAGGCCGGAAACATCGGTCAGCAGATAGGCTTTACCAGTAAGAAGGAGATATACATATGGAAAAAAAGGAATTACGTTTTA\tFFFFFFFFFFFFFF:FFFFFFFFFF:FFF:FFFFFFFF:FFF,F::F:FFFFFFFFFF,FF:FFFFFF:,FFFFFFFFFFF:FFFFFF,FF:FFFF:FFFFFFFFF:F,FFF:FFF,FF,,:FF:F,F,F,FFF\tNM:i:0\tMD:Z:51\tAS:i:51\tXS:i:0 \n",
      " 35 2883723\n",
      "A01968:63:H77VYDSX5:4:1101:28917:1157_1:N:0:AACGTGAT_BC:CTACGGAC\t0\tNC_002945.3\t2883723\t60\t40S51M60S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTATTACTAGACCGTCCAGTCTGGCAGGCCGGAAACAGCGGTCAGCAGATAGGCTTTACCAGTCAGAAGGAGATATACATATGGAAAAAAAGGAATTTCGTGTTTTGATAAAATACTGTTTT\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFF,F,::FF:F,FFFFFF:FF,F::FFFF,:FF,FF,:F,F,,FFFF:FFF:FFFFFF:,,F,,F:FF,:,F,FF:,F:,:,:,,:,FF,:,,:FF::F,FFF,,,F::F,FFFFF:F\tNM:i:1\tMD:Z:26T24\tAS:i:46\tXS:i:0 \n",
      " 35 2883723\n",
      "A01968:63:H77VYDSX5:4:1101:12717:1251_1:N:0:AACGTGAT_BC:ATTTCCTA\t0\tNC_002945.3\t2883723\t60\t40S51M50S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACTAGACCGTCCAGTCTGGCAGGCCGGAAACATCGGTCAGCAGATAGGCTTTACCAGTAAGAAGGAGATATACATATGGAAAAAAAGGAATTTCGTGTTTTGATAAA\tFFFFFFFFF:FFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:F,FFFFFFF:FFFFFFFFFFFFF\tNM:i:0\tMD:Z:51\tAS:i:51\tXS:i:0 \n",
      " 35 2883723\n",
      "A01968:63:H77VYDSX5:4:1101:15736:1376_1:N:0:AACGTGAT_BC:CCAACCTA\t0\tNC_002945.3\t2883723\t60\t40S51M11S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACTAGACCGTCCAGTCTGGCAGGCCGGAAACATCGGTCAGCAGATAGGCTTTACCAGTAAGAAGGAGA\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF,FFFFF\tNM:i:0\tMD:Z:51\tAS:i:51\tXS:i:0 \n",
      " 35 2883723\n",
      "A01968:63:H77VYDSX5:4:1101:14262:2425_1:N:0:AACGTGAT_BC:CACTTAGG\t0\tNC_002945.3\t2883723\t60\t40S51M60S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACTAGACCGTCCAGTCTGGCAGGCCGGAAACATCGGTCAGCAGATAGGCTTTACCAGTAAGAAGGAGATATACATATGGAAAAAAAGGAATTTCGTGTTTTGATAAAATACTGTTTT\tF,FFFF:FFFF:F:FFFFFFFFFFFFFFFFFFFFFFFF,FFF:FFF:FFFF:FFFFFFFFFFF::,FFFFFFF,FF,FFFFF:FFFFF,,F,FFF::F,F,FF,,F,,F:FF:F:F:,F:F,,FF,FFFFFF:,,FF:FFF,FFFFFFFFF\tNM:i:0\tMD:Z:51\tAS:i:51\tXS:i:0 \n",
      " 35 2883723\n",
      "A01968:63:H77VYDSX5:4:1101:17074:2660_1:N:0:AACGTGAT_BC:AAGACCAA\t0\tNC_002945.3\t2883723\t60\t40S51M60S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACTAGACCGTCCAGTCTGGCAGGCCGGAAACATCGGTCAGCAGATAGGCTTTACCAGTAAGAAGGAGATATACATATGGAAAAAAAGGAATTTCGTGTTTTGATAAAATACTGTTTT\tFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:F,F:F,F:FFFFFF,,FFFF:FFFFF,F:FFFFFF:FFFFFFFFF:FF,,FFF,FFFFFFFFF\tNM:i:0\tMD:Z:51\tAS:i:51\tXS:i:0 \n",
      " 35 2883723\n",
      "A01968:63:H77VYDSX5:4:1101:22535:1501_1:N:0:AACGTGAT_BC:TGATCGCC\t0\tNC_002945.3\t2911389\t31\t33S105M13S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAACAGACCCGGGCGACCCGGCGGCACCGCCGTTTACGCCCGCCCCCCCCGTACCCTAGACGAGCCCGCCCGCGCCCCCGTTACCCCCGTGACCGCCAAACCCAAACCCCCAGAACGG\tFFF:FFF,FFFFFFF,F:FFF:FF:FFFFFF,:FF:,FFFF,FF,FF,FFFF:FFF:FFFF,FFFFFF,F::F:,::FF,FF,,:,,,,,F,:,,:,FFF,FF:,:,:FFFF,:,F,,:,F,,,,,F,:F:,FF:::,,,,,:F:,F,,,F\tNM:i:13\tMD:Z:35C4G5G2G0T4G4G0T8G5G8G2T2C13\tAS:i:40\tXS:i:0 \n",
      " 35 2911389\n",
      "A01968:63:H77VYDSX5:4:1101:7003:2566_1:N:0:AACGTGAT_BC:TCACAATG\t0\tNC_002945.3\t3511515\t60\t33S115M3S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACGAGGCGCTGAAGGAACTCGAGGCCCAAGTCATCGCTCTGCAGCGAAGCGAGGGTAAGGGCCTGCTCAGCCGCCTGAGCTGAACGACTAGAGGATTGGGGAAGGGGCCCCCGGCTA\tFFFFF,FFFFFFFFFFFFF::,:FFFFF,FF:,FFFFFF:F:FFF,F::F,F:,FFF:FFFFF:FF:FF:FFFF:FFFFFFFF,FF:FFF,:FFFFF:FF:FF,:F,FFFFF:F:::FFFFF:F:FF:FFF,:FFF,:FFF:FF,F,F,,,\tNM:i:0\tMD:Z:115\tAS:i:115\tXS:i:0 \n",
      " 35 3511515\n",
      "A01968:63:H77VYDSX5:4:1101:20826:2863_1:N:0:AACGTGAT_BC:AGCTACAC\t0\tNC_002945.3\t3901948\t60\t33S109M9S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTACATCAAACTGCTGACCGGCGAGGTCGACACCGCGCTGGTGTACGGCTTCGGGAAGTCCTCGGCCGGACCGCTGCGCCGTGTGCTGTCCCGCCAGACCGACCCGTACAGCGTAGCTC\t,FF,FFFFFFFFFFFFF,FFFFF,FFFFFFFFFFF,FFFFF,,FF,FFFFFFFFFF::FFFFFFF::F,FFFFF,FFF:FFFFFFFFFFFFFFFFFFF:FFF,FFF:FFFF:FFFF::FFFF:F:F:FFFF,,FFF:FFFFFFF,F,,F,F\tNM:i:1\tMD:Z:69A39\tAS:i:104\tXS:i:0 \n",
      " 35 3901948\n",
      "A01968:63:H77VYDSX5:4:1101:14217:1470_1:N:0:AACGTGAT_BC:ATTAGGGT\t0\tNC_002945.3\t4315069\t60\t33S69M49S\t*\t0\t0\tGTCTAGAGACCGGGGACTTATCAGCCAACCTGTTAGAACTGGCTGCCCCCGCCGGCGGTTTTGGCCTGCAAAACCGCTCCCACCCGAGCGCCCGGGACCCACCCCCACCGGGTGGGCGCGCGAGCAACCGACCCCGCTCAACTCCAACCAA\t:FFF,FF,FFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFF:FFF:,:F,:FFFFFFF:,FFFF,::FFFFFF,,,F,FFF,:,,FFF:F,,,,:,,FF:,F,,,,,:,,,:,,,:,:,F,,,,:,,,,F,,F:,,::,\tNM:i:4\tMD:Z:28G9C4A18C6\tAS:i:49\tXS:i:0 \n",
      " 35 4315069\n"
     ]
    }
   ],
   "source": [
    "both_ends_reverse = []\n",
    "both_ends_forward = []\n",
    "for read in reads:\n",
    "    cig = read.split()[5]\n",
    "    cig_list = parse_cigar(cig)\n",
    "    strand = tn.parse_samflag(read)\n",
    "    if len(cig_list) >2 and strand==\"R\":\n",
    "        both_ends_reverse.append(read)\n",
    "        print(cig_list)\n",
    "    elif len(cig_list) >2 and strand==\"F\":\n",
    "        both_ends_forward.append(read)\n",
    "\n",
    "for read in both_ends_forward:\n",
    "    tag_pos = tn.find_tags2(read, \"ACTTATCAGCCAACCTGTTA\", 2)\n",
    "    ta_pos =  tn.find_ta_position(read, tag_pos)\n",
    "    print(read, \"\\n\", tag_pos, ta_pos)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### solution to mis-assigning tag position for reverse reads with soft-clipping at both ends of alignment\n",
    "\n",
    "Are the added bases on 3' end read-through of adapters that weren't removed from a shorter-than-average gDNA fragment?\n",
    "\n",
    "The find_tag program is finding tag from beginning of entire read--including soft-clipped bases at the 3' end of the reverse reads (left most end of sequence searched). Reduce search space and get more accurate position by having mmfind begin at the start of the aligned sequence (slicing with seq[soft-clipped base position: len(sequence]). Using rc of sequence just shifts problem to other end which also has soft-clipped bases that may be longer than tag length (so need to add to tag position) and also requires more time to find rc of each read vs using rc of tag only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cigar(cigar):\n",
    "    \"\"\"\n",
    "    Parse cigar string from sam file\n",
    "    regex: \\*|([0-9]+[MIDNSHPX=])+\n",
    "    \"\"\"\n",
    "    import re\n",
    "    #find all number/letter combos in cigar string\n",
    "    cig_list = re.findall(r'[0-9]+[MIDNSHPX=]+', cigar)\n",
    "    return cig_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mmfind1(G,n,H,m,max): # lengths; assume n>m\n",
    "  \"\"\"\n",
    "  A function to find matches of transposon sequence in read \n",
    "  (from tpp_tools.py, https://github.com/mad-lab/transit/blob/master/src/pytpp/tpp_tools.py)\n",
    "    Input               G                           sequence string\n",
    "                        n                           length of read sequence\n",
    "                        H                           pattern string\n",
    "                        m                           length of pattern\n",
    "                        max                         maximum mismatches\n",
    "    Output              i, -1                       start position of match, or -1 for no match\n",
    "  \"\"\"\n",
    "\n",
    "  a = G[:n].find(H[:m])\n",
    "  if a!=-1: return a # shortcut for perfect matches\n",
    "  for i in range(0,n-m):  # range of 0 to difference between seq length and len pattern\n",
    "    cnt = 0\n",
    "    for k in range(m):\n",
    "      if G[i+k]!=H[k]: cnt += 1\n",
    "      if cnt>max: break\n",
    "    if cnt<=max: return i\n",
    "  return -1\n",
    "\n",
    "\n",
    "def find_tags3(read, target_tag, max):\n",
    "    \"\"\"\n",
    "    Find transposon tag in sequence of each read.\n",
    "\n",
    "      Input           read              mapped read\n",
    "                      target_tag        string that matches transposon sequence\n",
    "                      max               maximum number of mismatches allowed (default=2)\n",
    "      Output          start             calculation of start of read/insertion point \n",
    "                                        from left-most start position of tag or -1 if no match\n",
    "    \"\"\"\n",
    "    from Bio.Seq import Seq\n",
    "    #find rc of tag\n",
    "    tag_seq = Seq(target_tag)\n",
    "    rc_tag  = str(tag_seq.reverse_complement())\n",
    "    # find strand of read\n",
    "    strand = parse_samflag(read)\n",
    "    \n",
    "    seq = read.split()[9]\n",
    "    if strand == \"F\":\n",
    "        #search string for transposon seq with max num mismatches\n",
    "        match = mmfind1(seq, len(seq), target_tag, len(target_tag), max)\n",
    "        if match != -1:\n",
    "            start = match + len(target_tag) #this gives start position of read (start of ta site)\n",
    "        else:\n",
    "            start = match\n",
    "    else: ##strand is reverse start searching sequence from other end \n",
    "        #search reverse sequence (3' to 5') for rc of tag (starting with TAAC--) start position of tag is end of tag and start of insert/ta site\n",
    "        #in some cases, soft-clipped bases at 3' end of read will make tag too long\n",
    "        # parse cigar string for relevant soft-clipping at 3' end of read\n",
    "        cig = read.split()[5]\n",
    "        cig_list = parse_cigar(cig)\n",
    "        if cig_list[0][-1]==\"S\": #if there is soft-clipping at both ends of read\n",
    "            sc_correction = int(cig_list[0][:-1]) #number of soft-clipped bases\n",
    "            match = mmfind1(seq[sc_correction:len(seq)], len(seq), rc_tag, len(rc_tag), max)\n",
    "        else:\n",
    "            match = mmfind1(seq, len(seq), rc_tag, len(rc_tag), max)\n",
    "        start = match\n",
    "    \n",
    "    return start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "A01968:63:H77VYDSX5:4:1101:3314:1626_1:N:0:AACGTGAT_BC:ATCGTGAT\t16\tNC_002945.3\t3002949\t60\t78M1D40M33S\t*\t0\t0\tGCGCAGGTGTTGAACACCACGACGTCGGCCTCGGAACCGTCGGTCGCCCTCCGGTAGCCGGCCGCTTCCAGCAGACCCCCAGCCGCTCGGAGTCGTGGACGTTCATCTGACAGCCGTAACAGGTTGGCTGATAAGTCCCCGGTCTCTAGAC\tFF:FFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFF:FFFFF:FFFFFFFFFFFF:FFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFF\tNM:i:1\tMD:Z:78^G40\tAS:i:111\tXS:i:0 \n",
      " 116 3003065\n",
      "number of correct matches:  35\n"
     ]
    }
   ],
   "source": [
    "print(len(both_ends_reverse))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bovis_fasta = \"ref_seqs/Mbovis_AF2122-97.fasta\"\n",
    "bovis_seq   = tn.open_fasta(bovis_fasta)\n",
    "ta_sites = tn.find_insertion_sites(bovis_seq)[0]\n",
    "count = 0\n",
    "for read in both_ends_reverse:\n",
    "    tag_pos = find_tags3(read, \"ACTTATCAGCCAACCTGTTA\", 2)\n",
    "    ta_pos =  tn.find_ta_position(read, tag_pos)\n",
    "   # print(read, \"\\n\", tag_pos, ta_pos)\n",
    "\n",
    "    if int(ta_pos) in ta_sites:\n",
    "        count +=1\n",
    "    else:    \n",
    "        print(read, \"\\n\", tag_pos, ta_pos)\n",
    "    \n",
    "print(\"number of correct matches: \", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one that doesn't work ends in TA but looks really badly mapped to genome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
